[{"id":0,"href":"/docs/docs/interview/docker/","title":"Docker","section":"面试题集","content":" docker COPY和ADD指令区别？ # copy ：将本地文件拷入容器内 add：除了将本地文件拷入容器内，还具有本地归档解压和支持远程网址访问（常用于本地自动解压归档文件到容器中） Docker 映像（image）是什么？ # Docker image 是 Docker 容器的源。映像（Images）通过 Docker build 命令创建， Docker 容器（container）是什么？ # Docker 容器包含其所有运行依赖环境,但与其它容器共享操作系统内核的应用，它运行在独立的主机操作系统用户空间进程中并不紧密依赖特定的基础平台 Docker 中心（hub）什么概念？ # 云端镜像仓库，可以打tag发布公共，私有镜像到远程仓库。 Docker 的资源隔离是怎么实现的 # Docker通过命名空间、控制组、容器镜像和安全配置等多种手段来实现对容器的资源隔禆，从而保证容器之间的独立性和安全性。 "},{"id":1,"href":"/docs/docs/interview/go/","title":"Go","section":"面试题集","content":" 代码问题 # 指针问题 # //使用副本的方式。所以m[stu.Name]=\u0026amp;stu实际上一致指向同一个指针， 最终该指针的值为遍历的最后一个struct的值拷贝 //副本值的指针-》指向真实指针-》真实值，随着遍历，真实指针移动到最后一个值 type student struct { Name string Age int } func pase_student() { m := make(map[string]*student) stus := []student{ {Name: \u0026#34;zhou\u0026#34;, Age: 24}, {Name: \u0026#34;li\u0026#34;, Age: 23}, {Name: \u0026#34;wang\u0026#34;, Age: 22}, } for _, stu := range stus { //值拷贝，没有存真实的切片[i]地址，只是一个指向真实切片的指针 m[stu.Name] = \u0026amp;stu //m[stu.Name] = \u0026amp;stus[i] 可以这种方式修改value真实地址 } } defer问题 # func calc(index string, a, b int) int { ret := a + b fmt.Println(index, a, b, ret) return ret } func main() { //defer 函数内的调用会直接运行，而直接的函数会入栈 a := 1 b := 2 defer calc(\u0026#34;1\u0026#34;, a, calc(\u0026#34;10\u0026#34;, a, b)) a = 0 defer calc(\u0026#34;2\u0026#34;, a, calc(\u0026#34;20\u0026#34;, a, b)) b = 1 } 参考答案： 10 1 2 3 20 0 2 2 2 0 2 2 1 1 3 4 接口问题 # package main import ( \u0026#34;fmt\u0026#34; ) type People interface { Show() } type Student struct{} func (stu *Student) Show() { } func live() People { var stu *Student return stu } func main() { if live() == nil { fmt.Println(\u0026#34;AAAAAAA\u0026#34;) } else { fmt.Println(\u0026#34;BBBBBBB\u0026#34;) } } 参考答案：“BBBBBBB”，因为接口必须要类型和值同时为nil才能等于nil，只要给接口赋值了一个有类型的变量则它的类型就不再是nil即便这个类型变量本身等于nil(如空指针)。 switch问题 # func main() { i := GetValue() switch i.(type) { case int: println(\u0026#34;int\u0026#34;) case string: println(\u0026#34;string\u0026#34;) case interface{}: println(\u0026#34;interface\u0026#34;) default: println(\u0026#34;unknown\u0026#34;) } } func GetValue() int { return 1 } 编译报错“cannot type switch on non-interface value i (type int)” type switch只能用于接口 位序问题 # const ( x = iota y z = \u0026#34;zz\u0026#34; k p = iota ) func main() { fmt.Println() fmt.Println(x, y, z, k, p) } 参考答案：0 1 zz zz 4 关键字iota代表其在一组常量中的位序，中间即便有其它赋值也没有影响后面的iota仍然代表总的位序。 常量问题 # package main const cl = 100 var bl = 123 func main() { println(\u0026amp;bl, bl) println(\u0026amp;cl, cl) } 参考答案：常量不会在运行时分配内存，所以不能读取它的地址。一般是在编译阶段把所有常量替换成它对应的值。 继承问题 # package main import\u0026#34;fmt\u0026#34; type T1 struct { } func(t T1)m1(){ fmt.Println(\u0026#34;T1.m1\u0026#34;) } type T2= T1 type MyStruct struct { T1 T2 } func main() { my:=MyStruct{} my.m1() } 参考答案：编译会报错：“ambiguous selector my.m1”。因为T1和T2都有m1()，所以调用时需指明如my.T1.m1()或者myT2.m1()。 闭包问题 # package main func test() []func() { var funs []func() for i := 0; i \u0026lt; 2; i++ { funs = append(funs, func() { println(\u0026amp;i, i) }) } return funs } func main() { funs := test() for _, f := range funs { f() } } 参考答案：输出的两行中每一行变量i的地址相同，i的取值也相同都等于2。当test函数返回后，函数闭包中引用到的外部变量将移动到堆里，地址不变 切片拷贝问题 # func main() { a := [3]int{1, 2, 3} for k, v := range a { //v在数组中是深拷贝，对原值没影响。切片是指向的原指针。 if k == 0 { a[0], a[1] = 100, 200 } a[k] = 100 + v } fmt.Print(a) //数组 101 102 103 } 协程传参问题 # // 输出10个10：for执行比较快，在协程创建，执行之前。 func main() { for i := 0; i \u0026lt; 10; i++ { go func() { fmt.Println(i) }() } time.Sleep(5 * time.Second) } //值乱序:给匿名函数传参，生成副本 func main() { for i := 0; i \u0026lt; 10; i++ { go func(i int) { fmt.Println(i) }(i) } time.Sleep(5 * time.Second) } //值乱序：引入局部变量 func main() { for i := 0; i \u0026lt; 10; i++ { tmp := i go func() { fmt.Println(tmp) }() } time.Sleep(5 * time.Second) } 切片扩容问题 # func change(s ...int) { s = append(s, 3) } func main() { slice := make([]int, 5, 5) slice[0] = 1 slice[1] = 2 change(slice...) //s触发了扩容，指针指向了新的内存。和slice没关系了 fmt.Println(slice) change(slice[0:2]...) //len是2，append后不需扩容，s指向的还是slice，值写入。 fmt.Println(slice) } //这个坑在面试中经常会遇到，当 slice 作为函数参数时，如果在函数内部发生了扩容，这时再修改 slice 中的值是不起作用的，因为修改发生在新的 array 内存中，对老的 array 内存不起作用，以下是代码的最小 case。 写代码 # golang channel顺序打印cat、dog、fish各100次 # func printWord(word string, count int, currentChan, nextChan chan struct{}, wg *sync.WaitGroup) { var i = 0 for i \u0026lt; count { if _, ok := \u0026lt;-currentChan; ok { fmt.Println(word) i++ nextChan \u0026lt;- struct{}{} } } wg.Done() } func main() { var ( wg sync.WaitGroup count = 100 dogChan = make(chan struct{}, 1) catChan = make(chan struct{}, 1) fishChan = make(chan struct{}, 1) ) wg.Add(3) // 按照 dog, cat, fish 顺序打印100遍 go printWord(\u0026#34;dog\u0026#34;, count, dogChan, catChan, \u0026amp;wg) go printWord(\u0026#34;cat\u0026#34;, count, catChan, fishChan, \u0026amp;wg) go printWord(\u0026#34;fish\u0026#34;, count, fishChan, dogChan, \u0026amp;wg) dogChan \u0026lt;- struct{}{} wg.Wait() } golang两个协程交替打印1\u0026hellip;n # func printNums(num *int, count int, currentChan, nextChan chan struct{}, wg *sync.WaitGroup) { defer wg.Done() for *num \u0026lt; count { if _, ok := \u0026lt;-currentChan; ok { fmt.Println(*num) *num++ } nextChan \u0026lt;- struct{}{} } } func main() { var ( wg sync.WaitGroup num = 0 count = 10 currentChan = make(chan struct{}, 1) nextChan = make(chan struct{}, 1) ) wg.Add(2) go printNums(\u0026amp;num, count, currentChan, nextChan, \u0026amp;wg) go printNums(\u0026amp;num, count, nextChan, currentChan, \u0026amp;wg) currentChan \u0026lt;- struct{}{} wg.Wait() } 比较切片是否相等 # func StringSliceEqualBCE(a, b []string) bool { if len(a) != len(b) { return false } if (a == nil) != (b == nil) { return false } b = b[:len(a)] for i, v := range a { if v != b[i] { return false } } return true } 实现Set # type inter interface{} type Set struct { m map[inter]bool sync.RWMutex } func New() *Set { return \u0026amp;Set{ m: map[inter]bool{}, } } func (s *Set) Add(item inter) { s.Lock() defer s.Unlock() s.m[item] = true } 控制一个goroutine生命周期 # ctx, cancel := context.WithCancel(context.TODO()) ch := make(chan int) // 开启一个协程 往管道写数据 go func() { for i := 0; ; i++ { select { case ch \u0026lt;- i: case \u0026lt;-ctx.Done(): return } } }() //读取数据，如果是特殊数据，就停止协程 for v := range ch { fmt.Println(v) if v == 5 { cancel() break } } 翻转含有中文、数字、英文字母的字符串 # func ReverseStr(str string) string { dst := []rune(str) for i, j := 0, len(dst)-1; i \u0026lt; j; i, j = i+1, j-1 { dst[i], dst[j] = dst[j], dst[i] } return string(dst) } 基础问题 # 常见语法错误 # 1.开大括号不能放到单独一行 2.未使用的变量 3.:=简式变量声明仅函数内部使用 4. nil用于interface 函数 map slice channel的零值 5. map 只有len,没有cap 6. 字符串不能为nil,和不能判断字符串是否为nil` 7.从一个现有的非interface类型创建新类型时，并不会继承原有方法，具体看“自定义类型不会继承原方法？” 8.访问map不存在的值，会返回对应类型的零值，可以用第二值ok，判断 9.对 defer 延迟执行的函数，它的参数会在声明时候就会求出具体值，而不是在执行时才求值： 10. 方法的值接收者或指针接收者都是可以实例调取的，区别是值是副本，指针会改变原值 切片支持并发吗？ # 不指定索引，动态扩容并发向切片添加数据，存在覆盖底层数组 指定索引，指定容量并发向切片添加数据，数组索引位置的数据会被替代 2个interface 可以比较吗？ # interface 的内部实现包含了 2 个字段，类型 T 和 值 V，interface 可以使用 == 或 != 比较 Go 语言的局部变量分配在栈上还是堆上？ # Go 语言编译器会自动决定把一个变量放在栈还是放在堆，编译器会做逃逸分析(escape analysis)，当发现变量的作用域没有超出函数范围，就可以在栈上，反之则必须分配在堆上. init 函数执行顺序？ # //初始化顺序不是按照从上到下的导入顺序，而是按照解析的依赖关系，没有依赖的包最先初始化 //initial()没有入参和返参，同一个源文件可以有多个init()顺序不定 import-\u0026gt;const-\u0026gt;var-\u0026gt;init()-main() 多返回值 # 返回值定义的变量 ，return 时不会创建临时变量保存返回值 空struct # fmt.Println(unsafe.Sizeof(struct{}{})) // 0 可以节省空间，比如map可以将空struct作为value的占位符 channel 控制并发时，可以用空struct作为信号 函数安全上，声明一个 空结构，将函数实现为它的方法，更安全。 定义枚举值 # type fruits string const ( apple fruits =\u0026#34;1\u0026#34; banana fruits =\u0026#34;2\u0026#34; ) %v 和 %+v区别 # %v 只打印value %+v 会打印key和value 值类型、引用类型的意义？ # 值类型：变量直接存储值，一般在栈上分配，值类型等号赋值后，更改相互不影响。 引用类型：变量存储的是一个地址，地址存的是最终的值，一般在堆上分配。修改值，会影响所有，作为函数入参时，不拷贝值的内容，而是传递的是指针，节省内存 可变参数？ # 函数和方法的入参 可以任意多个，在类型前面加上 ... 即可,可变参数其实是一个切片。 函数和方法的区别？ # 函数和方法类似，但是方法会在func和方法名间多了一个接收者，方法和接收者是绑定在一起的 接收者有两种类型：值类型，指针类型 值类型：调用的时候是接收者是值的副本，不会影响原来的值 指针类型：指针接收者传递的是一个指向原始指针的副本（指针的副本），指向的还是原来的值，修改接收者的值会影响原来的值 go语言的好处？ # 多平台编译，运行快 语言层支持并发 有垃圾回收 丰富的生态，语法糖简洁，和官方插件多 包管理 简短声明的变量需要注意？ # 在函数内，局部使用 struct的变量，不能用:=赋值 不能重复声明 :=具有声明初始化的作用 swith..case注意事项？ # 默认break,使用fallthrough强制往下判断，但是在接口的类型判断中，“cannot fallthrough in type switch” string 可以赋值为nil吗？ # 不能 编译不过 字符串不能被赋为 nil 字符串不能用nil判断 string类型可以修改吗？ # 不能，字符串也是引用类型，它类似切片，也包含一个指针字段，该字段指向它引用的字节系列的数组[]byte， 你改变一个字符串变量的值，只是改变指针字段的指向，指向新的值转化而来的字节数组，原来的字节数组并没有被改变， 而只是被弃用了而已（若引用数为零则被回收）； 而你的变量在声明时就已经确定了内存地址，和你给它赋什么值没关系； 可以通过将string转为[]byte再更改元素，再转为string,改变值 s := \u0026#34;hello\u0026#34; ss := []byte(s) ss[0] = \u0026#39;x\u0026#39; s = string(ss) go 字符串转成 byte 数组，会发生内存拷贝吗？ # 在Go语言中，将字符串转换为字节数组（[]byte）通常会发生内存拷贝。 内存拷贝是为了保持字符串的不变性。如果允许直接修改字符串的底层字节，那么可能会导致意外的行为，因为其他代码可能仍然持有对该字符串的引用。通过进行内存拷贝，可以确保原始的字符串内容不会被修改，从而维护了程序的正确性和一致性。 值为nil的slice、map会发送什么？ # 允许对值为nil的slice添加数据。 值为nil的map，也就是定义没有初始化，添加数据会造成panic 什么是rune和byte？ # uint8 类型，或者叫 byte 型，代表了 ASCII 码的一个字符。 rune 类型，代表一个 UTF-8 字符，当需要处理中文、日文或者其他复合字符时，则需要用到 rune 类型。rune 类型等价于 int32 类型。 打印时%v %+v %#v 区别？ # %v 只输出所有的值 %+v 输出字段名和对应值 %#v 先输出结构体名字值，再输出结构体（字段名字+字段的值）； 函数传参是值类型还是引用类型？ # 在Go语言中只存在值传递，要么是值的副本，要么是指针的副本。无论是值类型的变量还是引用类型的变量亦或是指针类型的变量作为参数传递都会发生值拷贝，开辟新的内存空间。 另外值传递、引用传递和值类型、引用类型是两个不同的概念，不要混淆了。引用类型作为变量传递可以影响到函数外部是因为发生值拷贝后新旧变量指向了相同的内存地址。 slice为nil和空的区别？ # slice := make([]int,0) slice不为nil，但是slice没有值，slice的底层的空间是空的。 slice := []int{} ：slice的值是nil，可用于需要返回slice的函数，当函数出现异常的时候，保证函数依然会有nil的返回值。 new和make的区别？ # new 根据传入的类型分配一片内存空间并返回指向这片内存空间的指针； make 初始化内置的数据结构，为silce、map、chanel分配空间，初始化，返回的是引用是实例。 select 作用？ # 监听channel的IO操作，触发相应的动作 如何在运行时判断变量类型？ # if _, ok := s.(string); ok { } switch s.(type) { case string: fmt.Printf(\u0026#34;type is string\u0026#34;) case int: fmt.Printf(\u0026#34;type is int\u0026#34;) default: fmt.Printf(\u0026#34;type is other\u0026#34;) } go 数据类型 # bool int8, int16, int32, int64 uint8, uint16, uint32, uint64 byte float32, float64 string complex64, complex128 array pointer struct slice map channel interface function 切片删除一个元素 # //管前不管后 s1 := []int{1, 2, 3, 4, 5} s1 = append(s1[:2], s1[3:]...) // 删除索引为2的元素 切片 s[:2]含义？ # //前包后不包 只包含从索引0到1的值，不包含2的索引值，可能考虑到[:len(s)-1]为全部数据 go nil切片和空切片区别 # nil切片和空切片的区别在于它们的底层数组指针是否为nil。nil切片的底层数组指针为nil，而空切片的底层数组指针不为nil。在使用上，对nil切片进行读写操作会导致运行时错误，而对空切片进行读写操作是安全的。因此，在实际编程中，建议使用空切片而不是nil切片来表示一个空的切片状态。 var s []int // 声明一个int类型的切片s，未进行初始化，s为nil切片 s := make([]int, 0) // 使用make函数创建一个长度为0的int类型切片s，s为空切片 数组和切片区别 # 两种不同的数据结构 数组固定大小，不能动态扩容，在编译期就会确定大小 切片由三个部分组成：指针、长度和容量。动态大小。需要使用make进行初始化。切片是一种数据结构，切片不是数组 尝试访问数组越界的索引，会导致运行时错误。 切片提供了越界检查，尝试访问超出当前长度的索引会返回一个零值和一个非nil的错误值。 printf 、Sprintf、Fprintf有什么不同 # printf：标注输出，一般是屏幕、命令行 Sprintf：将格式化的字符串输出到另一个字符串中 Fprintf：将输出到文件中 cap 作用于？ # array 、slice、channel //不能用于map，map使用len 值传递和引用传递 # 值传递，传的是值的副本，改动不影响原值 引用传递；指针、切片、map、chan、interface 指针意义是什么？ # 普通的变量，存储的是数据，而指针变量，存储的是数据的内存地址。省内存。 \u0026amp; 取指针的地址； * 取指针指向的数据。 =和:=区别？ # =是赋值，:=定义加赋值，变量只能定义一次，而可以多次赋值。全局变量定义在堆上，局部变量由编译器做逃逸分析决定堆或栈上。 json 包变量不加 tag 会怎么样？ # tag来指定JSON字段的名称。如果在结构体字段上没有添加tag，则会使用字段名作为默认的JSON字段名。 深入理解 # 方法的值接收者或指针接收者区别？ # 方法的调用可以是结构体实例，也可以是实现接口方法的原定义interface实例 区别是：结构体实例都可以调用 如果方法都是值接收者，interface实例是值或者指针都可以调用 如果方法的接收者只要有一个是指针，interface实例只能指针可以调用 package main type Animal interface { GetName() string GetAge() int } type Person struct { name string age int } func (p *Person) GetName() string { return p.name } func (p Person) GetAge() int { return p.age } func getPerson(name string, age int) Animal { return \u0026amp;Person{ name: \u0026#34;DaYu\u0026#34;, age: int(28), } } func main() { ani := getPerson(\u0026#34;sss\u0026#34;, 18) ani.GetName() ani.GetAge() } sync.Mutex # //是互斥锁用于资源，保证资源在使用时的独有性，不会因为并发而导致数据错乱，保证系统的稳定性。 var lock sync.Mutex //比较暴力，获取mutex后，其他无法访问 lock.Lock() // 加锁 lock.Unlock() // 解锁 sync.RWMutex # //读写锁 var rwlock sync.RWMutex rwlock.RLock() // 读锁,都可以读，不能写 rwlock.RUnlock() // 释放读锁 rwlock.Lock() // 读写锁，获取mutex后，其他无法访问 rwlock.Unlock() // 释放读写锁 sync.WaitGroup # //是协程并发任务的同步。 var wg sync.WaitGroup wg.Add() //计数器+1 wg.Done() //计数器-1 wg.Wait() //阻塞直到计数器变0 sync.Once # //保证资源只执行一次 var once sync.Once once.Do(func() { }) sync.Map # var smap sync.Map smap.Store(\u0026#34;ss\u0026#34;,11) //set v, ok := smap.Load(\u0026#34;sss\u0026#34;) //get smap.Delete(\u0026#34;sss\u0026#34;) //delete smap.Range(func(k, v interface{}) bool { //range return true }) sync/errgroup # // sync.WaitGroup的封装，加上了goroutine的错误返回 g, ctx := errgroup.WithContext(context.TODO()) dataMapping := []string{ \u0026#34;test\u0026#34;, \u0026#34;sadhkl\u0026#34;, \u0026#34;ejbfrkw\u0026#34;, } doces := make(chan interface{}, len(dataMapping)) g.Go(func() error { defer close(doces) for _, val := range dataMapping { select { case doces \u0026lt;- val: case \u0026lt;-ctx.Done(): //若可以读取，则其他进程已经发起了取消。 return ctx.Err() } } return nil }) if err := g.Wait(); err != nil { fmt.Println(err) } fmt.Println(doces) sync.Pool # //复用已经使用过的对象，来达到优化内存使用和回收的目的 var strPool = sync.Pool{ New: func() interface{} { //New定义你这个池子里面放的究竟是什么东西 return \u0026#34;test str\u0026#34; }, } str := strPool.Get() //从池子里面获取我们之前在New里面定义类型的数据。 strPool.Put(str) // 放回去，或者放别的同类型的数据进去 defer执行顺序，作用 # 先进后出的顺序执行，在return之后执行 保证defer的函数执行完毕后，defer在无使用后才被释放 常用于：文件关闭、数据库断开连接、协程计数减一 defer file.close() 顺序? # 在err之前:defer在程序return后执行，若打开文件的文件为空，file指向nil，file.close()无法释放一个指向空的文件，而且file.close内部没有判空操作，所以会报错 GOMAXPROCS的含义和用法？ # go任务调度器的p个数 维护线程池中线程与 CPU 核心数量的对应关系;runtime.GOMAXPROCS(逻辑CPU数量) GMP 是什么？ # 为了内核线程更好的调度goroutine， G：Goroutine M：Machine，操作系统的执行线程 P：Processor调度器，处理M与G的关系,G薪资G\u0026#39;时，优先加入P的G所在的队列。 创建G：每执行一次go f()就创建一个G，包含要执行的函数和上下文信息。 G到M的映射：调度器Sched负责G到M的映射。新创建的G放在P的本地队列中，等待运行。 M获取G并执行：M表示机器核心，它会从P的本地队列中获取G并执行。 队列限制：P的本地队列存放等待运行的G，数量限制为256个。 阻塞处理：当M执行G时，如果G发生阻塞，M会释放当前P，并尝试获取新的P来执行其他G。 唤醒处理：当阻塞的G被唤醒时，它会回到调度队列中等待被调度执行。 工作窃取：为了平衡负载，处于空闲状态的P会尝试从其他P那里窃取一半的可运行G来执行。 自旋状态：当M没有G可执行时，它会进入自旋状态，不断从全局队列或其他P的本地队列中尝试获取G。 系统调用：当M进行系统调用时，会阻塞当前线程，调度器会创建新的M来执行其他G。 调度器辅助线程：当调度器发现有空闲的P而无空闲的M时，会唤醒或创建一个新的M来执行P中的G。 gmp有了本地队列，为什么还要全局队列，为什么不直接从全局队列拿 # 负载均衡：当一个p空闲，另一个p过载的情况。 全局队列可以保证G在某个p长时间无法获取执行机会，在全局可以被其他P获取 提高并发量：可以作为所有p的备份队列，本地p满载可以给其他空闲的p机会 如何停止goroutine? # 发送一个信号通道停止，协程内使用for select持续监听信号 使用context.WithContext() 使用errgroup 闭包代码 # //给访问外部函数定义的内部变量创造了条件，将关于的函数的一切封闭到了函数内部，减少了全局变量 package main import \u0026#34;fmt\u0026#34; type logClosure func(format string, v ...interface{}) func LoggerWrapper(logType string) logClosure { return func(format string, v ...interface{}) { fmt.Printf(fmt.Sprintf(\u0026#34;[%s] %s\u0026#34;, logType, format), v...) fmt.Println() // 换行 } } func main() { info_logger := LoggerWrapper(\u0026#34;INFO\u0026#34;) warning_logger := LoggerWrapper(\u0026#34;WARNING\u0026#34;) info_logger(\u0026#34;this is a %s log\u0026#34;, \u0026#34;info\u0026#34;) warning_logger(\u0026#34;this is a %s log\u0026#34;, \u0026#34;warning\u0026#34;) } 内存对齐？ # 以字长（word size）为单位访问，32 位的 CPU ，字长为 4 字节。 合理的内存对齐可以提高内存读写的性能，并且便于实现变量操作的原子性。 两个interface怎么比较? # 具有相同的类型，则它们可以进行比较，两个空接口类型值的底层类型不同，则无法进行比较。 //判断类型是否一样 reflect.TypeOf(a).Kind() == reflect.TypeOf(b).Kind() //判断两个interface{}是否相等 reflect.DeepEqual(a, b interface{}) //将一个interface{}赋值给另一个interface{} reflect.ValueOf(a).Elem().Set(reflect.ValueOf(b)) 如何关闭http的响应体 # resp,err = http.get(\u0026#34;www.baidu.com\u0026#34;) defer resp.Body.Close() //正确关闭resp.body body,err:=ioutil.ReadAll(resp.Body) 如何从painc中恢复？ # 在defer延迟函数中调用recover，它能捕捉/中断painc var PanicBufLen = 1024 defer func() { if e := recover(); e != nil { buf := make([]byte, PanicBufLen) buf = buf[:runtime.Stack(buf, false)] log.Errorf(\u0026#34;[PANIC]%v\\n%s\\n\u0026#34;, e, buf) report.PanicNum.Incr() } cancel() }() defer执行时机？ # //直接调用 无效 recover() //直接defer recover() 无效 //defer调用多层嵌套，无效 defer func() { func() { recover() }() }() panic(1) //只有在defer函数中直接调用，有效 defer func() { recover() }() panic(1) go 触发异常的场景？ # 空指针解析参数。取值，没有make 分配空间 对interface类型变量，考虑不充分 除数为0 调用painc函数 三色标记 # 用于垃圾回收 1.首先将所有对象放到白色集合中 2.从根节点遍历直接调用的白色对象,遍历的白色对象放到灰色集合中,根遍历完毕. 3.遍历灰色集合,将灰色对象引用的白色对象放入到灰色集合中,并将开始的灰色对象放入到黑色集合中 4.循环上一步.直到灰色集合没有对象 5.此时白色集合中的对象就是不可达对象,进行垃圾回收 三色标记会存在stw问题，stw后整个程序会停止运行，一个白色对象被黑色对象引用了 且 该白色对象上游没有被任何灰色对象引用，会导致错误的回收，导致对象丢失 术语： 解决stw问题 # 强三色不变式：不存在黑色对象引用到白色对象的指针 弱三色不变式：所有被黑色对象引用的白色对象都处于灰色保护状态 defer 顺序 # //defer 的执行顺序是后进先出。当出现 panic 语句的时候，会先按照 defer 的后进先出的顺序执行，最后才会执行panic func defer_call() { defer func() { fmt.Println(\u0026#34;打印前\u0026#34;) }() defer func() { fmt.Println(\u0026#34;打印中\u0026#34;) }() defer func() { fmt.Println(\u0026#34;打印后\u0026#34;) }() panic(\u0026#34;触发异常\u0026#34;) } 自定义类型不会继承原方法？ # //mtx.Lock undefined (type myMutex has no field or method Lock)... type myMutex sync.Mutex func main() { var mtx myMutex mtx.Lock() mtx.UnLock() } // 类型以字段形式直接嵌入,这种可以 type myLocker struct { sync.Mutex } func main() { var locker myLocker locker.Lock() locker.Unlock() } 怎么避免内存逃逸？ # 它发生在本应在栈上分配的内存被错误地分配到了堆上，从而导致了不必要的堆内存分配和垃圾回收压力。 使用局部变量：尽量使用局部变量而不是全局变量。局部变量在函数执行结束后会被自动释放，而全局变量会一直存在，容易造成内存泄漏。 避免在循环中分配内存：在循环中频繁地分配和释放内存会导致性能下降。 减少使用指针： 使用逃逸分析工具 注意切片和映射的使用：切片（slice）和映射（map）是Go语言中常见的动态数据结构，但它们也容易导致内存逃逸 golang中互斥锁与读写锁都是什么有什么区别？ # 互斥锁:允许一个线程对共享资源进行访问，其他线程需要等待当前线程释放锁之后才能访问共享资源 读写锁:读锁允许多个线程同时对共享资源进行读操作,写锁和互斥锁一样 多台机器加读写锁有什么问题？ # 读写锁是单个进程或单个机器上并发访问的，多台机器使用读写锁无法保证全局一致性 可以考虑使用分布式锁，或者用消息队列串行访问。 对golang中context的理解？ # 用于管理请求的上下文，包括截止时间、取消信号、请求范围的数据等。 在请求链中传递特定信息，请求ID，用户信息，和全链路追踪。 context 可以设置请求的截止时间，当超过截止时间时，可以自动取消相关的操作 可以用于处理请求的超时和取消，确保及时释放资源和终止操作 singleflght是使用什么方式去通知其他线程，其他线程怎么阻塞的 # singleflight主要使用sync.Mutex和sync.WaitGroup进行并发控制. 对于key相同的请求, singleflight只会处理的一个进入的请求，后续的请求都会使用waitGroup.wait()将请求阻塞 a和b两个线程，a里面有defer recover，a里面新开了一个b，b没写defer recover，b发生了panic，ab两个线程会发生什么情况？ # defer recover只能捕获当前goroutine中的panic，无法跨goroutine捕获panic。 因此，线程b中的panic会导致整个程序崩溃，线程a中的defer recover无法对线程b中的panic进行处理。 在函数参数传递一个非指针的互斥锁会发生什么事情？为什么会发生？ # 函数内部会对该互斥锁进行值拷贝，而不是对原始互斥锁进行操作。会影响到原始的互斥锁，因为函数操作的是互斥锁的副本。 应该将互斥锁作为指针类型传递给函数 slice # 扩容后的slice是否相同 # 情况一：如果一开始切片的容量够大，追加后，不超过原容量，指针还是指向原来的数组 情况二：切片追加后，超过了原来数组容量，会先开辟出合适的内存区域，将原数组的值拷贝过来，然后追加新值，指向新的内存指针，但不影响切片实例，因为切片存的是值的指针。 拷贝大切片一定比小切片代价大吗？ # 拷贝大切片通常比拷贝小切片的代价大，但具体取决于切片中元素的类型、切片的容量和长度等因素。 如果切片中的元素是基本类型（如整数、浮点数等），那么拷贝操作会比较快，因为基本类型的复制只是简单地复制值。然而，如果切片中的元素是引用类型（如指针、切片、映射或自定义结构体等），那么拷贝操作会复制引用而不是底层数据。这种情况下，拷贝大切片和小切片的代价可能相差不大，因为只是复制了引用而不是实际的数据。 切片是怎么扩容的 # 如果新申请的容量大于2倍的旧容量，新容量就是最终容量 如果旧切片长度小于1024，两倍扩容 如果大于1024，循环增加原来的四分之一,直到最终容量可存储新变量 slice 深拷贝和浅拷贝 # 当你将一个切片赋值给另一个变量时，实际上是在复制切片的引用，而不是底层数组的数据。这种行为被称为浅拷贝,意味着两个切片变量引用的是相同的底层数组，因此它们共享相同的数据。对一个切片的修改将影响另一个切片，因为它们都指向相同的内存区域。 深拷贝（deep copy），你需要创建一个新的切片，并将原始切片的元素逐个复制到新切片中。这样可以确保新切片和原始切片具有独立的内存空间，互不影响。 channel # channel 需要注意什么？ # 无缓冲的 channel 是同步的，必须接收才能使用，而有缓冲的 channel 是非同步的，可以先存后用 关闭一个 nil channel 将会发生 panic 未初始化的通道是nil，对其进行读写操作会导致panic。 关闭的通道，写数据会引发panic，而从已关闭的通道接收数据会返回零值。 发送操作和接收操作在通道上是阻塞的。如果接收方没有准备好接收数据，发送方会被阻塞，直到有接收方准备好接收数据。 避免死锁 无缓冲、有缓存channel区别？ # var ch = make(chan string) //无缓冲，意味着必须有人接收后才能存储 var ch = make(chan string, 10) //有缓冲区，意味着可以存一些数据后，再接收。 容量为1的channel在什么情况下会堵塞 # 发送方：chan发送一个值后，接收者没有立即接收，此时再次发送会导致阻塞 接收者：chan接收了一个值后，发送方没有立即发送，此时再次接收操作会导致堵塞。 对已经关闭的的 chan 进行读写，会怎么样？为什么？ # 读取已关闭的通道： 当通道被关闭后，对其进行读取操作会立即返回通道中剩余的值。如果通道中没有剩余的值，那么读取操作将返回零值 当通道被关闭后，对其进行写入操作会导致panic。 对未初始化的的 chan 进行读写，会怎么样？为什么？ # 对未初始化的通道进行读写会导致程序在运行时发生panic。 如何判断一个 channel 已经关闭 # 多重返回值来判断一个 value, ok := \u0026lt;-ch，判断 map # map取值注意事项？ # 当map的值不存在，总是会返回零值（nil、false、0） 正确取值为，判断第二个值是否为ture 内置map是否有序，为什么？ # map内部存储机制是以key为hash的结构实现的，所以顺序是混乱的， 可以将key存到slice中，slice.sort()排序后,输出。 map是线程安全的吗 # Go的map在并发环境下并不是线程安全的,使用互斥锁（sync.Mutex）或并发安全的数据结构（如sync.Map）来保护map的读写操作 map底层实现 # map的底层实现是一个哈希表的数组，每个元素是一个桶（bucket），每个桶中存储了键值对。Go语言会根据键的哈希值找到对应的桶,当多个键映射到同一个桶时，它们会被存储在同一个桶中的链表中,即使发生哈希冲突，仍然可以通过链表来存储和查找键值对。 哈希表的大小是固定的，当元素数量增多时，会触发哈希表的扩容操作 map 扩容机制 # map中的键值对数量达到一定阈值时，触发扩容操作, 创建一个新的哈希表，其大小通常是原哈希表大小的两倍。 遍历原哈希表中的每个桶，将其中的键值对重新计算哈希值，并存储到新的哈希表中的对应位置。 释放原哈希表的内存空间，将map的指针指向新的哈希表。 map查找 # 采用哈希查找表，由一个key通过哈希函数得到哈希值，由这个哈希值将key对应存在不同的捅内，当多个哈希映射到相同的捅内，使用链表解决哈希冲突 go 的sync.map怎么实现的 # 采用读写分离，写的话到dirty，要加锁，读的话先read内查找，高速缓存，找到就返回，找不到就加锁到dirty里 map的一个bucket可以放多少个数据 # 每个 bucket 最多可以存储 8 个键值对。当 bucket 中的键值对数量超过这个限制时，会触发哈希冲突处理机制，比如使用链地址法,从而允许在同一个 bucket 中存储更多的键值对。 map key和value占用的内存空间是固定的吗 # map 是一种动态数据结构，它的内部实现会根据实际存储的元素数量动态调整内存空间的分配 map并发不安全为什么会panic，int并发出错会不会panic，为什么 # 多个 goroutine 同时对 map 进行读写操作时，可能会导致 map 的内部数据结构出现不一致的情况，比如链表结构被破坏或者指针指向错误的情况，从而触发了运行时 panic。 由于 int 类型是基本数据类型,操作是原子的,不会出现并发访问导致数据结构破坏的情况。 map里面的数据怎么存的，怎么读的 # 它通过哈希函数将键映射到存储值的位置。在 map 中，每个键值对被存储在一个桶（bucket）中，桶的数量是动态变化的，取决于 map 中存储的元素数量。 需要读取 map 中的数据时，首先会根据键计算哈希值，然后根据哈希值找到对应的桶，最后在桶中查找对应的值。如果存在哈希冲突（即多个键映射到同一个桶），则会使用链表或其他方法来解决冲突，确保能够正确地找到对应的值 map 不初始化使用会怎么样 # 如果你尝试使用一个未初始化的 map，将会出现运行时错误。在 Go 中，map 是一个引用类型，如果它没有被初始化，它的值会是 nil。当你尝试访问或修改一个 nil 的 map 时，程序将会出现 panic。 map 不初始化长度和初始化长度的区别 # map 的长度指的是 map 中存储的键值对的数量。对于未初始化的 map 和已初始化但为空的 map，它们的长度都是 0。 map 承载多大，大了怎么办 # Go 中的 map 没有固定的最大容量限制，而是受限于可用内存的大小。理论上，map 可以承载非常大的数据量，直到系统内存耗尽为止。 当 map 变得非常大时，可能会遇到性能问题。因为随着元素数量的增加，查找、插入和删除操作的平均时间复杂度可能会增加。此外，大量的内存占用也可能导致垃圾回收（Garbage Collection）的开销增加，从而对性能产生负面影响。 分段处理：将大的 map 分割成多个小的 map，每个小 map 负责一部分键值对。这样可以减少单个 map 的大小，提高查找和操作的效率。 使用其他数据结构：根据具体需求，可以考虑使用其他数据结构来替代 map。例如，如果只需要进行顺序遍历而不需要随机访问，可以使用切片（slice）或数组（array）。 优化键的哈希函数：在 Go 中，map 的性能很大程度上取决于键的哈希函数的质量 压缩数据：如果 map 中的数据存在冗余或可以压缩的空间，可以考虑对数据进行压缩后再存储到 map 中。这样可以减少内存占用，并提高性能。 总之，当 map 变得过大时，可以通过分段处理、使用其他数据结构、优化哈希函数或压缩数据等方法来解决问题。具体的选择取决于你的应用场景和需求。 map 触发扩容的时机，满足什么条件时扩容？ # 当前 map 的元素数量（即已存储的键值对数量）大于或等于当前容量的阈值。 map 的扩容操作是一个相对昂贵的操作，因为它涉及到重新散列和复制原有的键值对。因此，在使用 map 时，如果可以预先知道大致的元素数量，可以在创建 map 时指定一个合适的初始容量，以减少扩容操作的次数，提高性能。 "},{"id":2,"href":"/docs/docs/interview/grpc/","title":"Grpc","section":"面试题集","content":" 微服务 # 微服务的优势 # 独立开发 独立部署 故障隔离 混合技术栈 粒度缩放 微服务特点算法 # 解耦—系统内的服务很大程度上是分离的。 组件化—微服务被视为可以轻松更换和升级的独立组件 业务能力—微服务非常简单，专注于单一功能 自治—开发人员和团队可以彼此独立工作，从而提高速度 分散治理—重点是使用正确的工具来做正确的工作。 "},{"id":3,"href":"/docs/docs/interview/kafka/","title":"Kafka","section":"面试题集","content":" kafka怎么保证数据消费一次且仅消费一次？使用消息队列如何保证幂等性？ # 保证消息不丢失： Kafka 为生产者提供一个选项叫做“acks”，当这个选项被设置为“all”时，生产者发送的每一条消息除了发给 Leader 外还会发给所有的 ISR，并且必须得到 Leader 和所有 ISR 的确认后才被认为发送成功。这样，只有 Leader 和所有的 ISR 都挂了，消息才会丢失。 保证被消费一次： 幂等性： 生产者：在 Kafka0.11 版本和 Pulsar 中都支持“producer idempotency”的特性，翻译过来就是生产过程的幂等性，特性保证消息虽然可能在生产端产生重复，但是最终在消息队列存储时只会存储一份。 每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一 ID，消息队列的服务端会存储 \u0026lt; 生产者 ID，最后一条消息 ID\u0026gt; 的映射。当某一个生产者产生新的消息时，消息队列服务端会比对消息 ID 是否与存储的最后一条 ID 一致，如果一致，就认为是重复的消息，服务端会自动丢弃。 消费端：通用层和业务层，mysql增加事务，判断判断是否存在，再写入。 或者：乐观锁给这条信息加一个版本号，判断版本号是否是未更新的，否则不更新 kafka如何保证消息的顺序 # Kafka通过分区来保证消息的顺序性。在Kafka中，每个主题（topic）都被分为一个或多个分区（partition），每个分区内的消息是有序的。因此，如果您希望保证特定主题内的消息顺序，可以将该主题的分区数设置为1，这样所有的消息都会按照其写入的顺序进行存储和检索。 Kafka还可以通过消息的键（key）来保证特定键的消息被发送到同一个分区，从而保证特定键的消息在该分区内是有序的。这种方式适用于需要对特定实体的消息进行顺序处理的场景。 "},{"id":4,"href":"/docs/docs/interview/mysql/","title":"Mysql","section":"面试题集","content":" 数据库 # 关系型和非关系型数据库的区别？ # 关系型数据库 采用关系模型组织数据 保持数据的一致性 数据更新开销比较小 支持复杂查询(带where子句的查询) 非关系型数据库 无需经过sql层的解析，读写效率高 基于键值对，读写性能高，易于扩展 支持多种类型数据的存储，如图片、文档 扩展性强，适合数据量大高可用的日志系统，地理位置存储系统 详细说一下一条 MySQL 语句执行的步骤？ # 客户端请求-\u0026gt;连接器（验证身份，给与权限） 查询缓存（存在缓存直接返回，不存在计息往后执行） 分析器（对sql进行词法分析和语法分析） 优化器（对执行的sql优化选择最优的执行方案） 执行器（先看用户是否有执行权限）-\u0026gt;去引擎层获取数据返回（如开启查询缓存，则缓存查询结果） 怎么理解数据库的索引？ # 索引类似于书籍的目录,通过使用索引，数据库系统可以在执行查询时更快地定位到符合特定条件的数据行，从而提高查询性能。不必全表扫描 索引并非没有代价的。它们需要额外的存储空间，并且在插入、更新和删除数据时需要维护，这可能会导致一些性能开销 MySQL 索引的优缺点？ # 提高数据查询的效率，就像书的目录一样 创建唯一性索引，可以保证数据表每一行记录的唯一性 帮助引擎层避免排序和临时表 将随机io变为顺序io，加速表和表之间的连接 创建、维护索引需要消耗时间，对表的数据进行增加、删除 修改时索引也要动态维护，会降低这些执行的效率 索引也是需要占物理空间 索引的三种常见底层数据结构以及优缺点 # 哈希表 适用于等值查询的场景，不适合范围查询 有序数组 适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高 搜索树 索引的常见类型以及它是如何发挥作用的？ # 根据叶子节点的内容，索引分为主键索引和非主键索引 主键索引的叶子节点存的整行数据，在InnoDB里也被称为聚簇索引。 非主键索引叶子节点存的主键的值，在InnoDB里也被称为二级索引。 什么时聚簇索引何时用聚簇非聚簇索引？ # 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储和索引结构分开，索引结构的叶子节点指向数据对应行， 索引的几种类型？ # 主键索引：数据列不允许重复，不允许为NULL,一个表只能有一个主键 唯一索引：数据列不允许重复，允许为NULL,一个表允许多个列创建唯一索引 普通索引：没有唯一性限制，允许为NULL 全文索引：效率上是模糊查询的N倍 MySQL 存储引擎 MyISAM 与 InnoDB 区别 # 1.锁粒度方面：由于锁粒度不同，InnoDB 比 MyISAM 支持更高的并发;InnoDB 的锁粒度为行锁、MyISAM 的锁粒度为表锁、行锁需要对每一行进行加锁， 2.可恢复性上：由于 InnoDB 是有事务日志的，所以在产生由于数据库崩溃等 条件后，可以根据日志文件进行恢复。而 MyISAM 则没有事务日志。 3.查询性能上:MylSAM 要优于 InnoDB 因为 InnoDB 在查询过程中，是需要维护 数据缓存，而且查询过程是先定位到行所在的数据块，然后在从数据块中定位到要查找的行;而 MyISAM 可以直接定位到数据所在的内存地址，可以 直接找到数据。 4.表结构文件上:MyISAM 的表结构文件包括:frm(表结构定义),.MYI(索引),.MYD(数据);而 InnoDB 的表数据文件为:ibd 和 frm(表结构定义)。 MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？ # InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件 MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data 域存放的是数据记录的地址，索引文件和数据文件是分离的. B树和B+树有什么区别 # B树中，每个节点既可以存储数据，也可以存储子节点的指针,B+树中，非叶子节点只存储子节点的指针，不存储数据，所有的数据都存储在叶子节点中 在B树中，叶子节点之间不一定需要连接，因为数据可以存储在非叶子节点中。 B+树中，所有的叶子节点都通过指针连接在一起，形成一个有序链表，便于范围查询。 B树中，由于数据可以存储在非叶子节点中，因此查找数据时可能需要在非叶子节点和叶子节点之间进行多次查找。 B+树中，由于数据只存储在叶子节点中，因此查找数据时只需要在叶子节点中进行查找，可以减少查找的次数。 union union all 区别？ # union连接的sql列必须一致 union 多个相同的行会合并，合并有耗时 union all 允许重复 InnoDB 为什么设计 B+ 树索引？ # InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。 CPU 将磁盘上的数据加载到内存中需要花费大量时间。 为什么选择B+数 # 哈希索引虽然能提供O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。 B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机 IO。 而 B+ 树的所有叶节点可以通过指针来相互连接，减少顺序遍历带来的随机 IO。 为什么用b+树不用b树 # 幻读是怎么解决的 # redo，undo的作用和实现 # 普通索引还是唯一索引？ # 由于唯一索引用不上change buffer的优化机制，如果业务可以接收，从性能上建议优先非唯一索引 什么是覆盖索引和索引下推？ # // 覆盖索引： 在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引 覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 //索引下推： MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 什么是索引？ # 索引是一种特殊的文件，它包含着对数据表里所有记录的引用指针。相当于目录，为了查找书中的内容，通过对内容建立索引形成目录。 哪些操作会导致索引失效？ # 对索引使用左或者左右模糊匹配，也就是 like %xx 或者 like %xx% 这两种方式都会造成索引失效 对索引进行函数/对索引进行表达式计算,因为索引保持的是索引字段的原始值，而不是经过函数计算的值，自然就没办法走索引。 对索引进行隐式转换相当于使用了新函数。 WHERE 子句中的 OR语句，只要有条件列不是索引列，就会进行全表扫描。 字符串加索引? # 直接创建完整索引，这样可能会比较占用空间。 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引。 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题。 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。 MySQL 的 change buffer 是什么？ # 当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。 这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 注意唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。 适用场景： - 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 - 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 MySQL 是如何判断一行扫描数的？ # MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。 而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。 MySQL 的 redo log 和 binlog 区别？ # redo log 作用：用于崩溃恢复 实现方式：innodb存储引擎实现 记录方式：循环写的方式记录，写到结尾时，会回到开头循环写记录 文件大小：固定的 crash-safe能力：具有 日志类型：逻辑日志 binlog 作用：主从复制和数据恢复 实现方式：server层实现，所有的存储引擎都可以使用binlog日志 记录方式：通过追加的方式记录，当文件尺寸大于给定的配置值后，后续的日志会记录到新的文件上 文件大小：通过配置参数max_binlog_size设置每个binlog文件大小 crash-safe能力：没有 日志类型：物理日志 为什么需要 redo log？ # redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。 其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash后 内存数据丢失的隐患，而 redo log 具备 crash safe 的能力 为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？ # 第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有 redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log 这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。 当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让 innoDB 判断哪些数据已经刷盘，哪些数据还没有。 但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。 第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交 redo log 每次更新操作完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。 redo log 内部结构是基于页的，记录了这个页的字段值变化，只要crash后读取redo log进行重放，就可以恢复数据。 这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备。 当数据库 crash 后，如何恢复未刷盘的数据到内存中？ # change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。 change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从 binlog 恢复 redo log，再从 redo log 恢复 change buffer。 change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。 redo log 写入方式？ # redo log包括两部分内容，分别是内存中的日志缓冲(redo log buffer)和磁盘上的日志文件(redo log file)。 MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是WAL。 可以发现，redo log buffer写入到redo log file，是经过OS buffer中转的。其实可以通过参数innodb_flush_log_at_trx_commit进行配置，参数值含义如下： 0：称为延迟写，事务提交时不会将redo log buffer中日志写入到OS buffer，而是每秒写入OS buffer并调用写入到redo log file中。 1：称为实时写，实时刷”，事务每次提交都会将redo log buffer中的日志写入OS buffer并保存到redo log file中。 2：称为实时写，延迟刷。每次事务提交写入到OS buffer，然后是每秒将日志写入到redo log file。 redo log 的执行流程? # //update T set a =1 where id =666 MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server 层。 MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。 InnoDB 存储引擎层将a修改为1的这个操作记录到内存中。 记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。 此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。 等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。 在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。 inlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗? # binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。 当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。 但是 binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。 什么是两阶段提交？ # MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，这就是\u0026#34;两阶段提交\u0026#34; 为什么需要两阶段提交呢? 如果不用两阶段提交的话，可能会出现这样情况 先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。 先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log 备份恢复时，数据不一致。 两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。 在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog事务是否成功，决定是回滚还是执行。 MySQL 怎么知道 binlog 是完整的? # 一个事务的 binlog 是有完整格式的： statement 格式的 binlog，最后会有 COMMIT； row 格式的 binlog，最后会有一个 XID event。 什么是 WAL 技术，有什么优点？ # WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。 好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过redo log 恢复，所以能够实现快速响应 SQL 语句。 binlog 日志的三种格式 # Statement格式:基于SQL语句的复制((statement-based replication,SBR)) 每一条会修改数据的 SQL 都会记录在 binlog 中 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 缺点：由于记录的只是执行语句，为了这些语句能在备库上正确运行，还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在备库得到和在主库端执行时候相同的结果。 Row：基于行的复制。(row-based replication,RBR) 不记录 SQL 语句上下文相关信息，仅保存哪条记录被修改。 优点：binlog 中可以不记录执行的 SQL 语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。不会出现某些特定情况下的存储过程、或 function、或trigger的调用和触发无法被正确复制的问题。 缺点:可能会产生大量的日志内容。 Mixed：混合模式复制。(mixed-based replication,MBR) 实际上就是 Statement 与 Row 的结合。一般的语句修改使用 statment 格式保存 binlog，如一些函数，statement 无法完成主从复制的操作，则采用 row 格式保存 binlog，MySQL 会根据执行的每一条具体的 SQL 语句来区分对待记录的日志形式。 原本可以执行得很快的 SQL 语句，执行速度却比预期的慢很多，原因是什么？如何解决？ # MySQL 数据库本身被堵住了，比如：系统或网络资源不够。 SQL 语句被堵住了，比如：表锁，行锁等，导致存储引擎不执行对应的 SQL 语句。 确实是索引使用不当，没有走索引。 表中数据的特点导致的，走了索引，但回表次数庞大。 解决： 考虑采用 force index 强行选择一个索引 考虑修改语句，引导 MySQL 使用我们期望的索引。比如把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。 第三种方法是，在有些场景下，可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 如果确定是索引根本没必要，可以考虑删除索引。 InnoDB 数据页结构? # File Header：表示页的一些通用信息，占固定的38字节。 page Header：表示数据页专有信息，占固定的56字节。 inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的26字节。 User Records：真正存储我们插入的数据，大小不固定。 Free Space：页中尚未使用的部分，大小不固定。 Page Directory：页中某些记录的相对位置，也就是各个槽对应的记录在页面中的地址偏移量。 File Trailer：用于检验页是否完整，占固定大小 8 字节。 MySQL 是如何保证数据不丢失的？ # 只要redolog 和 binlog 保证持久化磁盘就能确保MySQL异常重启后回复数据 在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog事务是否成功，决定是回滚还是执行。 误删数据怎么办？ # DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的 myflash，也是一个不错的工具，本质都差不多 都是先解析 binlog event，然后在进行反转。把 delete 反转为insert，insert 反转为 delete，update前后 image 对调。 所以必须设置binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上 DDL语句误操作(truncate和drop)，由于DDL语句不管 binlog_format 是 row 还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。 只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长 rm 删除：使用备份跨机房，或者最好是跨城市保存。 drop、truncate 和 delete 的区别 # DELETE 语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 drop语句将表所占用的空间全释放掉。 在速度上，一般来说，drop\u0026gt; truncate \u0026gt; delete。 如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大； 如果想删除表，当然用 drop；如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可； 如果和事务有关，或者想触发 trigger，还是用 delete；如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据。 在 MySQL 中有两个 kill 命令? # 一个是 kill query + 线程 id，表示终止这个线程中正在执行的语句 一个是 kill connection + 线程 id，这里 connection 可缺省，表示断开这个线程的连接 kill 不掉的原因 kill命令被堵了，还没到位 kill命令到位了，但是没被立刻触发 kill命令被触发了，但执行完也需要时间 如何理解 MySQL 的边读边发 # 如果客户端接受慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间会很长。 服务端并不需要保存一个完整的结果集，取数据和发数据的流程都是通过一个 next_buffer 来操作的。 内存的数据页都是在 Buffer_Pool中操作的。 InnoDB 管理 Buffer_Pool 使用的是改进的 LRU 算法，使用链表实现，实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。 MySQL 的大表查询为什么不会爆内存？ # 由于 MySQL 是边读变发，因此对于数据量很大的查询结果来说，不会再 server 端保存完整的结果集，所以，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆。 InnoDB 引擎内部，由于有淘汰策略，InnoDB 管理 Buffer_Pool 使用的是改进的 LRU 算法，使用链表实现，实现上，按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域。对冷数据的全扫描，影响也能做到可控制。 MySQL 临时表的用法和特性? # 只对当前session可见。 可以与普通表重名。 增删改查用的是临时表。 show tables 不显示普通表。 在实际应用中，临时表一般用于处理比较复杂的计算逻辑。 由于临时表是每个线程自己可见的，所以不需要考虑多个线程执行同一个处理时临时表的重名问题，在线程退出的时候，临时表会自动删除。 使用union union all联合查询会产生临时表 order by 和group by 子句不一样时产生临时表 distinct 查询并且加上order by 时参数临时表 from中子查询产生临时表 sql优化经验 # 1.where 条件左侧 不要使用函数或表达式 2.使用explain命令优化select查询 3.只查询一条记录时，使用limit 1 4.不要直接用select* ,而是明确查询字段。 5.为每一个表设置一个id属性 6.避免在where条件进行NULL判断 7.避免where中使用！或\u0026gt; 8.使用between and代替in 9.为搜索字段建立索引 10.like左模糊查询不会走索引，like右模糊会走索引 11.选择合适的字段，尽可能小，尽可能定长，尽可能整数 12.字段设计尽可能使用not null MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）? # InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎 MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 Web、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎 MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问 都说 InnoDB 好，那还要不要使用 MEMORY 引擎？ # 内存表就是使用 memory 引擎创建的表 为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。 由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉。 如果数据库误操作, 如何执行数据恢复? # 数据库在某个时候误操作，就可以找到距离误操作最近的时间节点的bin log，重放到临时数据库里，然后选择误删的数据节点，恢复到线上数据库。 MySQL 是如何保证主备同步？ # 主备关系的建立： 一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从binlog文件A的位置P”开始同步，主库就从这个指定的位置开始往后发。 而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。 MySQL 主备切换流程： 客户端读写都是直接访问A，而节点B是备库，只要将A的更新都同步过来，到本地执行就可以保证数据是相同的。 当需要切换的时候就把节点换一下，A的节点B的备库 一个事务完整的同步过程： 备库B和主库A建立来了长链接，主库A内部专门线程用于维护了这个长链接。 在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求binlog包括文件名和日志偏移量 在备库B上执行start-slave命令备库会启动两个线程：io_thread和sql_thread分别负责建立连接和读取中转日志进行解析执行 备库读取主库传过来的binlog文件备库收到文件写到本地成为中转日志 后来由于多线程复制方案的引入，sql_thread演化成了多个线程。 什么是主备延迟? # 主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有： 有些部署条件下，备库所在机器的性能要比主库性能差。 备库的压力较大。 大事务，一个主库上语句执行10分钟，那么这个事务可能会导致从库延迟10分钟。 为什么要有多线程复制策略？ # 因为单线程复制的能力全面低于多线程复制，对于更新压力较大的主库，备库可能是一直追不上主库的，带来的现象就是备库上seconds_behind_master值越来越大。 在实际应用中，建议使用可靠性优先策略，减少主备延迟，提升系统可用性，尽量减少大事务操作，把大事务拆分小事务。 MySQL 的并行策略有哪些？ # 按表分发策略：如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个 worker 不会更新同一行。缺点：如果碰到热点表，比如所有的更新事务都会涉及到某一个表的时候，所有事务都会被分配到同一个 worker 中，就变成单线程复制了。 按行分发策略：如果两个事务没有更新相同的行，它们在备库上可以并行。如果两个事务没有更新相同的行，它们在备库上可以并行执行。显然，这个模式要求 binlog 格式必须是 row。缺点：相比于按表并行分发策略，按行并行策略在决定线程分发的时候，需要消耗更多的计算资源 MySQL的一主一备和一主多从有什么区别？ # 在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。 主库出问题如何解决? # 基于位点的主备切换：存在找同步位点这个问题 MySQL 5.6 版本引入了 GTID，彻底解决了这个困难。那么，GTID 到底是什么意思，又是如何解决找同步位点这个问题呢？ GTID：全局事务 ID，是一个事务在提交的时候生成的，是这个事务的唯一标识；它由两部分组成，格式是：GTID=server_uuid:gno 每个 MySQL 实例都维护了一个 GTID 集合，用来对应“这个实例执行过的所有事务”。 在基于 GTID 的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。因此，如果实例 B 需要的日志已经不存在，A’就拒绝把日志发给 B。 MySQL的并发链接和并发查询有什么区别？ # 在执行show processlist的结果里，看到了几千个连接，指的是并发连接。而\u0026#34;当前正在执行\u0026#34;的语句，才是并发查询。 并发连接数多影响的是内存，并发查询太高对CPU不利。一个机器的CPU核数有限，线程全冲进来，上下文切换的成本就会太高。 所以需要设置参数：innodb_thread_concurrency 用来限制线程数，当线程数达到该参数，InnoDB就会认为线程数用完了，会阻止其他语句进入引擎执行。 、短时间提高 MySQL 性能的方法? # 第一种方法：先处理掉那些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。kill connection + id 第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL 选错了索引（force index） 为什么 MySQL 自增主键 ID 不连续？ # 唯一键冲突 事务回滚 自增主键的批量申请 深层次原因是：MySQL 不判断自增主键是否存在，从而减少加锁的时间范围和粒度，这样能保持更高的性能，确保自增主键不能回退，所以才有自增主键不连续。 自增主键怎么做到唯一性？自增值加1来通过自增锁控制并发。 InnoDB 为什么要用自增 ID 作为主键？ # 自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。 每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。 而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机 因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。 如何最快的复制一张表？ # 为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表 一种方法是，使用 mysqldump 命令将数据导出成一组 INSERT 语句 另一种方法是直接将结果导出成.csv 文件。MySQL 提供语法，用来将查询结果导出到服务端本地目录：select * from db1.t where a\u0026gt;900 into outfile \u0026#39;/server_tmp/t.csv\u0026#39;;得到.csv 导出文件后，你就可以用下面的 load data 命令将数据导入到目标表 db2.t 中：load data infile \u0026#39;/server_tmp/t.csv\u0026#39; into table db2.t; 物理拷贝：在 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 grant 和 flush privileges语句? # grant语句会同时修改数据表和内存，判断权限的时候使用的内存数据，因此，规范使用是不需要加上 flush privileges 语句。 flush privileges 语句本身会用数据表的数据重建一份内存权限数据，所以在权限数据可能存在不一致的情况下再使用。 要不要使用分区表？ # 分区并不是越细越好。实际上，单表或者单分区的数据一千万行，只要没有特别大的索引，对于现在的硬件能力来说都已经是小表了。 分区也不要提前预留太多，在使用之前预先创建即可。比如，如果是按月分区，每年年底时再把下一年度的 12 个新分区创建上即可。对于没有数据的历史分区，要及时的 drop 掉。 join 用法? # 使用 left join 左边的表不一定是驱动表 如果需要 left join 的语义，就不能把被驱动表的字段放在 where 条件里面做等值判断或不等值判断，必须都写在 on 里面 标准的 group by 语句，是需要在 select 部分加一个聚合函数，比如select a,count(*) from t group by a order by null; MySQL 的锁? # 锁解决两个问题：事务隔离，保证多用户环境下数据库完整性和一致性 表级锁 1.开销小，加锁块不会出现死锁，锁颗粒度大，发送锁冲突的概率最高，并发度最低 2. 什么时候触发表锁？修改表结构时，批量删除 行锁 1.开销大，加锁慢，会出现死锁；锁颗粒度最小，发生锁冲突的概率最低，并发度也最高 2.MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住 页面锁 1. 开锁和加锁时间介于表锁和行锁之间，会出现死锁，并发一般 mysql 什么情况触发表锁，什么情况触发行锁 # 表锁： 修改表结构 LOCK TABLES语句显式锁定表，比如数据库备份和恢复操作 行锁： 事务对某一行数据进行更新 事务对某一行数据进行读取 InnoDB 事务四个隔离级别？ # RU :Read uncommitted：(读未提交)：最低级别，任何情况都无法保证。可能产生脏读，不可重复读，幻读． RC :Read committed : (读已提交)：可避免脏读的发生。但是会导致不可重复读 RR :Repeatable read: (可重复读)即在一个事务读取数据的过程中，其他事务不允许修改数据，所以解决了不可重复读．解决不可重复读，但是插入时会导致幻读。 S :Serializable (串行化)：最高的隔离级别,某一时刻只能有一个事务操作数据库 脏读 不可重复读 幻读 # 1.脏读: ：事务A读取了事务B在此过程中修改了的该数据，但没有提交，此时事务A读取的可能是脏数据，一旦事务B回滚，事务A便是脏读． 2.不可重复读：事务A需要在此次事务中多次读取同一个数据，在此期间，事务B修改了该数据并提交，导致事务A多次读取的数据不一样，因此称为不可重复读． 3.幻读：一个事务因读取到另一个事务已提交的insert数据。导致对同一张表读取两次以上的结果不一致 什么是事务 # 原子性（Atomicity）：事务最小工作单元，要么全成功，要么全失败 。 一致性：（Consistency）：事务开始和结束后，数据库的完整性不会被破坏 。 隔离性：（Isolation）：不同事务之间互不影响，四种隔离级别为RU（读未提交）、RC（读已提交）、RR（可重复读）、SERIALIZABLE （串行化） 持久性：（Durability）：事务提交后，对数据的修改是永久性的，即使系统故障也不会丢失 。 死锁 乐观锁 悲观锁 共享锁 排他锁？ # // 死锁 因相互竞争，而造成相互等待的现象，无外力作用，它们无法进行下去，使用共享锁，排他锁解决 //乐观锁 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性 //悲观锁 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作 每次拿数据的时候都认为别人会修改，所以每次拿数据时都会上锁 //共享锁 执行语句后 加上 lock in share mode 对于多个不同的事务，对同一资源共享同一个锁（一个门上一把锁，多个钥匙），只用于select，对于update,insert,delete语句会自动加排它锁 //排他锁 串行化隔离 就行排他锁 对于多个不同的事务，对同一个资源只能有一把锁。一把锁 死锁产生的4个必要条件？ # 1.互斥条件:某段时间内某资源只有一个进程占用，其他进程请求资源只能等待，直到被占资源得到释放 2.请求和保留条件：自己保持了一个资源，请求新的被别进程占用的资源，而阻塞，但是不释放自己保持的资源 3.不剥夺条件：自己保持的资源，未使用完成前，不可剥夺，只能自己释放 4.环路等待条件：发生死锁时，必然存在一个进程占用资源的环形链 什么时候使用乐观锁 悲观锁？ # 乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量 但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁 为什么 MySQL 会抖一下？ # 脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。 char varchar 区别？ # char 最大长度为255字节，定长，使用空格填充，查找效率高 varchar字段的最大长度为65535个字节，不定长，查找效率低 为什么删除了表，表文件的大小还是没变？ # 数据项删除之后 InnoDB 某个页 page A 会被标记为可复用。 delete 命令把整个表的数据删除，结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。 经过大量增删改的表，都是可能是存在空洞的。这些空洞也占空间所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。 重建表，就可以达到这样的目的。可以使用 alter table A engine=InnoDB 命令来重建表 count(*)实现方式以及各种 count 对比? # 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 对于 count(字段) 来说：如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。 但是 count * 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 所以结论是：按照效率排序的话，count(字段)\u0026lt;count(主键 id)\u0026lt;count(1)≈count(*)，所以建议尽量使用 count(*)。 百万级别或以上的数据如何删除 # 对数据进行 增加 删除 修改时 会产生额外对索引文件的操作，这会消耗I/O,降低增删改的效率 1.先删除索引 2.删除无用数据 3.重新创建索引 什么是最左匹配原则 # 创建多列索引时，where子句使用频繁的一列放到最左边 百万级别或以上的数据，你是如何删除的？ 我们想要删除百万数据的时候可以先删除索引 然后批量删除其中无用数据 删除完成后重新创建索引。\n什么是最左前缀原则？什么是最左匹配原则？ 最左前缀原则，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 当我们创建一个组合索引的时候，如(k1,k2,k3)，相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。。\n100亿的数据找出前10大的数，内存有限 # 数据分割成适当大小的块，然后对每个块进行排序 用过gorm，如果一张上百万的数据的表，要新建一个字段的索引，如何保证线上的服务尽量少的被影响 # 在非高峰期进行操作,比如深夜 分批次添加索引： 监控数据库性能： 预先测试： # "},{"id":5,"href":"/docs/docs/interview/redis/","title":"Redis","section":"面试题集","content":" Redis高性能的 key-value 数据库 # redis特点 # 1.Redis 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时 候可以再次加载进行使用。 2.Redis 不仅仅支持简单的 key-value 类型的数据，同时还提供 list， set，zset，hash 等数据结构的存储。 3.Redis 支持数据的备份，即 master-slave 模式的数据备份。 4.性能极高 – Redis 能读的速度是 110000 次/s,写的速度是 81000 次/s 5.原子 – Redis 的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性， 通过 MULTI 和 EXEC 指令包起来。 redis数据结构，用途 # string（字符串），hash（哈希），list（列表），set（集合）及 zsetsorted set：有序集合。 1.String：用于存储字符串值，常用于缓存、计数器、分布式锁等场景。 2.Hash：用于存储键值对集合，常用于存储对象的属性，如用户信息、商品信息等。 3.List：用于存储有序的字符串元素列表，常用于消息队列、栈等场景。 4.Set：用于存储字符串元素的无序集合，常用于去重、共同好友等场景。 5.Sorted Set：用于存储字符串元素及其分数的有序集合，常用于排行榜、范围查询等场景。 Redis 是单进程单线程的？ # Redis v6之前是单进程单线程的，原因是CPU 通常不会成为性能瓶颈，瓶颈往往是内存和网络，因此单线程足够了，redis 利用队列技术将并发访问变为串行访问，消 除了传统数据库串行控制的开销。 //好处： 1.避免过多的上下文切换开销 2.避免同步机制的开销 3.简单可维护：引入多线程必须的同步机制，那么所有的底层数据结构都必须实现成线程安全的，这无疑又使得 Redis 的实现变得更加复杂。 v6之后(多线程网络模型)Redis在6.0支持的多线程，并不是说指令操作的多线程，而是针对网络IO的多线程支持 针对网络IO的处理方式改成了多线程，通过多线程并行的方式提升了网络IO的处理效率。 但是对于客户端指令的执行过程，还是使用单线程方式来执行。 Redis 多线程是怎么做到无锁的？ # redis 多线程的模型是主线程负责搜集任务，放入全局读队列 clients_pending_read 和全局写队列 clients_pending_write，主线程在将队列中的任务以轮训的方式分发到每个线程对应的队列（list *io_threads_list[128]） 主线程将任务分发到子线程的队列中,等待所有子线程处理完所有任务,继续收集任务到全局队列.这样就避免了主线程和子线程同时访问队列的情况，主线程向队列写的时候子线程还没开始消费，子线程在消费的时候主线程在等待子线程消费完，子线程消费完后主线程才会往队列中继续写，就不用加锁了 1. 主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列； 2. 主线程通过轮询将可读 socket 分配给 IO 线程； 3. 主线程阻塞等待 IO 线程读取 socket 完成； 4. 主线程执行 IO 线程读取和解析出来的 Redis 请求命令； 5. 主线程阻塞等待 IO 线程将指令执行结果回写回 socket完毕； 6. 主线程清空全局队列，等待客户端后续的请求 redis 一个字符串类型的值能存储最大容量是多少？ # 512M Redis 的持久化机制是什么？各自的优缺点？ # RDB:只有一个文件 dump.rdb，某段时间间隔数据快照存储在磁盘，父进程判断需要save时，fork（父进程阻塞）一个子进程生成rdb文件，写入。 1.数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 Redis 发生 故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候 AOF:以追加的方式记录每一次redis写的操作，当服务器重启时,重新执行这些命令恢复原始数据, 1.数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行 一次命令操作就记录到 aof 文件中一次。 2.通过 append 模式写文件，即使中途服务器宕机，可以通过 redischeck-aof 工具解决数据一致性问题。 AOF 文件比 RDB 文件大，且恢复速度慢。 数据集大的时候，比 rdb 启动效率低。 Redis 常见性能问题和解决方案 # 1.Master 最好不要写内存快照，如果 Master 写内存快照，save 命令调度rdbSave 函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大 的，会间断性暂停服务 2.如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次 3.为了主从复制的速度和连接的稳定性，Master 和 Slave 最好在同一个局域网 redis 过期减删除策略？ # 1.定时删除:设置键的同时 设置过期时间，由定时器执行删除 2.惰性删除：取值的时候，检查是否过期，再删除 3.定期删除：每隔一段时间对数据库检查一次，删除过期键，算法决定删除多少键，和什么数据库 为什么redis需要把所有数据放到内存中？ # Redis 为了达到最快的读写速度将数据都读到内存中，并通过异步的方式 将数 据写入磁盘。所以 Redis 具有快速和数据持久化的特征,如果不将数据放在 内存中，磁盘 I/O 速度为严重影响 Redis 的性能, redis同步机制了解吗？ # Redis 可以使用主从同步，从从同步。第一次同步时，主节点做一次 bgsave， 并同时将后续修改操作记录到内存 buffer，待完成后将 rdb 文件全 量同步到 复制节点，复制节点接受完成后将 rdb 镜像加载到内存。加载完成 后，再通 知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 redis淘汰策略 # volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选 最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选 将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任 意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘 汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 redis 应用场景？ # 会话缓存（Session Cache） 全页缓存（FPC） 队列 排行榜/计数器 发布/订阅 假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？ # 使用 keys 指令可以扫出指定模式的 key 列表。对方接着追问：如果这个 Redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？ 这个时候你要回答 Redis 关键的一个特性：Redis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢 复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整 体所花费的时间会比直接用 keys 指令长。 如果有大量的 key 需要设置同一时间过期，一般需要注意什么？ # 如果大量的 key 过期时间设置的过于集中，到过期的那个时间点，Redis 可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间 分散 一些。 使用过 Redis 分布式锁么，它是什么回事 # 1.先拿 setnx 来争抢锁，抢到之后，再用 expire 给锁加一个过期时间防止锁忘 记了释放。setnx和expire不是原子操作,一旦redis宕机，expire没有设置成功，锁就无法释放, 2.2.6.12版本后set可以实现setnx和expire这是原子操作 redis和mysql一致性 # //先更新mysql 再删除redis 更新时，先更新mysql,然后删除redis,查询时 no cache,查询mysql,然后更新redis 1.问题：c1更新mysql,删除 redis,c3查询redis,查询mysql,c2更新mysql,删除redis,c3同步redis结论是redis保存的是c1的数据， 不是最新的 2.在删除redis时，其他请求拿到的还是旧数据 //延时双删 为了保证最终redis是最新的数据 更新时，先删除redis,再更新mysql,延迟删除redis 缓存可能出现的问题 # 数据不一致 缓存雪崩 缓存穿透 缓存并发竞争 缓存击穿（某热点数据失效） # //原因 高并发流量，访问的这个数据是热点数据，请求的数据在 DB 中存在，但是 Redis 存的那一份已经过期，后端需要从 DB 从加载数据并写到 Redis。 //解决 过期时间 + 随机值（过期时间=baes 时间+随机时间）：让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力 预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁，先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间在重试。 这样只让一个请求去数据库读取数据。 缓存穿透(redis mysql都不存在数据) # //原因 意味着有特殊请求在查询一个不存在的数据，即数据不存在 Redis 也不存在于数据库。 //解决 缓存空值：当请求的数据不存在 Redis 也不存在数据库的时候，设置一个缺省值（比如：None）。当后续再次进行查询则直接返回空值或者缺省值。 布隆过滤器：在数据写入数据库的同时将这个 ID 同步到到布隆过滤器中，当请求的 id 不存在布隆过滤器中则说明该请求查询的数据一定没有在数据库中保存，就不要去数据库查询了。 (布隆过滤器：分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0。添加：key经过多个hash组函数计算，的k位置置为1.判断key是否存在时，计算多个hansh组函数的k位置是否都是1) 缓存雪崩（大量数据同时失效） # //原因 缓存雪崩指的是大量的请求无法在 Redis 缓存系统中处理，请求全部打到数据库，导致数据库压力激增，甚至宕机。 大量数据同时过期，导致大量请求需要查询数据库并写到缓存； Redis 故障宕机，缓存系统异常。 //解决 过期时间添加随机值，要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟） 接口限流 一个 Redis 实例能支撑 10 万的 QPS，而一个数据库实例只有 1000 QPS。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 缓存预热 # 缓存没有数据时，预先把热点数据加载好。 redis分布式锁问题 # redis是单线程执行，当处理请求A时，请求B也可以得到响应，并被添加到队列中等待执行。 加锁问题：A加锁+过期时间，但是A在内部执行过长或者其他问题，在redis过期时间后，还没执行完毕，redis会自动删除key。这时B发现没锁，申请加锁+过期时间，B在内部执行中，A完毕后要释放锁会释放B的锁的问题：这时就需要使用luaScript脚本，找到key和对应的A设置的value,匹配才能删除。否则不能删除。 A在加锁后的过期时间后，redis删除了key,此时A还在处理共享资源，B加锁也进入共享资源内，资源不能互斥问题：此时需要第一个加锁的在过期时间要删除时，用守护进程自动延期时间。相关代码github.com/jefferyjob/go-redislock set底层实现 # Set 数据结构的底层实现通常是通过哈希表（Hash Table）来实现,哈希表的值则被设置为一个固定的常量（比如 NULL） "},{"id":6,"href":"/docs/docs/interview/%E6%B7%B1%E5%B1%82%E6%8A%80%E6%9C%AF/","title":"深层技术","section":"面试题集","content":" 高并发访问时避免冲突的方法? # 程序内使用互斥锁保护共享资源 使用事务来确保一系列操作的原子性，从而避免数据不一致或冲突。 分布式锁来保护共享资源，确保在不同节点上的并发访问不会产生冲突。 并发访问转化为顺序访问，通过消息队列等方式来串行化处理请求，避免并发冲突。 缓存来减轻数据库或其他资源的压力，通过缓存有效地降低并发访问对共享资源的冲突 并发与并行的区别 # 并发:系统中同时具有多个活动的部分，并且这些活动在一段时间内都在进行 并行:系统中同时具有多个活动的部分，并且这些活动在同一时刻进行 并发来提高系统的吞吐量和响应性，而通过并行来提高系统的计算能力。 用户从客户端访问一个页面，webserver如何主动的给这个页面推送一个通知 # 使用WebSocket，当用户访问一个页面时，前端给后端发送一个请求带上用户信息，后端接收到后，使用用户id建立一个WebSocket,前后端建立WebSocket后，后端给用户推送一个通知 "},{"id":7,"href":"/docs/docs/interview/%E7%AE%97%E6%B3%95/","title":"算法","section":"面试题集","content":" 1.斐波那契数列 Fibonacci,F(0)=1，F(1)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 2，n ∈ N*） # func Fibonacci(n int) int { if n \u0026lt;= 2 { return 1 } return Fibonacci(n-1) + Fibonacci(n-2) } func Fibonacci(n int) int { if n \u0026lt;= 2 { return 1 } sum := 0 n1 := 0 n2 := 1 for i := 1; i \u0026lt; n; i++ { sum = n1 + n2 n1 = n2 n2 = sum } return sum } "},{"id":8,"href":"/docs/docs/interview/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","title":"网络编程","section":"面试题集","content":" OSI七层和TCP/IP四层？ # 第七层：应用层（Telnet,HTTP80,FTP,TFTP,NFS,SMTP,SNMP,DNS） 第六层：表示层 (定义数据格式、加密、代码转换) 第五层：会话层 (开始、控制和结束一个会话,接收数据何时结束) 第四次：传输层 (TCP，UDP) 第三层：网络层 (包传输进行定义IP、ICMP、IGMP、RIP、OSPF、BGP) 第二层：数据传输层 (单个链路上如何传输数据SLIP、CSLIP、PPP、ARP、RARP、MTU) 第一层：物理层 (传输介质) 第四层：应用层 第三层：传输层 第二层：应用层 第一层：网络接口层 http状态码 # // 2** 请求成功 200 请求成功执行。 201 (已创建)请求成功并且服务器创建了新的资源。 202 （已接受） 服务器已接受请求，但尚未处理 203 （非授权信息） 204 （无内容） 服务器成功处理了请求，但没有返回任何内容。 // 3** 重定向 301 （永久移动） 请求的网页已永久移动到新位置。 //4** 客户端错误 400 客户端错误。 401 （未授权） 请求要求身份验证。 403 （禁止访问） 服务器拒绝请求。 404 （未找到） 资源找不到，可以是 URL 或 RESTful 资源。 405 （方法禁用） 禁用请求中指定的方法。 408 （请求超时） 服务器等候请求时发生超时。 414 （请求的 URI 过长） // 5** 服务器错误 500 （服务器内部错误） 服务器遇到错误，无法完成请求。 501 （尚未实施） 服务器不具备完成请求的功能。 502 （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 503 （服务不可用） 服务器目前无法使用 504 （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。 linux进程通信方式？ # 管道，有名管道 信号 消息队列 共享内存 信号量 套接字 socket 进程、线程、协程的区别 # 一般一个程序就是一个进程，如果一个进程不总是执行计算型任务，会导致cpu浪费，在cpu少的情况下是通过进程不停的上下文切换，实现同时运行多个进程。 进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。一个进程由一个或多个线程组成。 线程是一个进程中代码的不同执行路线；拥有自己独立的栈和共享的堆，进程间相互独立，但同一个进程内的各个线程之间共享着同样的代码和全局数据；多线程比多进程之间更容易共享数据，在上下文切换中线程一般比进程更高效。 协程是比线程更加轻量级的存在，由程序员自己写程序管理的轻量级线程。创建协程时，会从进程的堆中分配一段内存作为协程的栈。线程的栈一般是M级别，协程为K级别，go协程更夸张到2到4KB。 进程资源分配的基本单位，线程是执行的单位，多个线程共享进程资源。进程需要特殊的机制，管道，消息队列等。 线程之间可以通过共享内存方式通信，线程间通信相对简单。 线程切换开销较小，因为线程共享进程的资源，切换时不需要切换内存空间。 协程是用户态的轻量级线程，调度完全用户控制，协程的创建，切换销毁都在用户空间完成，不需要内核的参与，开销非常小。协程用于处理IO密集以及异步非阻塞的代码 可以将一个浏览器视为一个进程，每个打开的标签页或窗口视为一个线程，而在每个线程内部可以使用多个协程来处理不同的任务。这种比喻可以帮助理解进程、线程和协程之间的关系，以及它们在并发编程中的作用。 开多个线程和开多个协程会有什么区别 # 调度方式：线程的调度由操作系统内核实现，采用抢占式调度方式，依赖各种锁来确保线程安全。而协程的调度则由用户态的具体调度器进行，不需要内核的参与。 协程的切换不需要经过用户态与内核态的切换，且切换时只需要保存极少的状态值,线程的切换涉及到内核态的上下文切换，开销相对较大。 资源占用：协程属于用户态轻量级的线程，占用的内存较少。而线程需要操作系统为其分配独立的栈空间和其他系统资源，因此资源占用相对较多。 并发能力：协程依托于线程，可以在单线程中实现高并发。多个协程可由一个或多个线程管理，协程的调度发生在其所在的线程中。而线程的数量受限于系统的物理CPU核心数，因此并发能力相对受限。 协程的优势？ # 1.节省cpu：避免系统内核级的线程频繁切换，造成cpu资源浪费。协程是用户态的线程，程序员可以自行控制协程的创建和销毁，极大程度避免系统级线程上下文切换造成的资源浪费 2.节约内存：在64位linux中，一个线程需要分配8M栈内存和64M堆内存，而协程模式下可以轻松创建几十万协程。实现高并发 3.稳定性：线程之间通过内存共享数据，这样导致了任何一个线程出错，进程的所有线程都会跟踪崩溃 4.开发效率：使用协程开发程序中，可以很方便的将耗时的IO操作异步化。 协程的数据结构是什么，保存了哪些东西 # 1. 栈（Stack）：用于保存协程执行时的局部变量、函数调用信息等。每个协程都有自己的栈空间，用于存储执行时所需的数据。 2. 指令指针（PC）：用于指示当前协程执行的位置，即下一条要执行的指令在代码中的位置。 3. 协程状态（Status）：用于标识协程的状态，比如运行、阻塞、就绪等。 4. 栈的大小（Stack Size）：用于记录协程栈的大小，以便在需要时进行栈的扩容或缩减。 5. 上下文信息（Context）：包括协程的调度信息、运行时环境等。 进程五种状态 # 创建、就绪、运行、终止、阻塞 软连接和硬链接区别 # ln 硬链接：ln命令不能对目录创建硬链接, ln -s 软连接：一个快捷方式，删除软链接文件,对源文件及硬链接文件无任何影响 tcp三次握手 # 第一次握手（SYN）： 客户端发送一个特殊的 TCP 报文段，其中 SYN（同步）标志位被置为 1，表示客户端请求建立连接。 客户端会选择一个初始序列号（假设为 client_num），并将此编号放置在序号字段中。 这个报文段会被封装在一个 IP 数据报中发送给服务器。 第二次握手（SYN-ACK）： 服务器接收到客户端发送的 SYN 报文段后，会为该 TCP 连接分配缓存和变量，并发送允许连接的确认报文段 在允许连接的报文中，SYN 标志位仍被置为 1，确认号字段填的是 client_num + 1 的值。 服务器也会选取一个 server_num 存放到序号字段中，这个报文段称为 SYN-ACK 报文段 第三次握手（ACK）： 客户端接收到服务器发送的 SYN-ACK 报文段后，最后也要向服务器发送一个确认报文段。 这个报文段和前两个不一样，SYN 标志位置 0，在确认号字段中填上 server_num + 1 的值，并且这个报文段可以携带数据。 tcp四次挥手 # 客户端 TCP 会向服务器发送一个特殊的报文段，该报文段的 FIN 标志位会被置 1 服务器会向客户端发送一个确认报文段。 然后服务器也会客户端发送一个 FIN 标志位为 1 的终止报文段 随后客户端回送一个确认报文段，服务器立即断开连接 为什么是四次？ # TCP 协议是全双工的，也就是说客户端和服务端都可以发起断开连接。两边各发起一次断开连接的申请，加上各自的两次确认，看起来就像执行了四次挥手。 time_wait? # 客户端最后向服务器发送的确认 ACK 是有可能丢失的，当出现超时，服务端会再次发送 FIN 报文段，如果客户端已经关闭了就收不到了 close_wait # 大量 CLOSE_WAIT 表示程序出现了问题，对方的 socket 已经关闭连接，而我 方忙于读或写没有及时关闭连接 网络编程的一般步骤 # //对于TCP连接： Server端：create -- bind -- listen-- accept-- recv/send-- close Client端：create------- conncet------send/recv------close. //对于UDP连接 Server端：create----bind ----recvfrom/sendto----close Client端：create---- sendto/recvfrom----close 滑动窗口机制 # 确立收发的边界，能让发送方知道已经发送了多少（已确认）、尚未确认的字节数、尚待发送的字节数；让接收方知道（已经确认收到的字节数） http协议 # http1.0 ：短链接，每次请求结束后，就会断开tcp连接 http1.1:默认持久连接，只要客户端服务端没有断开tcp连接，就一直保持连接，可以发送多次HTTP请求。 http1.1有一个管线化理论，默认是关闭的，管线化是串行的，即一个响应必须完全返回后，下一个请求才会开始传输。 http1.1 支持断点续传，利用http消息头使用分块传输编码，将实体主体分块进行传输 http/2:不再以文本的方式传输，采用「二进制分帧层」，，对头部进行了「压缩」，支持「流控」，最主要就是HTTP/2是支持「多路复用」的（通过单一的TCP连接「并行」发起多个的请求和响应消息） HTTP/2多路复用则是利用「分帧」数据流，把HTTP协议分解为「互不依赖」的帧（为每个帧「标序」发送，接收回来的时候按序重组），进而可以「乱序」发送避免「一定程度上」的队首阻塞问题 线头阻塞 # 无论是HTTP1.1还是HTTP/2，response响应的「处理顺序」总是需要跟request请求顺序保持一致的。假如某个请求的response响应慢了，还是同样会有阻塞;受限于HTTP底层的传输协议是TCP,没办法完全解决 websocket 和 http 有什么区别？ # HTTP（超文本传输协议）：无状态的，每次请求都需要建立连接、发送请求、接收响应，然后关闭连接。通常是由客户端发起，服务器响应后即刻关闭连接，因此无法实现服务器主动向客户端推送数据。 WebSocket：全双工通信协议，允许客户端和服务器之间进行双向通信，实现了服务器主动向客户端推送数据的功能。保持长连接，双方可以随时发送和接收数据，而无需频繁地建立和关闭连接。通信开销更小，因为它使用较小的头部信息，并且避免了建立和关闭连接的开销。 RPC和HTTP请求有什么区别？ # RPC是远程调用另一台计算机（服务器）上的程序或函数。HTTP是一种网络协议，用于在互联网上传输各种类型的数据，最常用于传输Web页面和相关资源。 RPC跨越了传输层和应用层，而HTTP基于TCP/IP协议进行传输。 RPC支持同步和异步调用方式，可以更加灵活地处理并发请求。HTTP的请求响应通常是同步的 RPC请求的数据封装相对简洁，只包含必要的信息。HTTP请求需要封装在HTTP协议中，包含请求行、请求头、请求体等部分。 RPC通常与编程语言相关，需要服务调用方和服务提供方使用统一的RPC框架。HTTP则与编程语言无关，只需要遵循REST规范即可。 服务调用服务有哪些方式 # RESTful API,RPC调用,消息队列,WebSocket,服务网关 "},{"id":9,"href":"/docs/docs/interview/","title":"面试题集","section":"Docs","content":" 面试题集 # "},{"id":10,"href":"/docs/docs/2_%E5%B8%B8%E8%AF%86/","title":"常识","section":"Docs","content":" 数学 # 百分比 # 假设比如公司共有 13人，男的8个人，女的5个人。求男女各占总数的百分比。 男占公司员工比例8/13*100%=61.5%。 女占公司员工比例5/13*100%=38.5%(四舍五入)。 百分数化小数：去掉百分号，小数点左移两位。如：75%可化为0.75 小数化百分数：加上百分号，小数点右移两位。如：0.62可化为62% 计量单位 # 公斤=kg 1斤=500g 1斤=10两 1 吨 [t] = 2000 斤 1 吨 [t] = 1000 千克 [kg] 公里=km 1里=500米 1公里(km) = 1千米(km) "},{"id":11,"href":"/docs/docs/3_%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/","title":"各种工具安装","section":"Docs","content":" 彻底卸载禁用snap # sudo vim /etc/apt/preferences.d/nosnap.pref https://www.cnblogs.com/learner-and-helper-YZY/p/17654961.html oh-my-zsh # sudo apt install zsh -y sh -c \u0026#34;$(wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; git # 配置github用户 git config --global user.name \u0026#34;youname\u0026#34; git config --global user.email \u0026#34;youname@qq.com\u0026#34; 配置ssh key ssh-keygen -m PEM -t ed25519 -C \u0026#34;yourname@qq.com\u0026#34; # id_ed25519.pub内容 粘贴到远端github的setting-\u0026gt;SSH and GPG keys下 ssh -T git@github.com 配置github和公司仓库地址 cd ~/.ssh \u0026amp;\u0026amp; vim config Host github.com Hostname github.com Port 22 User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_github Host e.coding.net Hostname e.coding.net Port 22 User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_ed25519 node npm pnpm # # 安装nvm,重启终端 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash nvm ls-remote nvm install v20.10.0 nvm use v20.10.0 npm install -g pnpm npm源管理 # npm install -g nrm open@8.4.2 --save nrm use cnpm eslint # # 项目目录下 pnpm i eslint eslint-plugin-vue@latest @typescript-eslint/parser@latest @typescript-eslint/eslint-plugin@latest -D vscode安装eslint插件的配置 \u0026#34;eslint.validate\u0026#34;: [ \u0026#34;javascript\u0026#34;, \u0026#34;javascriptreact\u0026#34;, \u0026#34;vue\u0026#34;, \u0026#34;typescript\u0026#34; ], \u0026#34;eslint.format.enable\u0026#34;: true, \u0026#34;editor.codeActionsOnSave\u0026#34;: { \u0026#34;source.fixAll\u0026#34;: true, \u0026#34;eslint.autoFixOnSave\u0026#34;: true, }, .eslintrc.js(根目录下创建) module.exports = { parser: \u0026#39;vue-eslint-parser\u0026#39;, parserOptions: { parser: \u0026#39;@typescript-eslint/parser\u0026#39;, ecmaVersion: 2020 }, extends: [ \u0026#39;plugin:vue/vue3-recommended\u0026#39;, \u0026#39;plugin:@typescript-eslint/recommended\u0026#39; ], rules: { quotes: [\u0026#39;error\u0026#39;, \u0026#39;single\u0026#39;, { \u0026#39;allowTemplateLiterals\u0026#39;: true }],// 强制使用一致的反勾号、双引号或单引号 eqeqeq: \u0026#39;off\u0026#39;, // 要求使用 === 和 !== semi: [\u0026#39;error\u0026#39;, \u0026#39;never\u0026#39;], // 要求或禁止使用分号代替 ASI \u0026#39;max-len\u0026#39;: [\u0026#39;error\u0026#39;, 240], // 强制一行的最大长度 \u0026#39;eol-last\u0026#39;: \u0026#39;off\u0026#39;, // 要求或禁止文件末尾存在空行 \u0026#39;no-shadow\u0026#39;: \u0026#39;off\u0026#39;, // 禁止变量声明与外层作用域的变量同名 \u0026#39;import/no-cycle\u0026#39;: \u0026#39;off\u0026#39;, // 禁止一个模块导入一个有依赖路径的模块回到自己身上 \u0026#39;arrow-parens\u0026#39;: \u0026#39;off\u0026#39;, // 要求箭头函数的参数使用圆括号 \u0026#39;no-use-before-define\u0026#39;: \u0026#39;off\u0026#39;, // 禁止在变量定义之前使用它们，则倾向于默认输出 \u0026#39;prefer-const\u0026#39;: \u0026#39;warn\u0026#39;, // 要求使用 const 声明那些声明后不再被修改的变量 \u0026#39;global-require\u0026#39;: \u0026#39;off\u0026#39;, // 要求 require() 出现在顶层模块作用域中 \u0026#39;one-var-declaration-per-line\u0026#39;: \u0026#39;off\u0026#39;, // 要求或禁止在变量声明周围换行 \u0026#39;one-var\u0026#39;: \u0026#39;off\u0026#39;, // 强制函数中的变量要么一起声明要么分开声明 \u0026#39;object-curly-newline\u0026#39;: \u0026#39;off\u0026#39;, // 强制大括号内换行符的一致性 \u0026#39;default-case\u0026#39;: \u0026#39;off\u0026#39;, // 要求 switch 语句中有 default 分支 \u0026#39;no-trailing-spaces\u0026#39;: \u0026#39;off\u0026#39;, // 禁用行尾空格 \u0026#39;no-multi-spaces\u0026#39;: 2, \u0026#39;no-unused-expressions\u0026#39;: \u0026#39;off\u0026#39;, // 禁止出现未使用过的表达式 \u0026#39;no-underscore-dangle\u0026#39;: \u0026#39;off\u0026#39;, // 禁止标识符中有悬空下划线 \u0026#39;no-await-in-loop\u0026#39;: \u0026#39;off\u0026#39;, // 禁止在循环中出现 await \u0026#39;import/no-unresolved\u0026#39;: \u0026#39;off\u0026#39;, // 确保导入指向一个可以解析的文件/模块 \u0026#39;space-before-function-paren\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;comma-dangle\u0026#39;: [\u0026#39;error\u0026#39;, \u0026#39;never\u0026#39;],// 要求或禁止末尾逗号 \u0026#39;@typescript-eslint/ban-ts-ignore\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/explicit-function-return-type\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-explicit-any\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-var-requires\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-empty-function\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;vue/custom-event-name-casing\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;no-use-before-define\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-use-before-define\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/ban-ts-comment\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/ban-types\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-non-null-assertion\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/explicit-module-boundary-types\u0026#39;: \u0026#39;off\u0026#39;, \u0026#39;@typescript-eslint/no-unused-vars\u0026#39;: [ \u0026#39;error\u0026#39;, { argsIgnorePattern: \u0026#39;^h$\u0026#39;, varsIgnorePattern: \u0026#39;^h$\u0026#39; } ], \u0026#39;vue/max-attributes-per-line\u0026#39;: [\u0026#39;error\u0026#39;, { \u0026#39;singleline\u0026#39;: { \u0026#39;max\u0026#39;: 5 }, \u0026#39;multiline\u0026#39;: { \u0026#39;max\u0026#39;: 5 } }], \u0026#39;vue/first-attribute-linebreak\u0026#39;: [\u0026#39;error\u0026#39;, { \u0026#39;singleline\u0026#39;: \u0026#39;beside\u0026#39;, \u0026#39;multiline\u0026#39;: \u0026#39;beside\u0026#39; }], \u0026#39;vue/html-closing-bracket-newline\u0026#39;: [\u0026#39;error\u0026#39;, { \u0026#39;singleline\u0026#39;: \u0026#39;never\u0026#39;, \u0026#39;multiline\u0026#39;: \u0026#39;never\u0026#39; }], \u0026#39;vue/multi-word-component-names\u0026#39;: 0, //开启 关闭驼峰命名规则 \u0026#39;no-multiple-empty-lines\u0026#39;: [2, { max: 1 }] } } .eslintignore(忽略扫描) build/*.js src/assets public dist node_modules README.md docker docker-compose # curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun sudo mkdir -p /etc/docker sudo vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://registry.docker-cn.com\u0026#34;], \u0026#34;iptables\u0026#34;: false } sudo systemctl daemon-reload sudo systemctl restart docker sudo systemctl enable docker wsl2高级配置 # win11用户下新建.wslconfig [wsl2] networkingMode=mirrored dnsTunneling=true firewall=true autoProxy=true [experimental] sparseVhd=true autoMemoryReclaim=dropcache go # wget https://golang.google.cn/dl/go1.20.3.linux-amd64.tar.gz rm -rf /usr/local/go \u0026amp;\u0026amp; tar -C /usr/local -xzf go1.20.3.linux-amd64.tar.gz # vim ~/.zshrc export GOPATH=/home/xiaohu/gopath export GOBIN=/home/xiaohu/gopath/bin export PATH=$PATH:$GOBIN:/usr/local/go/bin export GOROOT=/usr/local/go export GOPROXY=https://goproxy.io,direct export GO111MODULE=on # 重启终端 go env vscode通用配置 # { //通用 \u0026#34;editor.fontSize\u0026#34;: 17, //字体大小 \u0026#34;editor.tabSize\u0026#34;: 4, //缩进 \u0026#34;editor.insertSpaces\u0026#34;: false, //使用tab缩进 \u0026#34;editor.formatOnSave\u0026#34;: true, //是否保存前格式化 \u0026#34;files.autoSave\u0026#34;: \u0026#34;afterDelay\u0026#34;, //文件自动保存 \u0026#34;editor.formatOnPaste\u0026#34;: true, //粘贴自动格式化 \u0026#34;files.trimTrailingWhitespace\u0026#34;: true, //保存时是否删除行尾空格 \u0026#34;editor.copyWithSyntaxHighlighting\u0026#34;: false, //复制代码是否携带语法高亮 \u0026#34;window.zoomLevel\u0026#34;: -1, //terminal tabs样式 \u0026#34;editor.rulers\u0026#34;: [ //标尺 120 ], \u0026#34;workbench.colorTheme\u0026#34;: \u0026#34;GitHub Dark\u0026#34;, //theme \u0026#34;workbench.iconTheme\u0026#34;: \u0026#34;vscode-icons\u0026#34;, //文件图标 \u0026#34;workbench.colorCustomizations\u0026#34;: { //颜色覆盖 \u0026#34;editorRuler.foreground\u0026#34;: \u0026#34;#08fc31\u0026#34;, }, //go \u0026#34;[go]\u0026#34;: { \u0026#34;editor.wordWrap\u0026#34;: \u0026#34;on\u0026#34;, //折行 \u0026#34;editor.codeActionsOnSave\u0026#34;: { \u0026#34;source.organizeImports\u0026#34;: true }, }, \u0026#34;go.testFlags\u0026#34;: [ \u0026#34;-gcflags=all=-l\u0026#34;, \u0026#34;-v\u0026#34; ], \u0026#34;go.alternateTools\u0026#34;: { \u0026#34;go.inferGopath\u0026#34;: true, \u0026#34;go.autocompleteUnimportedPackages\u0026#34;: true, \u0026#34;go.gocodePackageLookupMode\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;go.gotoSymbol.includeImports\u0026#34;: true, \u0026#34;go.useCodeSnippetsOnFunctionSuggest\u0026#34;: true, \u0026#34;go.useCodeSnippetsOnFunctionSuggestWithoutType\u0026#34;: true, \u0026#34;go.docsTool\u0026#34;: \u0026#34;gogetdoc\u0026#34;, }, \u0026#34;go.useLanguageServer\u0026#34;: true, \u0026#34;go.formatTool\u0026#34;: \u0026#34;goimports\u0026#34;, //vue \u0026#34;javascript.updateImportsOnFileMove.enabled\u0026#34;: \u0026#34;always\u0026#34;, \u0026#34;typescript.updateImportsOnFileMove.enabled\u0026#34;: \u0026#34;always\u0026#34;, //highlight \u0026#34;highlight-matching-tag.styles\u0026#34;: { \u0026#34;opening\u0026#34;: { \u0026#34;full\u0026#34;: { \u0026#34;custom\u0026#34;: { \u0026#34;borderWidth\u0026#34;: \u0026#34;1px\u0026#34;, \u0026#34;borderStyle\u0026#34;: \u0026#34;solid\u0026#34;, \u0026#34;borderColor\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;borderRadius\u0026#34;: \u0026#34;5px\u0026#34; } } } }, //gitlens \u0026#34;gitlens.defaultDateFormat\u0026#34;: \u0026#34;YYYY-MM-DD HH:mm:ss\u0026#34;, //tabnine \u0026#34;tabnine.experimentalAutoImports\u0026#34;: true, \u0026#34;vsicons.dontShowNewVersionMessage\u0026#34;: true, \u0026#34;git.confirmSync\u0026#34;: false, //eslint \u0026#34;eslint.validate\u0026#34;: [ \u0026#34;javascript\u0026#34;, \u0026#34;javascriptreact\u0026#34;, \u0026#34;vue\u0026#34;, \u0026#34;typescript\u0026#34; ], \u0026#34;eslint.format.enable\u0026#34;: true, \u0026#34;editor.codeActionsOnSave\u0026#34;: { \u0026#34;source.fixAll\u0026#34;: true, \u0026#34;eslint.autoFixOnSave\u0026#34;: true, }, } "},{"id":12,"href":"/docs/docs/4_linux/","title":"linux","section":"Docs","content":" 常用命令 # su passwd root //为ubuntu的root用户设置密码 sudo passwd root adduser user //新建用户 passwd user //修改用户密码 // chmod -v u+w /etc/sudoers //添加sudoers文件可写权限 vim /etc/sudoers xiaohu ALL=(ALL:ALL) NOPASSWD: ALL // chown -R docker //更改目录及其子目录文件的归属 chmod -R docker //更改目录及其子目录文件的权限 cat /etc/issue //查看操作系统 uname -a //查看内核版本 exit //退出当前登陆 which bash //查找bash可执行二进制文件位置 whereis bash //查找所有bash的文件 whatis bash //bash是干什么的 clear //清除屏幕内容 pwd //当前目录地址 touch fn //创建文件 mkdir dirname //创建目录 mkdir -p dirname //递归创建目录 ln -s fn link //给指定文件创建一个软链接 cat fn //显示整个内容 more fn //逐屏显示某文件内容,空格翻页,q退出 less fn //b向上翻一页,d向后翻半页,/搜索，n向下查找,N向上查找 head -3 fn //显示文件头部3行 tail -3 fn //显示文件尾3行 tail -f fn //持续显示文件尾部数据，可用于监控日志 wc fn //统计文件有多少行，多少个单词,多少个字节数 grep \u0026#34;info\u0026#34; fn //在文件中查找出现过info的内容 grep -r \u0026#34;info\u0026#34; dirname //递归查找dirname目录下info的内容+文件地址 whoami //显示登录用户 passwd user //修改用户密码 deluser user //删除用户 groupdel user //删除用户组 ps //查看当前会话进程 ps ax //查看所有进程 ps aux //查看所有进程详细信息 ps -u user //查看某用户进程 ps axjf //列出进程树 ps aux | grep httpd //查看名为 httpd 的所有进程 kill 287391 //杀死进程 kill -9 3829 //强制杀死进程 top //查看最活跃的进程 nohup main.go \u0026amp; //在后台长期运行某程序 date //显示日期 free //显示内存和交换区使用情况 df //显示磁盘使用情况 -m:M显示，-H:G显示 du -sh *\t//查看当前文件 目录大小 ps -ef|grep 3306\t//查看端口进程 //显示当前目录各个文件大小 netstat -a //列出所有端口 netstat -l //查看所有监听的端口 netstat -t //查看所有 TCP 链接 \u0026gt; file //将命令的标准输出重定向到文件，会覆盖文件 \u0026gt;\u0026gt; file //将命令的标准输出重定向到文件，追加不覆盖 tar -zcvf filename.tar.gz src //压缩 tar -zxvf filename.tar.gz /usr/local/ //解压 //总核数 = 物理CPU个数 X 每颗物理CPU的核数 //总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数 cat /proc/cpuinfo| grep \u0026#34;physical id\u0026#34;| sort| uniq| wc -l //查看物理CPU个数 cat /proc/cpuinfo| grep \u0026#34;cpu cores\u0026#34;| uniq //查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \u0026#34;processor\u0026#34;| wc -l //查看逻辑CPU的个数 cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c //查看CPU信息（型号） cat /proc/meminfo //查看内存信息 cat /proc/cpuinfo |grep MHz|uniq //查看cpu内核频率 "},{"id":13,"href":"/docs/docs/5_regexp/","title":"regexp","section":"Docs","content":" 校验数字的表达式 # 1 数字：^[0-9]*$ 2 n位的数字：^\\d{n}$ 3 至少n位的数字：^\\d{n,}$ 4 m-n位的数字：^\\d{m,n}$ 5 零和非零开头的数字：^(0|[1-9][0-9]*)$ 6 非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]{1,2})?$ 7 带1-2位小数的正数或负数：^(\\-)?\\d+(\\.\\d{1,2})?$ 8 正数、负数、和小数：^(\\-|\\+)?\\d+(\\.\\d+)?$ 9 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$ 10 有1~3位小数的正实数：^[0-9]+(.[0-9]{1,3})?$ 11 非零的正整数：^[1-9]\\d*$ 或 ^([1-9][0-9]*){1,3}$ 或 ^\\+?[1-9][0-9]*$ 12 非零的负整数：^\\-[1-9][]0-9\u0026#34;*$ 或 ^-[1-9]\\d*$ 13 非负整数：^\\d+$ 或 ^[1-9]\\d*|0$ 14 非正整数：^-[1-9]\\d*|0$ 或 ^((-\\d+)|(0+))$ 15 非负浮点数：^\\d+(\\.\\d+)?$ 或 ^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$ 16 非正浮点数：^((-\\d+(\\.\\d+)?)|(0+(\\.0+)?))$ 或 ^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$ 17 正浮点数：^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$ 或 ^(([0-9]+\\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\\.[0-9]+)|([0-9]*[1-9][0-9]*))$ 18 负浮点数：^-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*)$ 或 ^(-(([0-9]+\\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\\.[0-9]+)|([0-9]*[1-9][0-9]*)))$ 19 浮点数：^(-?\\d+)(\\.\\d+)?$ 或 ^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$ 校验字符的表达式 # 1 汉字：^[\\u4e00-\\u9fa5]{0,}$ 2 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$ 3 长度为3-20的所有字符：^.{3,20}$ 4 由26个英文字母组成的字符串：^[A-Za-z]+$ 5 由26个大写英文字母组成的字符串：^[A-Z]+$ 6 由26个小写英文字母组成的字符串：^[a-z]+$ 7 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$ 8 由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 或 ^\\w{3,20}$ 9 中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$ 10 中文、英文、数字但不包括下划线等符号：^[\\u4E00-\\u9FA5A-Za-z0-9]+$ 或 ^[\\u4E00-\\u9FA5A-Za-z0-9]{2,20}$ 11 可以输入含有^%\u0026amp;\u0026#39;,;=?$\\\u0026#34;等字符：[^%\u0026amp;\u0026#39;,;=?$\\x22]+ 12 禁止输入含有~的字符：[^~\\x22]+ 特殊需求表达式 # 1 Email地址：^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*$ 2 域名：[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(/.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+/.? 3 InternetURL：[a-zA-z]+://[^\\s]* 或 ^http://([\\w-]+\\.)+[\\w-]+(/[\\w-./?%\u0026amp;=]*)?$ 4 手机号码：^(13[0-9]|14[5|7]|15[0|1|2|3|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d{8}$ 5 电话号码(\u0026#34;XXX-XXXXXXX\u0026#34;、\u0026#34;XXXX-XXXXXXXX\u0026#34;、\u0026#34;XXX-XXXXXXX\u0026#34;、\u0026#34;XXX-XXXXXXXX\u0026#34;、\u0026#34;XXXXXXX\u0026#34;和\u0026#34;XXXXXXXX)：^(\\(\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$ 6 国内电话号码(0511-4405222、021-87888822)：\\d{3}-\\d{8}|\\d{4}-\\d{7} 7 身份证号(15位、18位数字)：^\\d{15}|\\d{18}$ 8 短身份证号码(数字、字母x结尾)：^([0-9]){7,18}(x|X)?$ 或 ^\\d{8,18}|[0-9x]{8,18}|[0-9X]{8,18}?$ 9 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 10 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\\w{5,17}$ 11 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$ 12 日期格式：^\\d{4}-\\d{1,2}-\\d{1,2} 13 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$ 14 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ 15 钱的输入格式： 16 1.有四种钱的表示形式我们可以接受:\u0026#34;10000.00\u0026#34; 和 \u0026#34;10,000.00\u0026#34;, 和没有 \u0026#34;分\u0026#34; 的 \u0026#34;10000\u0026#34; 和 \u0026#34;10,000\u0026#34;：^[1-9][0-9]*$ 17 2.这表示任意一个不以0开头的数字,但是,这也意味着一个字符\u0026#34;0\u0026#34;不通过,所以我们采用下面的形式：^(0|[1-9][0-9]*)$ 18 3.一个0或者一个不以0开头的数字.我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$ 19 4.这表示一个0或者一个可能为负的开头不为0的数字.让用户以0开头好了.把负号的也去掉,因为钱总不能是负的吧.下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$ 20 5.必须说明的是,小数点后面至少应该有1位数,所以\u0026#34;10.\u0026#34;是不通过的,但是 \u0026#34;10\u0026#34; 和 \u0026#34;10.2\u0026#34; 是通过的：^[0-9]+(.[0-9]{2})?$ 21 6.这样我们规定小数点后面必须有两位,如果你认为太苛刻了,可以这样：^[0-9]+(.[0-9]{1,2})?$ 22 7.这样就允许用户只写一位小数.下面我们该考虑数字中的逗号了,我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$ 23 8.1到3个数字,后面跟着任意个 逗号+3个数字,逗号成为可选,而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$ 24 备注：这就是最终结果了,别忘了\u0026#34;+\u0026#34;可以用\u0026#34;*\u0026#34;替代如果你觉得空字符串也可以接受的话(奇怪,为什么?)最后,别忘了在用函数时去掉去掉那个反斜杠,一般的错误都在这里 25 xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\\\.[x|X][m|M][l|L]$ 26 中文字符的正则表达式：[\\u4e00-\\u9fa5] 27 双字节字符：[^\\x00-\\xff] (包括汉字在内，可以用来计算字符串的长度(一个双字节字符长度计2，ASCII字符计1)) 28 空白行的正则表达式：\\n\\s*\\r (可以用来删除空白行) 29 HTML标记的正则表达式：\u0026lt;(\\S*?)[^\u0026gt;]*\u0026gt;.*?\u0026lt;/\\1\u0026gt;|\u0026lt;.*? /\u0026gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力) 30 首尾空白字符的正则表达式：^\\s*|\\s*$或(^\\s*)|(\\s*$) (可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式) 31 腾讯QQ号：[1-9][0-9]{4,} (腾讯QQ号从10000开始) 32 中国邮政编码：[1-9]\\d{5}(?!\\d) (中国邮政编码为6位数字) 33 IP地址：\\d+\\.\\d+\\.\\d+\\.\\d+ (提取IP地址时有用) 34 IP地址：((?:(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d?\\\\d)\\\\.){3}(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d?\\\\d)) "},{"id":14,"href":"/docs/docs/6_%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式","section":"Docs","content":" 设计模式 # 创建型 //在一些情况下，要创建的对象需要一系列复杂的初始化操作，比如查配置文件、查数据库表、初始化成员对象等，如果把这些逻辑放在构造函数中，会极大影响代码的可读性。不妨定义一个类来专门负责对象的创建，这样的类就是工厂类，这种做法就是工厂模式 简单工厂模式:根据传入的入参，生成不同的结构体的接口方法(有一个具体的工厂类，可以根据传入参数，生产不同的产品)，由一个对象负责所有具体类的实例化。 工厂方法模式:解决根据简单工厂入参的判断问题，（只需要知道具体工厂名，即可生产对应产品），将对象创建从由一个对象负责所有具体类的实例化，变成由一群子类来负责对具体类的实例化，从而将过程解耦 抽象工厂模式：子类会越来越多，可以将产品进行分组，每组中的不同产品由同一个工厂类的不同方法来创建。 创建者模式:将一个复杂对象的构建分离成多个简单对象的构建组合;(将一个复杂的对象的构造和表示分离，使同样的构造过程可以创建不同的产品。) 原型模式:使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。 单例模式: 结构型模式 外观模式 适配器模式 代理模式 组合模式 享元模式 装饰模式 桥模式 行为型模式 中介者模式: 观察者模式: 命令模式: 迭代器模式: 模板方法模式: 策略模式: 状态模式: 备忘录模式: 解释器模式: 职责链模式: 访问者模式: 创建型 # 简单工厂模式 # //简单工厂模式有一个具体的工厂类，可以根据传入的入参，生产不同的产品 func main() { f := getFruit(\u0026#34;apple\u0026#34;) fmt.Println(f.Fruit()) } type FruitFactory interface { Fruit() string } func getFruit(t string) FruitFactory { switch t { case \u0026#34;apple\u0026#34;: return \u0026amp;apple{} case \u0026#34;banana\u0026#34;: return \u0026amp;banana{} } return nil } type apple struct{} func (*apple) Fruit() string { return \u0026#34;我是苹果，我很好吃\u0026#34; } type banana struct{} func (*banana) Fruit() string { return \u0026#34;我是香蕉，我最好吃了\u0026#34; } 工厂方法模式 # //工厂方法 调用方只需要知道具体工厂名即可生成对应产品 //工厂方法模式使用子类的方式延迟生成对象到子类中实现。 func main() { apple := appleFactory{} fmt.Println(apple.Fruit()) banana := bananaFactory{} fmt.Println(banana.Fruit()) } type Fruit interface { Fruit() string } type appleFactory struct{} func (*appleFactory) Fruit() string { return \u0026#34;我是苹果，我很好吃\u0026#34; } type bananaFactory struct{} func (*bananaFactory) Fruit() string { return \u0026#34;我是香蕉，我最好吃了\u0026#34; } 抽象工厂模式 # func main() { f := WuhanFruitFactory{} b := f.ChooseApple() b.Fruit() } type FruitInterface interface { ChooseApple() ProductInterface ChooseBanana() ProductInterface } type ProductInterface interface { Fruit() } type HainanApple struct { } func (h HainanApple) Fruit() { fmt.Println(\u0026#34;我是苹果，来自海南\u0026#34;) } type HainanBanana struct { } func (h HainanBanana) Fruit() { fmt.Println(\u0026#34;我是香蕉，来自海南\u0026#34;) } type WuhanApple struct { } func (w WuhanApple) Fruit() { fmt.Println(\u0026#34;我是苹果，来自武汉\u0026#34;) } type WuhanBanana struct { } func (w WuhanBanana) Fruit() { fmt.Println(\u0026#34;我是香蕉，来自武汉\u0026#34;) } type WuhanFruitFactory struct { } func (w WuhanFruitFactory) ChooseApple() ProductInterface { return WuhanApple{} } func (w WuhanFruitFactory) ChooseBanana() ProductInterface { return WuhanBanana{} } type HainanFruitFactory struct { } func (gd HainanFruitFactory) ChooseApple() ProductInterface { return HainanApple{} } func (gd HainanFruitFactory) ChooseBanana() ProductInterface { return HainanBanana{} } 创建者模式 # //Builder 是生成器接口 type Builder interface { Part1() Part2() Part3() } type Director struct { builder Builder } // NewDirector ... func NewDirector(builder Builder) *Director { return \u0026amp;Director{ builder: builder, } } //Construct Product func (d *Director) Construct() { d.builder.Part1() d.builder.Part2() d.builder.Part3() } type Builder1 struct { result string } func (b *Builder1) Part1() { b.result += \u0026#34;1\u0026#34; } func (b *Builder1) Part2() { b.result += \u0026#34;2\u0026#34; } func (b *Builder1) Part3() { b.result += \u0026#34;3\u0026#34; } func (b *Builder1) GetResult() string { return b.result } 原型模式 # 单例模式 # 结构型模式 # 外观模式 # 适配器模式 # 代理模式 # 组合模式 # 享元模式 # 装饰模式 # 桥模式 # 行为型模式 # 中介者模式 # 观察者模式 # 命令模式 # 迭代器模式 # 模板方法模式 # 策略模式 # 状态模式 # 备忘录模式 # 解释器模式 # 职责链模式 # 访问者模式 # "},{"id":15,"href":"/docs/docs/7_shell/","title":"shell","section":"Docs","content":" shell脚本 # shell命令 # str=\u0026#34;hello word !\u0026#34; # 定义变量,没有数据类型的概念，全是字符串，单引号用于保留字符的字面含义，各种特殊字符在单引号里面，双引号的美元符号（$）、反引号（`）和反斜杠（\\）有特殊含义 (()) # 整数进行算术运算,((i++)),((i--)) [[]] # 条件判断，==，\u0026amp;\u0026amp;，|| {} # 取出变量值 \u0026amp;\u0026amp; -a # and || -o # or -n str1 # 判断字符串不为空（长度大于零） -z str1 # 判断字符串为空（长度等于零） echo ${str} # 查看变量内容 ${#str} # 返回字符串长度 echo ${str:4:4} # 获取子串，不会改变原始字符串 str2=\u0026#34;$str$str1\u0026#34; # 字符串拼接，可以有空格 str3=$(($str+$str1)) # 整型运算 a[0]=\u0026#34;2\u0026#34; # 定义数组 a=( foo bar \u0026#34;a b c\u0026#34; 42 ) # 定义数组 ${a[0]} # 取得数组中的元素 ${#a[@]} # 取得数组的长度 ${#a[i]} # 取得数组中某个变量的长度 (${a1[@]} ${a2[@]}) # 两个数组拼接 unset a[5] # 删除数组元素，unset a 删除整个数组 read # 接收命令行输入 -p提示 ####### if ######### if [ \u0026#34;$varname\u0026#34; = \u0026#34;foo\u0026#34; ]; then echo \u0026#34;this is foo\u0026#34; elif [ \u0026#34;$varname\u0026#34; = \u0026#34;bar\u0026#34; ]; then echo \u0026#34;this is bar\u0026#34; else echo \u0026#34;neither\u0026#34; fi ##### case ########## case expression in pattern1 ) statements ;; pattern2 ) statements ;; * ) otherwise ;; esac ######for######### for i in {1..10}; do echo $i done for ((i = 0; i \u0026lt; 10; i++)); do echo $i; done ####### func ######### function myfunc(){ # $1 代表第一个入参，$N 代表第 N 个入参 # $# 代表参数个数 # $0 代表被调用者自身的名字 # $@ 代表所有参数，类型是个数组，想传递所有参数给其他命令用 cmd \u0026#34;$@\u0026#34; # $* 空格链接起来的所有参数，类型是字符串 } myfunc # 调用函数 myfunc myfunc arg1 arg2 arg3 # 带参数的函数调用 myfunc \u0026#34;$@\u0026#34; # 将所有参数传递给函数 myfunc \u0026#34;${array[@]}\u0026#34; # 将一个数组当作多个参数传递给函数 unset -f myfunc # 删除函数 ##########文件########### -a file # 判断文件存在，如 [ -a /tmp/abc ] -e file # 判断文件夹是否存在 -d file # 判断文件存在，且该文件是一个目录 -f file # 判断文件存在，且该文件是一个普通文件（非目录等） -r file # 判断文件存在，且可读 -w file # 判断文件存在，且可写 -x file # 判断文件存在，且执行 -s file # 判断文件存在，且尺寸大于0 -O file # 文件存在且属于当前用户 -G file # 文件存在且匹配你的用户组 file1 -nt file2 # 文件1 比 文件2 新 file1 -ot file2 # 文件1 比 文件2 旧 #############数字########### num1 -eq num2 # 数字判断：num1 == num2 num1 -ne num2 # 数字判断：num1 != num2 num1 -lt num2 # 数字判断：num1 \u0026lt; num2 num1 -le num2 # 数字判断：num1 \u0026lt;= num2 num1 -gt num2 # 数字判断：num1 \u0026gt; num2 num1 -ge num2 # 数字判断：num1 \u0026gt;= num2 常用命令 # awk \u0026#39;{print $5}\u0026#39; file # 打印文件中以空格分隔的第五列 awk -F \u0026#39;,\u0026#39; \u0026#39;{print $5}\u0026#39; file # 打印文件中以逗号分隔的第五列 awk \u0026#39;/str/ {print $2}\u0026#39; file # 打印文件中包含 str 的所有行的第二列 awk -F \u0026#39;,\u0026#39; \u0026#39;{print $NF}\u0026#39; file # 打印逗号分隔的文件中的每行最后一列 awk \u0026#39;{s+=$1} END {print s}\u0026#39; file # 计算所有第一列的合 awk \u0026#39;NR%3==1\u0026#39; file # 从第一行开始，每隔三行打印一行 sed \u0026#39;s/find/replace/\u0026#39; file # 替换文件中首次出现的字符串并输出结果 sed \u0026#39;10s/find/replace/\u0026#39; file # 替换文件第 10 行内容 sed \u0026#39;10,20s/find/replace/\u0026#39; file # 替换文件中 10-20 行内容 sed -r \u0026#39;s/regex/replace/g\u0026#39; file # 替换文件中所有出现的字符串 sed -i \u0026#39;s/find/replace/g\u0026#39; file # 替换文件中所有出现的字符并且覆盖文件 sed -i \u0026#39;/find/i\\newline\u0026#39; file # 在文件的匹配文本前插入行 sed -i \u0026#39;/find/a\\newline\u0026#39; file # 在文件的匹配文本后插入行 sed \u0026#39;/line/s/find/replace/\u0026#39; file # 先搜索行特征再执行替换 sed -e \u0026#39;s/f/r/\u0026#39; -e \u0026#39;s/f/r\u0026#39; file # 执行多次替换 sed \u0026#39;s#find#replace#\u0026#39; file # 使用 # 替换 / 来避免 pattern 中有斜杆 sed -i -r \u0026#39;s/^\\s+//g\u0026#39; file # 删除文件每行头部空格 sed \u0026#39;/^$/d\u0026#39; file # 删除文件空行并打印 sed -i \u0026#39;s/\\s\\+$//\u0026#39; file # 删除文件每行末尾多余空格 sed -n \u0026#39;2p\u0026#39; file # 打印文件第二行 sed -n \u0026#39;2,5p\u0026#39; file # 打印文件第二到第五行 sort file # 排序文件 sort -r file # 反向排序（降序） sort -n file # 使用数字而不是字符串进行比较 sort -t: -k 3n /etc/passwd # 按 passwd 文件的第三列进行排序 sort -u file # 去重排序 ps aux | sort -nk +4 | tail # 显示前十个运行的进程并按内存使用量排序 # {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-03-31 14:13:25.514\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;/home/xiaohu/project/alpha/middleware/trace.go:26\u0026#34;,\u0026#34;trace_id\u0026#34;:\u0026#34;d9a7097f-5b27-4660-b675-25b4824dbc82\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;req\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;172.12.0.1\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;favicon.ico\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;query\u0026#34;:\u0026#34;\u0026#34;} # awk 复杂文本格式化；单分隔符：-F \u0026#39;,\u0026#39; ；多分隔符：-F \u0026#39;[,;]\u0026#39; # {print $6} 输出分割的第几个值 # sort 排序 # uniq 去重 # wc 统计 -l统计行数 awk -F \u0026#39;,\u0026#39; \u0026#39;{print $6}\u0026#39; latest_log | awk -F \u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39; | sort|uniq|wc -l # 查看有多少个IP访问 grep \u0026#34;/index.php\u0026#34; log_file | wc -l # 查看某一个页面被访问的次数 # BEGIN：放的是执行前的语句；END：这里面放的是处理完所有的行后要执行的语句 # {++S[$1]} 对空格或tab分割的第一个字符，进行计数。s[2.22.2.1]=9 # sort r倒叙，t分隔符 n数值型 k排序依据 awk \u0026#39;{++S[$1]} END {for (a in S) print a,S[a]}\u0026#39; log_file \u0026gt; log.txt # 查看每一个IP访问了多少个页面 sort -n -t \u0026#39; \u0026#39; -k 2 log.txt awk \u0026#39;{++S[$1]} END {for (a in S) print S[a],a}\u0026#39; log_file | sort -n # 将每个IP访问的页面数进行从小到大排序 grep ^111.111.111.111 log_file| awk \u0026#39;{print $1,$7}\u0026#39; # 查看某一个IP访问了哪些页面 awk \u0026#39;{print $12,$1}\u0026#39; log_file | grep ^\\\u0026#34;Mozilla | awk \u0026#39;{print $2}\u0026#39; |sort | uniq | wc -l # 去掉搜索引擎统计的页面 awk \u0026#39;{print $4,$1}\u0026#39; log_file | grep 16/Aug/2015:14 | awk \u0026#39;{print $2}\u0026#39;| sort | uniq | wc -l # 查看2015年8月16日14时这一个小时内有多少IP访问 awk \u0026#39;{print $1}\u0026#39; |sort|uniq -c|sort -nr |head -10 access_log # 查看访问前十个ip地址 cat log_file|awk \u0026#39;{print $11}\u0026#39;|sort|uniq -c|sort -nr | head -10 # 访问次数最多的10个文件或页面 cat www.access.log |awk \u0026#39;($7~/\\.php/){print $10 \u0026#34; \u0026#34; $1 \u0026#34; \u0026#34; $4 \u0026#34; \u0026#34; $7}\u0026#39;|sort -nr|head -100 # 列出传输大小最大的几个文件 cat www.access.log |awk \u0026#39;($10 \u0026gt; 200000 \u0026amp;\u0026amp; $7~/\\.php/){print $7}\u0026#39;|sort -n|uniq -c|sort -nr|head -100 # 列出输出大于200000byte(约200kb)的页面以及对应页面发生次数 cat www.access.log |awk \u0026#39;($7~/\\.php/){print $NF \u0026#34; \u0026#34; $1 \u0026#34; \u0026#34; $4 \u0026#34; \u0026#34; $7}\u0026#39;|sort -nr|head -100 # 如果日志最后一列记录的是页面文件传输时间，则有列出到客户端最耗时的页面 cat www.access.log |awk \u0026#39;($NF \u0026gt; 60 \u0026amp;\u0026amp; $7~/\\.php/){print $7}\u0026#39;|sort -n|uniq -c|sort -nr|head -100 # 列出最最耗时的页面(超过60秒的)的以及对应页面发生次数 cat www.access.log |awk \u0026#39;($NF \u0026gt; 30){print $7}\u0026#39;|sort -n|uniq -c|sort -nr|head -20 # 列出传输时间超过 30 秒的文件 ps -ef | awk -F \u0026#39; \u0026#39; \u0026#39;{print $8 \u0026#34; \u0026#34; $9}\u0026#39; |sort | uniq -c |sort -nr |head -20 # 列出当前服务器每一进程运行的数量，倒序排列 cat access.log |grep \u0026#39;04/May/2012\u0026#39;| awk \u0026#39;{print $11}\u0026#39;|sort|uniq -c|sort -nr|head -20 # 分析日志文件下 2012-05-04 访问页面最高 的前20个 URL 并排序 cat access_log | awk \u0026#39;($11~/\\www.abc.com/){print $1}\u0026#39;|sort|uniq -c|sort -nr # 查询访问页面的URL地址中 含有 某个网址的 IP 地址 cat linewow-access.log|awk \u0026#39;{print $1}\u0026#39;|sort|uniq -c|sort -nr|head -10 # 获取访问最高的10个IP地址 同时也可以按时间来查询 cat log_file | egrep \u0026#39;15/Aug/2015|16/Aug/2015\u0026#39; |awk \u0026#39;{print $1}\u0026#39;|sort|uniq -c|sort -nr|head -10 # 时间段查询日志时间段的情况 cat access.log |awk \u0026#39;($NF \u0026gt; 60 \u0026amp;\u0026amp; $7~/\\.php/){print $7}\u0026#39;|sort -n|uniq -c|sort -nr|head -100 # 列出最最耗时的页面(超过60秒的)的以及对应页面发生次数 cat access.log |awk \u0026#39;{sum+=$10} END {print sum/1024/1024/1024}\u0026#39; # 统计网站流量（G) awk \u0026#39;($9 ~/404/)\u0026#39; access.log | awk \u0026#39;{print $9,$7}\u0026#39; | sort # 统计404的连接 cat access.log |awk \u0026#39;{print $9}\u0026#39;|sort|uniq -c|sort -rn # 统计http status cat /tmp/access.log | grep \u0026#34;20/Mar/2011\u0026#34; |awk \u0026#39;{print $3}\u0026#39;|sort |uniq -c|sort -nr|head # 找出某天访问次数最多的10个IP awk -vFS=\u0026#34;[:]\u0026#34; \u0026#39;{gsub(\u0026#34;-.*\u0026#34;,\u0026#34;\u0026#34;,$1);num[$2\u0026#34; \u0026#34;$1]++}END{for(i in num)print i,num[i]}\u0026#39; log_file | sort -n -k 3 -r | head -10 # 小时单位里ip连接数最多的10个时段 awk \u0026#39;{print $1}\u0026#39; access.log | grep \u0026#34;20/Mar/2011\u0026#34; |cut -c 14-18|sort|uniq -c|sort -nr|head # 找出访问次数最多的几个分钟 netstat -nat |awk \u0026#39;{print $6}\u0026#39;|sort|uniq -c|sort -rn # 查看tcp的链接状态 netstat -n |awk \u0026#39;/^tcp/ {print $NF}\u0026#39;|sort|uniq -c|sort -rn # 查看tcp的链接状态 netstat -anlp|grep 80|grep tcp|awk \u0026#39;{print $5}\u0026#39;|awk -F: \u0026#39;{print $1}\u0026#39;|sort|uniq -c|sort -nr|head -n20 # 查找请求数前20个IP（常用于查找攻来源） netstat -ant |awk \u0026#39;/:80/{split($5,ip,\u0026#34;:\u0026#34;);++A[ip[1]]}END{for(i in A) print A[i],i}\u0026#39; |sort -rn|head -n20 # 查找请求数前20个IP（常用于查找攻来源） tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F\u0026#34;.\u0026#34; \u0026#39;{print $1\u0026#34;.\u0026#34;$2\u0026#34;.\u0026#34;$3\u0026#34;.\u0026#34;$4}\u0026#39; | sort | uniq -c | sort -nr |head -20 # 用tcpdump嗅探80端口的访问看看谁最高 netstat -n|grep TIME_WAIT|awk \u0026#39;{print $5}\u0026#39;|sort|uniq -c|sort -rn|head -n20 # 查找较多time_wait连接 netstat -an | grep SYN | awk \u0026#39;{print $5}\u0026#39; | awk -F: \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr | more # 找查较多的SYN连接 netstat -ntlp | grep 80 | awk \u0026#39;{print $7}\u0026#39; | cut -d/ -f1 # 根据端口列进程 netstat -ant | grep $ip:80 | wc -l # 查看了连接数和当前的连接数 netstat -ant | grep $ip:80 | grep EST | wc -l # 查看了连接数和当前的连接数 netstat -nat|grep \u0026#34;:80\u0026#34;|awk \u0026#39;{print $5}\u0026#39; |awk -F: \u0026#39;{print $1}\u0026#39; | sort| uniq -c|sort -n # 查看IP访问次数 "},{"id":16,"href":"/docs/docs/8_docker/","title":"docker","section":"Docs","content":" docker命令 # sudo systemctl start docker //启动 sudo systemctl enable docker.service //开机启动 docker ps -a //查看所有容器 docker ps //查看正在运行的容器 docker stop 容器 //停止容器 docker start 容器 docker restart 容器 docker rm 容器 //删除容器 docker images //查看所有镜像 docker rmi 镜像 //删除镜像 docker logs -f -t --tail n 容器 //查看容器日志最新的第n条数据(f:跟踪日志输出；-t:显示时间戳；--tail:列出最新的n条日志) docker inspect 容器 //查看容器详细信息 docker exec -it 容器 bash //进入运行的容器 docker exec -u 0 -it kibana /bim/bash //root权限进入容器 docker stats //查看当前运行容器 (解释:https://www.runoob.com/docker/docker-stats-command.html) docker system prune //清除none镜像 可能所有容器删除 docker rm $(docker ps -aq) //删除所有容器 docker rmi -f $(docker images -qa) //删除所有镜像 docker build -t name:tag . //构建 docker stop $(docker ps -aq) //停止所有容器 docker 网络模型 # bridge :默认模式，为每一个容器分配、设置 IP 等，并将容器连接到一个 docker0 虚拟网桥 host:使用宿主机的ip、端口 none:容器有独立的 Network namespace，但并没有对其进行任何网络设置，如分配 veth pair 和网桥连接，IP 等。 container：新创建的容器不会创建自己的网卡和配置自己的IP，而是和一个指定的容器共享 IP、端口范围等。 获取容器连接ip # docker inspect --format=\u0026#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; prometheus 出现 Kibana server is not ready yet？ # es //elasticsearch.yml添加： xpack.security.enabled: true xpack.license.self_generated.type: basic xpack.security.transport.ssl.enabled: true //执行 bin/elasticsearch-setup-passwords interactive 输入 y kibana 进入容器 cd config vim kibana.yml server.name: kibana server.host: \u0026#34;0.0.0.0\u0026#34; elasticsearch.hosts: [ \u0026#34;http://{IpAddress}:9200\u0026#34; ] monitoring.ui.container.elasticsearch.enabled: true elasticsearch.username: \u0026#34;elastic\u0026#34; elasticsearch.password: \u0026#34;1qaz!QAZ\u0026#34; i18n.locale: \u0026#34;zh-CN\u0026#34; docker restart kibana "},{"id":17,"href":"/docs/docs/9_dockerfile/","title":"dockerfile","section":"Docs","content":" 命令 # FROM： //指定基础镜像 MAINTAINER： //作者信息 RUN： //运行Linux系统的命令 CMD： //指定容器启动执行的命令（CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令） LABEL： //指定生成镜像的源数据标签 EXPOSE： //指定镜像容器监听端口号 ENV： //设置环境变量 ADD： //对压缩文件进行解压缩，将本机数据或远程文件（url）添加到容器指定的目录 COPY： //复制宿主机数据到镜像内部使用（与ADD不同，不会自动解压文件，也不能访问网络资源） WORKDIR： //切换到镜像容器中的指定目录中,终端默认登陆的进来工作目录 VOLUME： //挂载数据卷到镜像容器中，持久化目录 USER： //指定运行容器的用户 ARG： //指定镜像的版本号信息 ONBUILD： //创建镜像，作为其他镜像的基础镜像运行操作指令 ENTRYPOINT： //配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖 实例 # FROM golang:1.20.3 AS builder WORKDIR /src COPY . /src RUN export GOPROXY=https://goproxy.io \u0026amp;\u0026amp; make build FROM debian:stable-slim LABEL MAINTAINER=\u0026#34;xiaohubai@outlook.com\u0026#34; RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ ca-certificates \\ netbase \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/ \\ \u0026amp;\u0026amp; apt-get autoremove -y \u0026amp;\u0026amp; apt-get autoclean -y WORKDIR /app COPY --from=builder /src/server /app COPY --from=builder /src/rbac_model.conf /app EXPOSE 8000 EXPOSE 9000 CMD [\u0026#34;./server\u0026#34;,\u0026#34;-env\u0026#34;, \u0026#34;remote\u0026#34;, \u0026#34;-chost\u0026#34;, \u0026#34;172.21.0.2:8500\u0026#34;, \u0026#34;-ctype\u0026#34;, \u0026#34;consul\u0026#34; ,\u0026#34;-cpath\u0026#34;, \u0026#34;prod/config.yaml\u0026#34;] "},{"id":18,"href":"/docs/docs/10_docker-compose/","title":"docker-compose","section":"Docs","content":" docker-compose.yml # version: \u0026#39;3\u0026#39; ## 遇到容器不停重启，首先怀疑是挂载目录读写执行权限问题 services: #### mysql #### mysql: image: bitnami/mysql:8.0 container_name: mysql restart: always ports: - \u0026#34;3306:3306\u0026#34; volumes: - /usr/local/volumes/mysql:/bitnami/mysql environment: - TZ=Asia/Shanghai - MYSQL_ROOT_USER=root - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=go-layout - MYSQL_CHARACTER_SET=utf8mb4 - MYSQL_COLLATE=utf8mb4_general_ci #### redis #### redis: image: redis:7.0.0 container_name: redis restart: always ports: - \u0026#34;6379:6379\u0026#34; command: redis-server --appendonly yes --requirepass \u0026#34;123456\u0026#34; volumes: - /usr/local/volumes/redis/redis.conf:/etc/redis.conf - /usr/local/volumes/redis/data:/data environment: - TZ=Asia/Shanghai #### jaeger #### jaeger: image: jaegertracing/all-in-one:1.37 container_name: jaeger restart: always ports: - \u0026#34;5775:5775/udp\u0026#34; - \u0026#34;6831:6831/udp\u0026#34; - \u0026#34;6832:6832/udp\u0026#34; - \u0026#34;5778:5778\u0026#34; - \u0026#34;14268:14268\u0026#34; - \u0026#34;9411:9411\u0026#34; - \u0026#34;16686:16686\u0026#34; environment: - TZ=Asia/Shanghai - COLLECTOR_ZIPKIN_HTTP_PORT=9411 #### prometheus #### ## 注意数据集data要保证其他用户可读写，直接设置777 prometheus: image: bitnami/prometheus:2.37.0 container_name: prometheus restart: always ports: - \u0026#34;9090:9090\u0026#34; volumes: - /usr/local/volumes/prometheus/prometheus.yml:/opt/bitnami/prometheus/conf/prometheus.yml - /usr/local/volumes/prometheus/data:/opt/bitnami/prometheus/data environment: - TZ=Asia/Shanghai #### grafana #### ##注意数据集data要保证其他用户可读写，直接设置777 ## grafana重设密码 ## docker exec --user 472 -it grafana /bin/bash ## cd /usr/share/grafana/bin ## ./grafana-cli admin reset-admin-password admin grafana: image: bitnami/grafana:9.2.1 container_name: grafana restart: always ports: - \u0026#34;3000:3000\u0026#34; volumes: - /usr/local/volumes/grafana/data:/opt/bitnami/grafana/data - /usr/local/volumes/grafana/grafana.ini:/opt/bitnami/grafana/conf/grafana.ini environment: - TZ=Asia/Shanghai depends_on: - prometheus #### zookeeper #### zookeeper: image: bitnami/zookeeper:3.7.1 container_name: zookeeper restart: always ports: - \u0026#34;2181:2181\u0026#34; environment: - TZ=Asia/Shanghai - ALLOW_ANONYMOUS_LOGIN=yes #### kafka #### kafka: image: bitnami/kafka:3.0.2 container_name: kafka restart: always user: root ports: - \u0026#39;9092:9092\u0026#39; volumes: - /usr/local/volumes/kafka:/bitnami/kafka environment: - TZ=Asia/Shanghai - KAFKA_BROKER_ID=1 - KAFKA_LISTENERS=PLAINTEXT://:9092 - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://172.21.0.2:9092 - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper #### elasticsearch #### # 不停重启，数据卷的data目录权限应root，777权限 # 增加安全认证：在配置文件挂在或者容器内的elasticsearch.yml添加： # xpack.security.enabled: true # xpack.license.self_generated.type: basic # xpack.security.transport.ssl.enabled: true # 在cd bin 然后elasticsearch-setup-passwords interactive 回车输入y，填写密码，重启容器 elasticsearch: image: elasticsearch:7.17.0 container_name: elasticsearch restart: always ports: - \u0026#34;9200:9200\u0026#34; - \u0026#34;9300:9300\u0026#34; volumes: - /usr/local/volumes/elasticsearch/data:/usr/share/elasticsearch/data - /usr/local/volumes/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml environment: - TZ=Asia/Shanghai - discovery.type=single-node - ES_JAVA_OPTS=-Xms512m -Xmx512m #### kibana #### kibana: image: kibana:7.17.0 container_name: kibana restart: always ports: - \u0026#34;5601:5601\u0026#34; volumes: - /usr/local/volumes/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml environment: - elasticsearch.hosts=http://172.21.0.2:9200 depends_on: - elasticsearch #### consul #### consul: image: bitnami/consul:1.13.0 container_name: consul restart: always ports: - \u0026#34;8300:8300\u0026#34; - \u0026#34;8500:8500\u0026#34; - \u0026#34;8600:8600/udp\u0026#34; volumes: - /usr/local/volumes/consul:/bitnami/consul environment: - TZ=Asia/Shanghai - CONSUL_AGENT_MODE=server - CONSUL_ENABLE_UI=true - CONSUL_BOOTSTRAP_EXPECT=1 - CONSUL_CLIENT_LAN_ADDRESS=0.0.0.0 #### nginx #### #dist解压到/usr/local/volumes/nginx/html目录下 nginx: image: nginx:1.22 container_name: nginx restart: always ports: - \u0026#34;8080:8080\u0026#34; volumes: - /usr/local/volumes/nginx/conf.d:/etc/nginx/conf.d - /usr/local/volumes/nginx/nginx.conf:/etc/nginx/nginx.conf - /usr/local/volumes/nginx/html:/usr/share/nginx/html environment: - TZ=Asia/Shanghai consul # 配置ACL(容器的/opt/bitnami/consul/conf/consul.conf添加：) # \u0026#34;acl\u0026#34;:{ \u0026#34;enabled\u0026#34;:true, \u0026#34;default_policy\u0026#34;:\u0026#34;deny\u0026#34;, \u0026#34;enable_token_persistence\u0026#34;:true } 获取root token #重启后，容器内执行 consul acl bootstrap //将SecretID(Token)赋值到配置中 \u0026#34;acl\u0026#34;:{ \u0026#34;enabled\u0026#34;:true, \u0026#34;default_policy\u0026#34;:\u0026#34;deny\u0026#34;, \u0026#34;enable_token_persistence\u0026#34;:true, \u0026#34;tokens\u0026#34;: { \u0026#34;master\u0026#34;: \u0026#34;ac9b7b85-8819-cffb-c3f6-1bbd43ca1402\u0026#34; } } 重置acl # 官方介绍 https://developer.hashicorp.com/consul/tutorials/security/access-control-troubleshoot#reset-the-acl-system # 13为reset index echo 13 \u0026gt;\u0026gt; bitnami/consul/acl-bootstrap-reset "},{"id":19,"href":"/docs/docs/11_git/","title":"git","section":"Docs","content":" 命令 # git remote -v //查看远程管理分支 git remote rm origin //删除关联的origin远程库 git branch //查看分支 git branch -D 分支 //强制删除分支 git push -f origin test:test //强制推送 git checkout -b 分支 //创建分支，并进入分支 git checkout 分支 //切换分支 git merge dev //将分支dev合并到当前分支内(切换到目标分支) git reset --hard 030bfd0393a55ead2c6b9a3dc3312a6932e9d0ae //代码回退到某一个时间戳 git log //查看commit记录 git remote prune origin //远程、本地已经删除分支，本地缓存还有分支名称 git reset --soft HEAD^ //撤销本次commit，不删除git add添加的内容 git reset --soft HEAD~2 //撤销最近两次的commit，不删除git add添加的内容 git commit --amend //修改注释 这时候会进入vim编辑器，修改完成你要的注释后保存即可 git reflog //显示所有发生的commit git branch dev_xhubai_core_unitTest HEAD@{692} // 找回本地删除的分支 git merge --squash 分支 //压缩合并commit git fetch --tags //拉取远程tag git tag v0.0.1 //本地打tag git push origin v0.0.1 //提交tag git push origin --tags //提交所有未提交tag git tag -d $(git tag -l) //删除本地所有tag git push origin --delete $(git tag -l) //删除远程tag git stash //保存当前未commit的代码 git stash pop //应用最近一次的stash，随后删除该记录 git revert 21dcd937fe555f58841b17466a99118deb489212 //还原自己提交的commit代码(用于线上bug代码回退，不影响同事提交代码) git symbolic-ref --short -q HEAD //获取当前的分支名称 配置项目到远端 # $ git remote add origin git@github.com:xiaohubai/go-grpc-layout.git 分支命名 # master:\t主分支 develop:\t开发分支 feature:\t功能分支 release：\t预发布分支 hotfix:\t线上紧急修复分支 fixbug:\t日常bug修复 //类型/开发者_几月几号几点_描述 feature/xiaohubai_050112_casbin fixbug/xiaohubai_050115_casbin_role_error commit # feat: 新功能 fix: bug修复 perf: 改进性能 style: 不影响代码含义的更改，比如只是增加了空格，格式化了代码，增加了缺少的分号等 docs: 文档 refactor: 重构 test: 测试 chore: 构建过程或者增加依赖库、工具等 revert: 回滚上一个版本 类型(作用范围): 简短描述 1.描述点 ssues号/TAPD号 "},{"id":20,"href":"/docs/docs/12_go/","title":"go","section":"Docs","content":" 类型转换 # string # 1. int int8 int16 int32 str:=strconv.Itoa(int(value)) 2. int64 str:=strconv.FormatInt(value, 10) 3. uint uint8 uint16 uint32 str:=strconv.FormatUint(uint64(value), 10) 4. uint64 str:=strconv.FormatUint(value, 10) 5. float32 str:=strconv.FormatFloat(float64(value), \u0026#39;f\u0026#39;, -1, 32) 6. float64 str:=strconv.FormatFloat(value, \u0026#39;f\u0026#39;, -1, 64) 7. bool str:=strconv.FormatBool(value) 8. []byte str:=string(value) 9. time.Time if value.IsZero() { return \u0026#34;\u0026#34; } return value.String() 10. *time.Time if value == nil { return \u0026#34;\u0026#34; } return value.String() int # 1. int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 float32 float64 i:=int(int64(value)) 2. bool if value { return 1 } return 0 3. string []byte f64,_:=strconv.ParseFloat(string(value), 64) i:=int(f64) uint # 1. int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 float32 float64 ui:=uint64(value) 2. bool if value { return 1 } return 0 3. string []byte u64,err:=strconv.ParseUint(string(value), 10, 64) ui:=uint(u64) float64 # 1. int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 float32 f64:=float64(value) 2. string f64,err:=strconv.ParseFloat(value, 64) channel # var 变量 chan 元素类型 //声明 make(chan 元素类型, [缓冲大小]) var 通道实例 chan\u0026lt;- 元素类型 ch := make(\u0026lt;-chan int) //只写的单向通道 var 通道实例 \u0026lt;-chan 元素类型 ch := make(chan\u0026lt;- int) //只读的单向通道 ch := make(chan int) ch \u0026lt;- 10 //发送 x := \u0026lt;- ch //接受 close(ch) //关闭 i, ok := \u0026lt;-ch //判断通道是否关闭 chan\u0026lt;- int //是一个只能发送的通道，可以发送但是不能接收 \u0026lt;-chan int //是一个只能接收的通道，可以接收但是不能发送 close(ch) //关闭 格式化输出 # %d :整数 %f :浮点数 %s:字符串、[]byte %v:值的默认格式 %+v:类似%v,但结构体使添加字段名 %% 输出% string # ``:原始输出 分割：strings.Split(字符串，切割符) 包含：strings.Contains(字符串，子串) 拼接：fmt.Sprint(\u0026#34;%s%s\u0026#34;,字符串1，字符串2) 单侧覆盖率 # go test -coverprofile=cover.out //生成测试分析文件 go tool cover -func=cover.out //查看每个函数覆盖率 go tool cover -html=cover.out -o cover.html //查看覆盖程度，和每一个文件覆盖率 代码 # excel导入数据库 # package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/360EntSecGroup-Skylar/excelize/v2\u0026#34; \u0026#34;gorm.io/driver/mysql\u0026#34; \u0026#34;gorm.io/gorm\u0026#34; \u0026#34;gorm.io/gorm/schema\u0026#34; ) type cityToIvrnumber struct { ID int City string `json:\u0026#34;city\u0026#34;` CityNumber string `json:\u0026#34;cityNumber\u0026#34;` Dtmf string `json:\u0026#34;dtnf\u0026#34;` CityIvrNumber string `json:\u0026#34;cityIvrNumber\u0026#34;` Reserved_1 string `json:\u0026#34;reserved_1\u0026#34;` Reserved_2 string `json:\u0026#34;reserved_2\u0026#34;` Reserved_3 string `json:\u0026#34;reserved_3\u0026#34;` Reserved_4 string `json:\u0026#34;reserved_4\u0026#34;` Reserved_5 string `json:\u0026#34;reserved_5\u0026#34;` } func main() { dsn := \u0026#34;root:123456@tcp(127.0.0.1:3306)/sdp?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34; db, err := gorm.Open(mysql.Open(dsn), \u0026amp;gorm.Config{NamingStrategy: schema.NamingStrategy{SingularTable: true}}) if err != nil { fmt.Println(\u0026#34;连接数据库失败\u0026#34;) os.Exit(0) } excel, err := excelize.OpenFile(\u0026#34;aaa.xlsx\u0026#34;) if err != nil { fmt.Println(err) return } sheetList := excel.GetSheetList() for i := 1; i \u0026lt;= 2; i++ { rows, err := excel.GetRows(sheetList[i]) if err != nil { return } for j := 1; j \u0026lt; len(rows); j++ { row := rows[j] if len(row) == 0 || row[1] == \u0026#34;\u0026#34; { continue } var city cityToIvrnumber if result := db.Where(\u0026#34;city_number=? and dtmf=? and city_ivr_number=?\u0026#34;, row[1], strconv.Itoa(i), strings.Replace(row[3], \u0026#34;+VCCID+\u0026#34;, \u0026#34;500505\u0026#34;, -1)).Find(\u0026amp;city); result.RowsAffected \u0026gt; 0 || result.Error != nil { continue } city = cityToIvrnumber{ City: row[0], CityNumber: row[1], Dtmf: strconv.Itoa(i), CityIvrNumber: strings.Replace(row[3], \u0026#34;+VCCID+\u0026#34;, \u0026#34;500505\u0026#34;, -1), } if err := db.Create(\u0026amp;city).Error; err != nil { fmt.Println(\u0026#34;插入失败\u0026#34;) } } } fmt.Printf(\u0026#34;导入完成！\u0026#34;) } 递归遍历子目录和读取文件行内容 # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) func main() { path := \u0026#34;./a\u0026#34; err := findDir(path) if err != nil { fmt.Println(err) } } func findDir(path string) error { dirs, err := ioutil.ReadDir(path) if err != nil { fmt.Println(path, \u0026#34;打开失败\u0026#34;) return err } for _, dirOrFile := range dirs { if dirOrFile.IsDir() { err := findDir(path + \u0026#34;/\u0026#34; + dirOrFile.Name()) if err != nil { fmt.Println(path, \u0026#34;打开失败\u0026#34;) return err } } else { file, err := os.Open(path + \u0026#34;/\u0026#34; + dirOrFile.Name()) if err != nil { return err } defer file.Close() row := bufio.NewReader(file) for { rowText, _, err := row.ReadLine() if err != nil || err == io.EOF { break } fmt.Println(string(rowText)) } } } return nil } 通用数据库查询 # func BaseSql(req interface{}, table interface{}) (interface{}, error) { if reflect.TypeOf(table).Kind() == reflect.Struct { t := reflect.TypeOf(table).Name() key := reflect.TypeOf(table) value := reflect.ValueOf(table) tx := config.G_DB.Where(\u0026#34;1 = ?\u0026#34;, 1) for i := 0; i \u0026lt; reflect.ValueOf(table).NumField(); i++ { k := key.Field(i).Name v := value.Field(i).Interface() where := fmt.Sprintf(\u0026#34;%s = ?\u0026#34;, k) switch v { case v.(string): if v.(string) != \u0026#34;\u0026#34; { tx = tx.Where(where, v) } case v.(int), v.(float64): if v.(int) != 0 || v.(float64) != 0 { tx = tx.Where(where, v) } } } err := tx.Find(\u0026amp;t).Error return t, err } return nil, nil } 结构体参数正则校验 # var regexpText = map[string]string{ //字母、数字、下划线、横线 \u0026#34;AppName\u0026#34;: \u0026#34;^[A-Za-z0-9_-]+$\u0026#34;, \u0026#34;Os\u0026#34;: \u0026#34;^[A-Za-z0-9_-]+$\u0026#34;, //数字、点号 \u0026#34;OsVersion\u0026#34;: \u0026#34;^[0-9.]+$\u0026#34;, \u0026#34;AppVersion\u0026#34;: \u0026#34;^[0-9.]+$\u0026#34;, //数字 \u0026#34;RequestTime\u0026#34;: \u0026#34;^[0-9]+$\u0026#34;, \u0026#34;LoginTypeCode\u0026#34;: \u0026#34;^[0-9]+$\u0026#34;, \u0026#34;ScreenHeight\u0026#34;: \u0026#34;^[0-9]+$\u0026#34;, \u0026#34;ScreenWidth\u0026#34;: \u0026#34;^[0-9]+$\u0026#34;, \u0026#34;IsJailbreak\u0026#34;: \u0026#34;^[0-9]+$\u0026#34;, //数字、点号、冒号、字母 \u0026#34;Ip\u0026#34;: \u0026#34;^[A-Fa-f0-9.:]+$\u0026#34;, //数字、字母 \u0026#34;Uid\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, \u0026#34;Suid\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, \u0026#34;OmgBizId\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, \u0026#34;Qimei36\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, \u0026#34;MaterialId\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, \u0026#34;AudienceId\u0026#34;: \u0026#34;^[A-Za-z0-9]+$\u0026#34;, //字母、数字、横线 \u0026#34;Qimei\u0026#34;: \u0026#34;^[A-Za-z0-9-]+$\u0026#34;, \u0026#34;Idfa\u0026#34;: \u0026#34;^[A-Za-z0-9-]+$\u0026#34;, \u0026#34;DevId\u0026#34;: \u0026#34;^[A-Za-z0-9-]+$\u0026#34;, } func RPCReportValidator(rpcReq pb.RiskRequest) error { if rpcReq.Suid == \u0026#34;\u0026#34; \u0026amp;\u0026amp; rpcReq.Uid == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;suidAndUid_is_empty\u0026#34;) } if rpcReq.Qimei36 == \u0026#34;\u0026#34; \u0026amp;\u0026amp; rpcReq.Qimei == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;qimei36Andqimei_is_empty\u0026#34;) } keys := reflect.TypeOf(rpcReq) values := reflect.ValueOf(rpcReq) for i := 0; i \u0026lt; values.NumField(); i++ { key := keys.Field(i).Name value := values.Field(i).String() if reg, ok := regexpText[key]; ok \u0026amp;\u0026amp; value != \u0026#34;\u0026#34; { isMatched, err := regexp.MatchString(reg, value) if err != nil || !isMatched { return fmt.Errorf(\u0026#34;%s_value_not_matched\u0026#34;, key) } } } return nil } 控制协程运行时间 # func Go(ctx context.Context, timeout time.Duration, handler func(context.Context)) error { oldMsg := codec.Message(ctx) newCtx, newMsg := codec.WithNewMessage(detach(ctx)) codec.CopyMsg(newMsg, oldMsg) newCtx, cancel := context.WithTimeout(newCtx, timeout) go func() { defer func() { if e := recover(); e != nil { buf := make([]byte, PanicBufLen) buf = buf[:runtime.Stack(buf, false)] log.Errorf(\u0026#34;[PANIC]%v\\n%s\\n\u0026#34;, e, buf) report.PanicNum.Incr() } cancel() }() handler(newCtx) }() return nil } 文件操作 # 创建空文件 # newFile, err := os.Create(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } log.Println(newFile) newFile.Close() 裁剪文件 # err := os.Truncate(\u0026#34;test.txt\u0026#34;, 100) //0：清空文件 if err != nil { log.Fatal(err) } 获取文件信息 # // 如果文件不存在，则返回错误 fileInfo, err := os.Stat(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;File name:\u0026#34;, fileInfo.Name()) fmt.Println(\u0026#34;Size in bytes:\u0026#34;, fileInfo.Size()) fmt.Println(\u0026#34;Permissions:\u0026#34;, fileInfo.Mode()) fmt.Println(\u0026#34;Last modified:\u0026#34;, fileInfo.ModTime()) fmt.Println(\u0026#34;Is Directory: \u0026#34;, fileInfo.IsDir()) fmt.Printf(\u0026#34;System interface type: %T\\n\u0026#34;, fileInfo.Sys()) fmt.Printf(\u0026#34;System info: %+v\\n\\n\u0026#34;, fileInfo.Sys()) 删除文件 # err := os.Remove(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } 打开文件 # //只读方式 file, err := os.Open(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } file.Close() // os.O_RDONLY // 只读 // os.O_WRONLY // 只写 // os.O_RDWR // 读写 // os.O_APPEND // 往文件中添建（Append） // os.O_CREATE // 如果文件不存在则先创建 // os.O_TRUNC // 文件打开时裁剪文件 // os.O_EXCL // 和O_CREATE一起使用，文件不能存在 // os.O_SYNC // 以同步I/O的方式打开 file, err = os.OpenFile(\u0026#34;test.txt\u0026#34;, os.O_APPEND, 0666) if err != nil { log.Fatal(err) } file.Close() 检查文件是否存在 # fileInfo, err := os.Stat(\u0026#34;test.txt\u0026#34;) if err != nil { if os.IsNotExist(err) { log.Fatal(\u0026#34;File does not exist.\u0026#34;) } } 检查读写权限 # file, err := os.OpenFile(\u0026#34;test.txt\u0026#34;, os.O_WRONLY, 0666) if err != nil { if os.IsPermission(err) { log.Println(\u0026#34;Error: Write permission denied.\u0026#34;) } } file.Close() file, err = os.OpenFile(\u0026#34;test.txt\u0026#34;, os.O_RDONLY, 0666) if err != nil { if os.IsPermission(err) { log.Println(\u0026#34;Error: Read permission denied.\u0026#34;) } } file.Close() 复制文件 # // 打开原始文件 originalFile, err := os.Open(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } defer originalFile.Close() // 创建新的文件作为目标文件 newFile, err := os.Create(\u0026#34;test_copy.txt\u0026#34;) if err != nil { log.Fatal(err) } defer newFile.Close() // 从源中复制字节到目标文件 bytesWritten, err := io.Copy(newFile, originalFile) if err != nil { log.Fatal(err) } log.Printf(\u0026#34;Copied %d bytes.\u0026#34;, bytesWritten) // 将文件内容flush到硬盘中 err = newFile.Sync() if err != nil { log.Fatal(err) } 写文件 # // 可写方式打开文件 file, err := os.OpenFile( \u0026#34;test.txt\u0026#34;, os.O_WRONLY|os.O_TRUNC|os.O_CREATE, 0666, ) if err != nil { log.Fatal(err) } defer file.Close() // 写字节到文件中 byteSlice := []byte(\u0026#34;Bytes!\\n\u0026#34;) bytesWritten, err := file.Write(byteSlice) if err != nil { log.Fatal(err) } log.Printf(\u0026#34;Wrote %d bytes.\\n\u0026#34;, bytesWritten) 读取全部字节 # file, err := os.Open(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } // os.File.Read(), io.ReadFull() 和 // io.ReadAtLeast() 在读取之前都需要一个固定大小的byte slice。 // 但ioutil.ReadAll()会读取reader(这个例子中是file)的每一个字节，然后把字节slice返回。 data, err := ioutil.ReadAll(file) if err != nil { log.Fatal(err) } 快读到内存 # // 读取文件到byte slice中 data, err := ioutil.ReadFile(\u0026#34;test.txt\u0026#34;) if err != nil { log.Fatal(err) } log.Printf(\u0026#34;Data read: %s\\n\u0026#34;, data) 打包(zip) 文件 # // 创建一个打包文件 outFile, err := os.Create(\u0026#34;test.zip\u0026#34;) if err != nil { log.Fatal(err) } defer outFile.Close() // 创建zip writer zipWriter := zip.NewWriter(outFile) // 往打包文件中写文件。 // 这里我们使用硬编码的内容，你可以遍历一个文件夹，把文件夹下的文件以及它们的内容写入到这个打包文件中。 var filesToArchive = []struct { Name, Body string } { {\u0026#34;test.txt\u0026#34;, \u0026#34;String contents of file\u0026#34;}, {\u0026#34;test2.txt\u0026#34;, \u0026#34;\\x61\\x62\\x63\\n\u0026#34;}, } // 下面将要打包的内容写入到打包文件中，依次写入。 for _, file := range filesToArchive { fileWriter, err := zipWriter.Create(file.Name) if err != nil { log.Fatal(err) } _, err = fileWriter.Write([]byte(file.Body)) if err != nil { log.Fatal(err) } } // 清理 err = zipWriter.Close() if err != nil { log.Fatal(err) } 解压（zip）文件 # zipReader, err := zip.OpenReader(\u0026#34;test.zip\u0026#34;) if err != nil { log.Fatal(err) } defer zipReader.Close() // 遍历打包文件中的每一文件/文件夹 for _, file := range zipReader.Reader.File { // 打包文件中的文件就像普通的一个文件对象一样 zippedFile, err := file.Open() if err != nil { log.Fatal(err) } defer zippedFile.Close() // 指定抽取的文件名。 // 你可以指定全路径名或者一个前缀，这样可以把它们放在不同的文件夹中。 // 我们这个例子使用打包文件中相同的文件名。 targetDir := \u0026#34;./\u0026#34; extractedFilePath := filepath.Join( targetDir, file.Name, ) // 抽取项目或者创建文件夹 if file.FileInfo().IsDir() { // 创建文件夹并设置同样的权限 log.Println(\u0026#34;Creating directory:\u0026#34;, extractedFilePath) os.MkdirAll(extractedFilePath, file.Mode()) } else { //抽取正常的文件 log.Println(\u0026#34;Extracting file:\u0026#34;, file.Name) outputFile, err := os.OpenFile( extractedFilePath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, file.Mode(), ) if err != nil { log.Fatal(err) } defer outputFile.Close() // 通过io.Copy简洁地复制文件内容 _, err = io.Copy(outputFile, zippedFile) if err != nil { log.Fatal(err) } } } 压缩gz文件 # outputFile, err := os.Create(\u0026#34;test.txt.gz\u0026#34;) if err != nil { log.Fatal(err) } gzipWriter := gzip.NewWriter(outputFile) defer gzipWriter.Close() // 当我们写如到gizp writer数据时，它会依次压缩数据并写入到底层的文件中。 // 我们不必关心它是如何压缩的，还是像普通的writer一样操作即可。 _, err = gzipWriter.Write([]byte(\u0026#34;Gophers rule!\\n\u0026#34;)) if err != nil { log.Fatal(err) } log.Println(\u0026#34;Compressed data written to file.\u0026#34;) 解压缩gz文件 # gzipFile, err := os.Open(\u0026#34;test.txt.gz\u0026#34;) if err != nil { log.Fatal(err) } gzipReader, err := gzip.NewReader(gzipFile) if err != nil { log.Fatal(err) } defer gzipReader.Close() // 解压缩到一个writer,它是一个file writer outfileWriter, err := os.Create(\u0026#34;unzipped.txt\u0026#34;) if err != nil { log.Fatal(err) } defer outfileWriter.Close() // 复制内容 _, err = io.Copy(outfileWriter, gzipReader) if err != nil { log.Fatal(err) } 创建临时文件夹、文件 # // 在系统临时文件夹中创建一个临时文件夹 tempDirPath, err := ioutil.TempDir(\u0026#34;\u0026#34;, \u0026#34;myTempDir\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Temp dir created:\u0026#34;, tempDirPath) // 在临时文件夹中创建临时文件 tempFile, err := ioutil.TempFile(tempDirPath, \u0026#34;myTempFile.txt\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Temp file created:\u0026#34;, tempFile.Name()) // 关闭文件 err = tempFile.Close() if err != nil { log.Fatal(err) } // 删除我们创建的资源 err = os.Remove(tempFile.Name()) if err != nil { log.Fatal(err) } err = os.Remove(tempDirPath) if err != nil { log.Fatal(err) } 方法重写 # package main import \u0026#34;fmt\u0026#34; type SaySomething interface { SayHi2() } type Human2 struct { name string age int phone string } func Say2(say SaySomething) { say.SayHi2() } func (h Human2) SayHi2() { fmt.Printf(\u0026#34;Hi, I am %s you can call me on %s\\n\u0026#34;, h.name, h.phone) } type Employee2 struct { Human2 company string } //Employee的method重写Human的method func (e Employee2) SayHi2() { fmt.Printf(\u0026#34;Hi, I am %s, I work at %s. Call me on %s\\n\u0026#34;, e.name, e.company, e.phone) //Yes you can split into 2 lines here. } func main() { mark := Human2{\u0026#34;Mark\u0026#34;, 25, \u0026#34;222-222-YYYY\u0026#34;} sam := Employee2{Human2{\u0026#34;Sam\u0026#34;, 45, \u0026#34;111-888-XXXX\u0026#34;}, \u0026#34;Golang Inc\u0026#34;} mark.SayHi2() Say2(mark) fmt.Println(\u0026#34;==============\u0026#34;) sam.SayHi2() Say2(sam) } 汉字转拼音 # var ( newFileName = \u0026#34;\u0026#34; ) func main() { file_name, err := ioutil.ReadFile(\u0026#34;写入要转换的文件名.txt\u0026#34;) if err != nil { log.Fatal(err) } newFileName = strings.TrimSpace(string(file_name)) f, err := excelize.OpenFile(\u0026#34;./\u0026#34; + newFileName + \u0026#34;.xlsx\u0026#34;) if err != nil { fmt.Println(err) return } defer func() { // Close the spreadsheet. if err := f.Close(); err != nil { fmt.Println(err) } }() // Get all the rows in the Sheet1. rows, err := f.GetRows(\u0026#34;Sheet1\u0026#34;) if err != nil { fmt.Println(err) return } sss := []string{} for _, row := range rows { ss := \u0026#34;\u0026#34; fmt.Println(row[0]) for _, r := range row[0] { if unicode.Is(unicode.Han, r) { py := PYin(string(r)) ss = ss + py + \u0026#34;\u0026#39;\u0026#34; } else { ss = ss + string(r) } } sss = append(sss, row[0], ss) } WriteFile(sss) time.Sleep(time.Hour) } func PYin(data string) string { a := pinyin.NewArgs() pinyins := pinyin.Pinyin(data, a) for _, py := range pinyins { return py[0] } return \u0026#34;\u0026#34; } func WriteFile(data []string) { // 可写方式打开文件 file, err := os.OpenFile(\u0026#34;./\u0026#34;+ newFileName+\u0026#34;拼音.txt\u0026#34;, os.O_WRONLY|os.O_TRUNC|os.O_CREATE|os.O_APPEND, 0666, ) if err != nil { log.Fatal(err) } defer file.Close() write := bufio.NewWriter(file) for _, line := range data { _, _ = write.WriteString(line + \u0026#34;\\n\u0026#34;) } write.Flush() } errgroup # var g errgroup.Group g.Go(func() error { return nil }) if err := g.Wait(); err != nil { email.SendWarn(context.Background(), err) log.Errorw(\u0026#34;key\u0026#34;, \u0026#34;warn\u0026#34;, \u0026#34;msg\u0026#34;, err.Error()) } "},{"id":21,"href":"/docs/docs/13_mysql/","title":"mysql","section":"Docs","content":" mysql基础 # 事务ACID # A:Atomicity (原子性)：事务中的操作要么都发生，要么都不发生。 C:Consistency(一致性)：事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。 I:Isolation(隔离性)：多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 D:Durability(持久性)：，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 事务四种隔离级别 # RU :Read uncommitted：(读未提交)：最低级别，任何情况都无法保证。可能产生脏读，不可重复读，幻读． RC :Read committed : (读已提交)：可避免脏读的发生。 RR :Repeatable read: (可重复读)即在一个事务读取数据的过程中，其他事务不允许修改数据，所以解决了不可重复读． S :Serializable (串行化)：最高的隔离级别,某一时刻只能有一个事务操作数据库 脏读 不可重复读 幻读 # 脏读: ：事务A读取数据库某一数据，事务B在此过程中修改了该数据，但没有提交，此时事务A读取的可能是脏数据，一旦事务B回滚，事务A便是脏读． 不可重复读：事务A需要在此次事务中多次读取同一个数据，在此期间，事务B修改了该数据并提交，导致事务A多次读取的数据不一样，因此称为不可重复读． 幻读：涉及两个数据库的更新操作，比如事务A更新整个数据库，还未更新完时，事务B在此过程中插入一行，在此之后，事务A发现数据库中还有一行没有更新，好像产生了幻觉一样． 乐观锁 悲观锁 # //乐观锁 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据 //悲观锁 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作 table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突 每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 乐观锁、悲观锁区别？ # 乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量 但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁 共享锁 # //执行语句后 加上 lock in share mode 对于多个不同的事务，对同一资源共享同一个锁（一个门上一把锁，多个钥匙），只用于select，对于update,insert,delete语句会自动加排它锁 排他锁 # //执行语句后加上 for update 对于多个不同的事务，对同一个资源只能有一把锁。 行锁 # MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住. 表锁 # 给这个表加上锁。 函数 # substring() //提取字符串一部分 convert()//数据类型转换 len()字符串长度 lower()字符串转为小写 upper()字符串转为大写 ltrim()去除字符串左空格 rtrim()去除字符串右空格 substr()提取字符串部分 avg()某列平均值 count()某列行数 //count(*)不过滤null,count(age)过滤age的null max()某列最大值 min()某列最小值 sum()求和 查询语法 # order by desc/asc :排序 降/升（放置最后） # select user from test order by user,age desc; limit # select user from test limit 1; //:取一个 select user from test limit 1,3; //:从1开始，取3个 select user from test limit 3 offset 1; //从第1行开始，取三个(limit 1,3 == limit 3 offset 1) join # select a.*,b.* from test1 as a left join test2 as b on a.user =b.user where a.age \u0026lt;=20 //:left join select a.*,b.* from test1 as a right join test2 as b on a.user =b.user where a.age \u0026lt;=20 //:right join select a.*,b.* from test1 as a inner join test2 as b on a.user =b.user where a.age \u0026lt;=20 //:inner join distinct # select distinct user from test; //去重 select distinct user,age from test; //组合去重 in # select user from test where user in (\u0026#39;aa\u0026#39;,\u0026#39;bb\u0026#39;); //代替or,里面的任何一个 select user from test where user not in (\u0026#39;aa\u0026#39;,\u0026#39;bb\u0026#39;);//代替or,不在里面的任何一个 like # select user from test where user like \u0026#39;a%\u0026#39; //a开头的任意字符 （区分大小写） select user from test where user like \u0026#39;F%y\u0026#39;//F开头y结尾的任意字符 select user from test where user like \u0026#39;a_\u0026#39; //a开头匹配一个 连接两个字段成一个值，去除空格 # select concat(user,rtrim(age)) as aa from test; group by # select user from test group by age having age\u0026gt;=18; //having 作用于分组条件 子查询 # select user from test where user in (select customer from test1 where customer_age \u0026gt; 18); select user,(select cost from test1 where test.id=test1.id) as user_cost from test ; select * from test union select * from customer; //自动压缩多个结果集合中的重复结果(去重) select * from test union all select * from customer; //将所有的结果全部显示出来,可重复 insert # insert into test values(\u0026#34;aa\u0026#34;,18,\u0026#34;13269118806\u0026#34;); insert into test(user,age) values(\u0026#34;aa\u0026#34;,18); create table new_table as select * from test; //创建一个新表，并将test表内容复制到new_table表中 update # update test set age=20,phone=\u0026#34;132731907\u0026#34; where user = \u0026#34;aa\u0026#34;; delete # delete from test where user = \u0026#34;aa\u0026#34;; //删除一行 delete from test; //清空表 创建表结构 # //表要有： 创建时间、更新时间、state（0初始，1使用，2 3停用.4删除）创建人，更新人 create table `test`( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `uuid` varchar(180) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;uuid\u0026#39;, `user` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;用户名\u0026#39;, `passwd` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;登录密码\u0026#39;, `age` int(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;年龄\u0026#39;, `cost` double(20, 4) NOT NULL DEFAULT \u0026#39;0.0000\u0026#39; COMMENT \u0026#39;花费\u0026#39;, `download_count` bigint(20) unsigned NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;下载量\u0026#39;, `date` date NOT NULL DEFAULT \u0026#39;2006-01-02\u0026#39; COMMENT \u0026#39;日期\u0026#39;, `state` int(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;0:初始,1:使用,2:保留,3:停用，4:删除\u0026#39;, `commit` text NOT NULL COMMENT \u0026#39;\u0026#39;, `create_user` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;创建人\u0026#39;, `update_user` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;修改人\u0026#39;, `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;记录创建时间\u0026#39;, `update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;记录修改时间\u0026#39;, `zone_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;创建时间戳，设置mysql地区时间，会相应变化\u0026#39;, PRIMARY KEY (`id`), KEY `idx_key_value` (`user`, `cost`, `download_count`) ) ENGINE = InnoDB AUTO_INCREMENT = 100 DEFAULT CHARSET = utf8mb4 COMMENT = \u0026#39;测试表\u0026#39;; 设置地区时间 # mysql\u0026gt; SET time_zone = \u0026#39;America/Los_Angeles\u0026#39;; mysql\u0026gt; SET time_zone = \u0026#39;Asia/Shanghai\u0026#39;; 更改表 # alter table test drop age; //删除test表的age字段 alter table test add phone varchar(20) NOT NULL COMMENT \u0026#39;手机号\u0026#39;; //新增字段 alter table test change phone phone int(11);//修改数据类型 alter table test change phone call_number varchar(20);//修改字段名 alter table test alter phone set default \u0026#39;\u0026#39;; //修改默认值 alter table test add index idx_phone (phone,age); //新建索引(unique,primary key,fulltext,index) alter table test drop index idx_phone; //删除索引 rename table test to test1; //重命名表 删除表 # drop table test; 高级查询 # case when # SELECT round(AVG(u1),0) AS new_user ,round(SUM(u3) / SUM(u2), 4) AS re_1 ,round(SUM(u5) / SUM(u4), 4) AS re_30,sum(new_user_no_filter)-sum(u1) AS filter case when dim=\u0026#39;huawei-store\u0026#39; then \u0026#39;华为商店\u0026#39; when dim=\u0026#39;vivo-online\u0026#39; then \u0026#39;OPPO线上\u0026#39; else dim end as dim FROM table1 WHERE ds \u0026gt;=\u0026#39;2021-09-10\u0026#39; AND ds \u0026gt;= \u0026#39;2021-09-10\u0026#39; AND dim_type in (\u0026#39;channel_cate2\u0026#39;, \u0026#39;channel_cate1\u0026#39; ) and (source in(\u0026#39;huawei\u0026#39;, \u0026#39;oppo\u0026#39;, \u0026#39;vivo\u0026#39;) or source rlike \u0026#39;^ios_ys\u0026#39; or source rlike \u0026#39;^ios_yw\u0026#39;) GROUP BY dim order by ds desc LIMIT 100000 // like 不是正则，默认全匹配；rlike 子匹配，可以是正则，特殊字符需要转义 类似 rlike \u0026#39;hu\u0026#39;=like %hu% join # select t1.*,t2.dau, if(t2.value is not null, t2.value, t1.value)as value from ( select new_date,channel_id as sub_category ,round(max((if(data_type=0,uv,0))*100),2) as u0 , max(if(data_type=1,uv,0)) as u1 , max(if(data_type=7,uv,0)) as u7 , max(if(data_type=30,uv,0)) as u30 from table1 where new_date=\u0026#34;2021-09-10\u0026#34; group by new_date,channel_id )t1 left join ( select new_date,channel_id as sub_category, max(if(data_type=0,uv_valid,0)) as dau from table2 where new_date=\u0026#34;2021-09-10\u0026#34; group by new_date,channel_id )t2 on t1.new_date=t2.new_date and t1.sub_category=t2.sub_category 多表+子查询 # select Student.Sname,course.cname,score from Student,SC,Course ,Teacher where Student.s_id=SC.s_id and SC.c_id=Course.c_id and sc.t_id=teacher.t_id and Teacher.Tname=\u0026#39;tname553\u0026#39; and SC.score=(select max(score)from SC where sc.t_id=teacher.t_Id); 查询 参数是否是表字段的子串 # SELECT * FROM `menu` WHERE FIND_IN_SET(\u0026#39;0\u0026#39;,`menu`.`roleIDs`) "},{"id":22,"href":"/docs/docs/14_gorm/","title":"gorm","section":"Docs","content":" 代码生成 # gentool -dsn \u0026#34;root:123456@tcp(172.21.0.2:3306)/go-layout?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34; --modelPkgName=\u0026#34;./internal/data/model\u0026#34; -outPath=\u0026#34;./internal/data/gen\u0026#34; 常见问题 # gorm 做更新操作 要特别注意 默认值, 再不确定更新那个struct字段时,要求请求参数全部有值,gorm进行map[string]interface{}指定全部请求参数更新 gorm 统计总数 加上 delete_at is null 删除非真删除,只是delete_at 不为null "},{"id":23,"href":"/docs/docs/15_elasticsearch/","title":"elasticsearch","section":"Docs","content":" 常用名称 # 索引（index）：数据库,一个可检索的文档对象的集合 类型（type):表,不常用，一般一个index下建一个type，默认为_doc 文档（document):一条记录 映射关系 （mapping），设置字段类型。如何索引数据 字段（Field）：doc组成部分 分片(shard)：索引的数据量太大时，需要水平拆分，分片在创建索引时创建，之后不能更改 副本(replica)：分片的 Copy，每个主分片都有一个或多个副本分片，当主分片异常时，副本可以提供数据的查询等操作。 脑裂现象 # // 原因 网络问题： 集群间的网络延迟导致一些节点访问不到 Master，认为 Master 挂掉了从而选举出新的 Master，并对 Master 上的分片和副本标红，分配新的主分片。 节点负载： 主节点的角色既为 Master 又为 Data，访问量较大时可能会导致 ES 停止响应（假死状态）造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。 内存回收： 主节点的角色既为 Master 又为 Data，当 Data 节点上的 ES 进程占用的内存较大，引发 JVM 的大规模内存回收，造成 ES 进程失去响应。 // 解决 适当调大响应时间：减少误判。 通过参数 discovery.zen.ping_timeout 设置节点状态的响应时间，默认为 3s，可以适当调大。 选举触发：我们需要在候选集群中的节点的配置文件中设置参数 discovery.zen.munimum_master_nodes 的值。这个参数表示在选举主节点时需要参与选举的候选主节点的节点数，默认值是 1，官方建议取值(master_eligibel_nodes2)+1，其中 master_eligibel_nodes 为候选主节点的个数。 角色分离：即是上面我们提到的候选主节点和数据节点进行角色分离，这样可以减轻主节点的负担，防止主节点的假死状态发生，减少对主节点“已死”的误判。 mapping # //long, integer, short, byte, double, float, half_float, scaled_float（需配合scaling_factor（缩放因子）使用，存的是*缩放因子的整数；57.34的字段缩放因子为100，存起来就是5734） PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;number_of_replicas\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;ik\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik\u0026#34; }, \u0026#34;path\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik\u0026#34; } } } } { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, //分片数量 \u0026#34;number_of_replicas\u0026#34;: 0 //副本数量 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { //字符串类型 \u0026#34;key1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; //会分词、模糊搜索 }, \u0026#34;key2\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;,//不分词、作为整体进行搜索 \u0026#34;index\u0026#34;: true, //字段是否可被搜索，true(默认)或false \u0026#34;store\u0026#34;: false //字段值是否应与_source字段分开存储和检索。 true或false(默认) }, //数值型 \u0026#34;key3\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;byte\u0026#34; //-127到128 }, \u0026#34;key4\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;short\u0026#34; //-32768到32767 }, \u0026#34;key5\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; //有符号的32位整数 }, \u0026#34;key6\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; //32位单精度 }, \u0026#34;key7\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; //双精度64位 }, \u0026#34;key8\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; //有符号的64位整数 }, //日期类型 \u0026#34;key9\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34;, //日期格式，epoch_second(秒时间戳)，epoch_millis（毫秒时间戳），日期字符串 \u0026#34;ignore_malformed\u0026#34;: false //true:格式错误的数字将被忽略(默认)；false:格式错误的数字会引发异常并拒绝整个文档。 }, //布尔类型 \u0026#34;key10\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; //false:(false,\u0026#34;false\u0026#34;,\u0026#34;off\u0026#34;,\u0026#34;no\u0026#34;,\u0026#34;0\u0026#34;,\u0026#34;\u0026#34;,0,\u0026#34;0.0\u0026#34;);true:非false值 }, \u0026#34;key11\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;binary\u0026#34; //存储Base64编码字符串的二进制，不可搜索 }, //range范围 \u0026#34;key12\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date_range\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; //gte:开始位置，lte:结束位置 }, \u0026#34;expected_attendees\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer_range\u0026#34; }, \u0026#34;time_frame\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date_range\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; } //对象类型 \u0026#34;log_1\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;key1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;key2\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; } } }, //地理坐标类型 \u0026#34;key13\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; //纬度/经度坐标 eg \u0026#34;41.12,-71.34\u0026#34; }, // ip地址 \u0026#34;key14\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34;, //索引、存储ipv4和ipv6 \u0026#34;ignore_malformed\u0026#34;: true } } } } 查询返回结果字段解析 # { \u0026#34;took\u0026#34; : 1, ----花费时长,单位毫秒 \u0026#34;timed_out\u0026#34; : false, ----是否超时 \u0026#34;_shards\u0026#34; : { --- 分片信息 // 你的搜索请求打到了几个shard上面去。 // Primary Shard可以承接读、写流量。Replica Shard会承接读流量。 \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0,// 跳过了0个 \u0026#34;failed\u0026#34; : 0//失败了0个 }, \u0026#34;hits\u0026#34; : { --- 命中的结果 \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 100, --- 结果总数 \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 3.8400407, ---相关性的最高得分 \u0026#34;hits\u0026#34; : [ --- 搜索结果的对象数组 { \u0026#34;_index\u0026#34; : \u0026#34;kibana_sample_data_ecommerce\u0026#34;, --- 索引 \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, --- 类型 \u0026#34;_id\u0026#34; : \u0026#34;vU35438BS6ggFfJcdbsp\u0026#34;, --- 唯一id \u0026#34;_score\u0026#34; : 3.8400407, ---文档得分 \u0026#34;_source\u0026#34; : { --- 文档的数据源 \u0026#34;email\u0026#34; : \u0026#34;eddie@underwood-family.zzz\u0026#34; } } ] } } 基础命令 # 查看版本信息 # GET / 检查集群 # GET _cluster/health //green:主分片和副本都正常分配；yellow：主分片正常分配，有副本分片未能正常分配；red:有主分片未能分配 创建索引 # PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 0 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;request_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyyMMdd||epoch_millis\u0026#34; }, \u0026#34;request_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;register_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyyMMdd||epoch_millis\u0026#34; }, \u0026#34;ip\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34;, \u0026#34;ignore_malformed\u0026#34;: true }, \u0026#34;uid\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;yean_res\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;beat_desc\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;evil_level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;evil_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 查看索引 # GET /my_index/_mapping 增加字段 # PUT /my_index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;extras\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } 插入数据 # POST /my_index/_doc { \u0026#34;action\u0026#34;:\u0026#34;afada\u0026#34; } 删除索引 # DELETE my_index 清空doc数据 # post my_index/_doc/_delete_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;:{} } } 删除一条记录 # //全量替换时，原来的document是没有被删除的！而是被标记为deleted，被标记成的deleted是不会被检索出来的，当ES中数据越来越多时，才会删除它。 PUT /my_index/_doc/1?pretty { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } // 原来的document是没有被删除的！而是被标记为deleted，被标记成的deleted是不会被检索出来的，当ES中数据越来越多时，才会删除它。 DELETE /customer/_doc/1 更新一条记录 # POST /my_index/_doc/1/_update?pretty { \u0026#34;doc\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;changwu\u0026#34; } } 制定日期发现规则 # PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;_doc\u0026#34;: { \u0026#34;dynamic_date_formats\u0026#34;: [\u0026#34;MM/dd/yyyy\u0026#34;] } } } PUT my_index/_doc/1 { \u0026#34;create_date\u0026#34;: \u0026#34;09/25/2015\u0026#34; } 基础查询 # filter(不参与得分，只是条件过滤)、must（参与得分） ——\u0026gt;and should ——\u0026gt; or must_not ——\u0026gt;非 match 分词（模糊匹配） term 关键词（精准匹配） \u0026#34;_source\u0026#34;: [\u0026#34;过滤字段\u0026#34;]：只显示自己需要的字段 gte\t大于等于 lte\t小于等于 gt\t大于 lt\t小于 fuzzy 模糊查询 multi_match 多个字段中查询 minimum_should_match：去尾 match_phrase ：doc中的字段值和给定的值完全相同 match_phrase_prefix:实现的效果类似于百度搜索,输入时，会发起一个搜索。 and # // and 关系 GET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;activity\u0026#34;: \u0026#34;point_push\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;action\u0026#34;: \u0026#34;1\u0026#34; }} ] } } } GET /renaissance-risk-data/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ {\u0026#34;term\u0026#34;: {\u0026#34;activity\u0026#34; : \u0026#34;post_weibo\u0026#34;}}, {\u0026#34;term\u0026#34;: {\u0026#34;action\u0026#34; : \u0026#34;1\u0026#34;}}, {\u0026#34;range\u0026#34;: {\u0026#34;request_time\u0026#34;: {\u0026#34;gte\u0026#34;: \u0026#34;2022-03-31 00:00:00\u0026#34;,\u0026#34;lte\u0026#34;: \u0026#34;2022-03-31 23:59:59\u0026#34;}}} ] } }, \u0026#34;sort\u0026#34;: [ {\u0026#34;request_time\u0026#34;: {\u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34;}} ], \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 100 } or # // or 关系 GET risk_engine_action/_doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;activity\u0026#34;: \u0026#34;point_push\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;action\u0026#34;: \u0026#34;1\u0026#34; }} ] } } } 查询关键词在多个字段中,指定显示字段 # GET /my_index/_search { \u0026#34;_source\u0026#34;: { \u0026#34;includes: [\u0026#34;过滤字段\u0026#34;], //includes只显示自己需要的字段 ,excludes：来指定不想要显示的字段 }, \u0026#34;query\u0026#34;:{ \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;米\u0026#34;, \u0026#34;fields\u0026#34;:[\u0026#34;title\u0026#34;,\u0026#34;name\u0026#34;] } } } { //most_fields策略、优先返回命中更多关键词的doc;如下从title、name、content中搜索包含“赐我白日梦”的doc //best_fields优先返回某个field完全匹配你给定关键字的doc \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;赐我白日梦\u0026#34;, # 指定检索的策略most_fields \u0026#34;type\u0026#34;:\u0026#34;most_fields\u0026#34;, \u0026#34;fields\u0026#34;:[\u0026#34;title\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;content\u0026#34;] } } } { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;golang java\u0026#34;, # cross_fields 要求golang：必须在title或者在content中出现 # cross_fields 要求java：必须在title或者在content中出现 \u0026#34;type\u0026#34;:\u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;:[\u0026#34;title\u0026#34;,\u0026#34;content\u0026#34;] } } } 模糊查询 # GET /xiaowu/_search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;appla\u0026#34;, \u0026#34;fuzziness\u0026#34;: 1 //差距不得超过2 } } } } 精准100%值匹配 # { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;白日梦\u0026#34; } } } 前缀匹配 # { 前缀匹配，相对于全文检索，前缀匹配是不会对前缀进行分词的。 而且每次匹配都会扫描整个倒排索引，直到扫描完一遍才会停下来 前缀搜索不会计算相关性得分所有的doc的得分都是1 前缀越短能匹配到的doc就越多，性能越不好 \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34; : { \u0026#34;user\u0026#34; : \u0026#34;白日梦\u0026#34; } } } // 前缀+权重 { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;白日梦\u0026#34;, \u0026#34;boost\u0026#34;: 2.0 } } } } 通配符匹配 # { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;白日梦的*笔记\u0026#34; } } } 正则匹配 # { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;name.first\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;s.*y\u0026#34;, \u0026#34;boost\u0026#34;: 1.2 //权重 } } } } 预览搜索 # { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;关注白日梦\u0026#34;, \u0026#34;max_expansions\u0026#34;: 10, \u0026#34;slop\u0026#34;: 10 //提高召回率，使用slop调整term persition，贡献得分 } } } } 查询my_index下全部doc # GET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 查询命中率多少才返回 # GET /your_index/your_type/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;欢迎关注白日梦！\u0026#34;, \u0026#34;operator\u0026#34;:\u0026#34;and\u0026#34;, // 默认是or （命中任何一个词就算） # 上面的query可能被分词成： 欢迎、关注、白日梦、欢迎关注、关注白日梦这五个词。 # 默认来说只要命中其中的一个词，那个doc就会被返回，所以有长尾现象。 # 去长尾：控制至少命中3/4个词的doc才算是真正命中。 \u0026#34;minimum_should_match\u0026#34;:\u0026#34;75%\u0026#34; } } } } 分页搜索 # { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10 } 模糊查询中取出所有match中得分最高的数据 # { \u0026#34;query\u0026#34;: { # 这种用法不容忽略 # 直接取下面多个query中得分最高的query当成最终得分 \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;:[ {\u0026#34;match\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;白日梦\u0026#34;}}, {\u0026#34;match\u0026#34;:{\u0026#34;content\u0026#34;:\u0026#34;关注白日梦！\u0026#34;}} ] } } } terms 精准匹配，相当于in(\u0026ldquo;a\u0026rdquo;,\u0026ldquo;b\u0026rdquo;) # { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;想搜索的字段名\u0026#34;: [ \u0026#34;tom\u0026#34;, \u0026#34;jerry\u0026#34; ] } } } } } 高级查询 # 聚合（桶bucket和度量） # 桶就像mysql中的分组查询,比如说学号相同,姓名相同的肯定是同一个人,我们就把它当成一组,在这里就是一个桶。 度量--以每个桶为基础,做运算 下钻：现有的分好组的bucket继续分组，比如可以先按性别分组、下钻再按年龄分组。 常用的度量方法: avg：求平均值 max：求最大值 min：求最小值 percentiles:占比百分位统计 extended_stats:比stats查询多四个结果：平方和、方差、标准差、平均值加/减两个标准差的区间 stats：同时返回avg、max、min、sum、count等 sum：求和 Top hits Aggregation：求前几 value_count：求总数(统计某字段有值的文档数) cardinality ：去重（查询文档中字段有多少某个值） percentile_ranks：统计值小于等于指定值的文档占比 top_hits：用于分桶后获取该桶内最匹配的顶部文档列表，即详情数据 terms：按照指定字段进行分桶 range:按照指定字段的值的范围进行分桶 date_range按照日期字段的日期范围进行分桶 dis_max:取query中得分最高的match得分作为doc的最终得分 percentile_ranks 统计值小于等于指定值的文档占比 # { \u0026#34;aggs\u0026#34;:{ \u0026#34;gge_perc_rank\u0026#34;:{ \u0026#34;percentile_ranks\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;price\u0026#34;, \u0026#34;values\u0026#34;:[ 100, 200 ] } } } } top_hits：用于分桶后获取该桶内最匹配的顶部文档列表，即详情数据 # { \u0026#34;aggs\u0026#34;:{ \u0026#34;jobs\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;job.keyword\u0026#34;, \u0026#34;size\u0026#34;:10 }, \u0026#34;aggs\u0026#34;:{ \u0026#34;top_jobs\u0026#34;:{ \u0026#34;top_hits\u0026#34;:{ \u0026#34;size\u0026#34;:10, \u0026#34;sort\u0026#34;:[ { \u0026#34;age\u0026#34;:{ \u0026#34;order\u0026#34;:\u0026#34;desc\u0026#34; } } ] } } } } } } 分组 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;terms_jobs\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:{\u0026#34;job.keyword\u0026#34;}, \u0026#34;size\u0026#34;:5 } } } } //针对某一个字段分组 { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_name\u0026#34;: { //自定义的名字 \u0026#34;term\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; // group by name } } } } //有条件的分组 { # 先查询 “query”:{ \u0026#34;term\u0026#34;:{ \u0026#34;gender\u0026#34;:\u0026#34;man\u0026#34; } }, # 再聚合 \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_name\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; # 指定聚合的字段， 意思是 group by name } } } } Range：按照指定字段的值的范围进行分桶 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_price\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;price\u0026#34;, \u0026#34;ranges\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;\u0026lt;200\u0026#34;, \u0026#34;to\u0026#34;:200 }, { \u0026#34;from\u0026#34;:200, \u0026#34;to\u0026#34;:400 }, { \u0026#34;key\u0026#34;:\u0026#34;\u0026gt;400\u0026#34;, \u0026#34;from\u0026#34;:400 } ] } } } } date_range按照日期字段的日期范围进行分桶 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_birth\u0026#34;:{ \u0026#34;date_range\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;birth\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy\u0026#34;, \u0026#34;ranges\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;2000年以前\u0026#34;, \u0026#34;to\u0026#34;:\u0026#34;2000\u0026#34; }, { \u0026#34;key\u0026#34;:\u0026#34;2000年 - 2020年\u0026#34;, \u0026#34;from\u0026#34;:\u0026#34;2000\u0026#34;, \u0026#34;to\u0026#34;:\u0026#34;2020\u0026#34; }, { \u0026#34;key\u0026#34;:\u0026#34;2020年以后\u0026#34;, \u0026#34;from\u0026#34;:\u0026#34;2020\u0026#34; } ] } } } } 按照不同年龄段分桶，求每个年龄段的工资平均值 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_age\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34;, \u0026#34;ranges\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;\u0026lt;20\u0026#34;, \u0026#34;to\u0026#34;:20 }, { \u0026#34;key\u0026#34;:\u0026#34;20 - 50\u0026#34;, \u0026#34;from\u0026#34;:20, \u0026#34;to\u0026#34;:50 }, { \u0026#34;key\u0026#34;:\u0026#34;\u0026gt;50\u0026#34;, \u0026#34;from\u0026#34;:50 } ] }, \u0026#34;aggs\u0026#34;:{ \u0026#34;avg_salary\u0026#34;:{ \u0026#34;avg\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;salary\u0026#34; } } } } } } 分桶再分桶：先根据job分桶，再按照不同年龄划分 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_job\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;job\u0026#34;, \u0026#34;size\u0026#34;:10 }, \u0026#34;aggs\u0026#34;:{ \u0026#34;range_age\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34;, \u0026#34;ranges\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;\u0026lt;20\u0026#34;, \u0026#34;to\u0026#34;:20 }, { \u0026#34;key\u0026#34;:\u0026#34;20 - 50\u0026#34;, \u0026#34;from\u0026#34;:20, \u0026#34;to\u0026#34;:50 }, { \u0026#34;key\u0026#34;:\u0026#34;\u0026gt;50\u0026#34;, \u0026#34;from\u0026#34;:50 } ] } } } } } } 分桶后进行数据分析 # { \u0026#34;size\u0026#34;:0, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_job\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;job\u0026#34;, \u0026#34;size\u0026#34;:10 }, \u0026#34;aggs\u0026#34;:{ \u0026#34;avg_salary\u0026#34;:{ \u0026#34;stats\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;salary\u0026#34; } } } } } } 相同值分一组，每组的某一项的min max avg等 # { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_name\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;average_age\u0026#34;: { \u0026#34;avg\u0026#34;: { //min max avg \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } } } } } } 统计不同年龄段有多少人，男女多少 # { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_age\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: 20, \u0026#34;to\u0026#34;: 25 }, { \u0026#34;from\u0026#34;: 25, \u0026#34;to\u0026#34;: 30 }, { \u0026#34;from\u0026#34;: 30, \u0026#34;to\u0026#34;: 35 }, { \u0026#34;from\u0026#34;: 35, \u0026#34;to\u0026#34;: 40 } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_gender\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gender.keyword\u0026#34; } } } } } } 每个年龄段，每个性别的平均账户余额 # { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_age\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: 20, \u0026#34;to\u0026#34;: 30 } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;group_by_gender\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gender.keyword\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;average_balance\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;balance\u0026#34; } } } } } } } } 第三方包 # https://github.com/olivere/elastic/v7 https://github.com/elastic/go-elasticsearch query 语法 # query := elastic.NewBoolQuery() query.Must(elastic.NewTermQuery(\u0026#34;date\u0026#34;, request.SearchDate)) //must 相当于sql的and query.Should(elastic.NewTermQuery(\u0026#34;qq\u0026#34;, request.QQ)) //should 相当于sql的or CURD # func Es() *elastic.Client { client, err := elastic.NewClient( elastic.SetURL(configs.Cfg.Es.Path), //服务地址，多个服务地址使用逗号分隔 elastic.SetSniff(false), elastic.SetBasicAuth(\u0026#34;user\u0026#34;, \u0026#34;pwd\u0026#34;), //基于http base auth验证机制的账号和密码 elastic.SetHealthcheck(false), //设置监控检查 ) if err != nil { panic(err) } return client } // InsertDoc 插入记录，自动创建id func InsertDoc(indexName string, data interface{}) (err error) { _, err = configs.Es.Index().Index(indexName).BodyJson(data).Do(context.TODO()) return err } // InsertDocById 插入记录，根据id func InsertDocById(indexName, id string, data interface{}) (err error) { _, err = configs.Es.Index().Index(indexName).Id(id).BodyJson(data).Do(context.TODO()) return err } // ExistsIndex 判断index是否存在 func ExistsIndex(indexName string) (bool, error) { exists, err := configs.Es.IndexExists(indexName).Do(context.TODO()) if err != nil { return false, err } if !exists { return false, err } return true, nil } // ExistsDocByQuery 判断doc是否存在，根据查询条件 func ExistsDocByQuery(indexName string, query elastic.Query) (bool, error) { //query=elastic.NewTermQuery(\u0026#34;msgid.keyword\u0026#34;, msgItem.MsgID) ret, err := configs.Es.Search(indexName).Query(query).Size(0).Do(context.TODO()) if err != nil { return false, err } if ret.TotalHits() \u0026lt;= 0 { return false, err } return true, nil } // SearchDocList 分页搜索数据，根据查询条件 /* query := elastic.NewBoolQuery() if len(request.SearchDate)\u0026gt;0 { query.Must(elastic.NewTermQuery(\u0026#34;date\u0026#34;, request.SearchDate)) } offset=(request.Page-1)*request.Size ascending=false */ func SearchDocList(indexName string, query elastic.Query, offset, size int, sortField string, ascending bool) ( *elastic.SearchResult, error) { op := configs.Es.Search(indexName).Query(query) if len(sortField) \u0026gt; 0 { op = op.Sort(sortField, ascending) } ret, err := op.Size(size).From(offset).Do(context.TODO()) return ret, err } // SearchDoc 搜索doc,根据查询条件 func SearchDoc(indexName string, query elastic.Query) (*elastic.SearchResult, error) { ret, err := configs.Es.Search(indexName).Query(query).Do(context.TODO()) return ret, err } // GetDocById 获取doc 根据id func GetDocById(indexName, id string) (*elastic.GetResult, error) { ret, err := configs.Es.Get().Index(indexName).Id(id).Do(context.TODO()) return ret, err } // CreateIndex 创建index func CreateIndex(indexName, mapping string) error { ret, err := configs.Es.CreateIndex(indexName).BodyString(mapping).Do(context.TODO()) if err != nil { return err } if !ret.Acknowledged { return fmt.Errorf(\u0026#34;create index err\u0026#34;) } return nil } // DeleteIndex 删除index func DeleteIndex(indexName string) error { ret, err := configs.Es.DeleteIndex(indexName).Do(context.TODO()) if err != nil { return err } if !ret.Acknowledged { return fmt.Errorf(\u0026#34;delete index err\u0026#34;) } return nil } /* refresh: \u0026#34;true\u0026#34;:强制同步(慎用) 节点立即完成刷新(性能最差) \u0026#34;false\u0026#34;:异步刷新(默认值) 写入数据一段时间后可见(性能最好) \u0026#34;wait_for\u0026#34;:/等待同步 客户端等待, 直到节点自动完成刷新 */ // DeleteDocById 删除doc 根据id func DeleteDocById(indexName, id, refresh string) error { ret, err := configs.Es.Delete().Index(indexName).Id(id).Refresh(refresh).Do(context.TODO()) if err != nil { return err } if ret.Result != \u0026#34;deleted\u0026#34; { return fmt.Errorf(\u0026#34;delete id err\u0026#34;) } return nil } // UpdateDocById 更新doc 根据id func UpdateDocById(indexName, id, refresh string, data interface{}) error { ret, err := configs.Es.Update().Index(indexName).Id(id).Doc(data).Refresh(refresh).Do(context.TODO()) if err != nil { return err } if ret.Result != \u0026#34;updated\u0026#34; { return fmt.Errorf(\u0026#34;delete id err\u0026#34;) } return nil } func UpdateByQuery(ctx define.MsgContext, index string, query elasticV7.Query, script *elasticV7.Script) (int64, error) { rsp, err := Es.UpdateByQuery(index).Query(query).Script(script).Refresh(\u0026#34;true\u0026#34;).Do(ctx.Ctx) if err != nil { return 0, err } return rsp.Updated, nil } "},{"id":24,"href":"/docs/docs/16_grafana/","title":"grafana","section":"Docs","content":" 语法 # //每分钟请求/响应数 sum(increase(fwd_req_request_counter[1m])) sum(increase(fwd_res_response_counter[1m])) // qps sum(irate(fwd_req_request_counter{}[1m])) sum(irate(fwd_res_response_counter{}[1m])) //请求P9x图 histogram_quantile(0.90, sum(rate(fwd_req_time_cost_histogram_bucket[1m])) by (le)) histogram_quantile(0.95, sum(rate(fwd_req_time_cost_histogram_bucket[1m])) by (le)) histogram_quantile(0.99, sum(rate(fwd_req_time_cost_histogram_bucket[1m])) by (le)) //响应P9x图 histogram_quantile(0.90, sum(rate(fwd_res_time_cost_histogram_bucket[1m])) by (le)) histogram_quantile(0.95, sum(rate(fwd_res_time_cost_histogram_bucket[1m])) by (le)) histogram_quantile(0.99, sum(rate(fwd_res_time_cost_histogram_bucket[1m])) by (le)) //请求最大耗时 sum(max_over_time(fwd_req_time_cost_gauge[15s])) //响应最大耗时 sum(max_over_time(fwd_res_time_cost_gauge[15s])) //每分钟ip数 count(increase(fwd_req_request_counter[1m])\u0026gt;0) count(increase(fwd_res_response_counter[1m])\u0026gt;0) 指标 # Gauge(仪表盘): 值是有变化的,如CPU使用率，有高有低 Counter(计数器):从程序开始，只增不减 Histogram(直方图): 把观测值归入的 bucket 的数量 Summary(摘要):与Histogram类似类型，用于表示一段时间内的数据采样结果,但它直接存储了分位数（通过客户端计算，然后展示出来），而不是通过区间计算 函数 # sum: 对瞬时向量求和, 加by(),按照字段分组: group by min: 最小值 可配合by max: 最大值 可配合by avg: 平均值 可配合by count: 瞬时向量个数求和,可配合by topk:对瞬时向量的值从大到小进行排列，并获取前N个值 可配合by bottomk 和topk相反 changes返回给定的区间向量中，对比于当前值，发生变化的元素的数量。 delta:返回区间向量中，第一个元素和最后一个元素之间的变化值须作用在gauge类型的指标 "},{"id":25,"href":"/docs/docs/17_grpc/","title":"grpc","section":"Docs","content":" proto3 # "},{"id":26,"href":"/docs/docs/19_redis/","title":"redis","section":"Docs","content":" 基础操作 # String（key-value） # get # # 返回字符串value,不存在-\u0026gt;nil,非字符串-\u0026gt;err get key set # # 设置key-value 字符串，存在value，覆盖;返回ok set key value set key value ex second # 设置键的过期时间为秒 set key value px millisecond # 设置键的过期时间为毫秒 set key value nx # 键不存在才设置 set key value xx # 只有键存在才设置 set key value ex second nx # 设置key的过期时间为秒，不存在才设置 strlen # # 返回value字符串长度，非字符串-\u0026gt;err strlen key append # # key存在，追加到value尾部，反之设为value append key value decr # # 数字value减一,不存在，key的值设为-1，非数字返回错误 decr key decrby # # 已存储的value,减去数字value decrby key 20 incr # # 数字加一，不存在key设为1，非数字-\u0026gt;err incr key incrby # # 已存储的value,减去数字value incrby key 20 incrbyfloat # # 已存储的value，加上数字浮点数 incrby key 20 getrange # # 返回字符串字串，两边都包含，-1：最后一个字符，-2倒数第二 getrange key 0 4 getset # # 覆盖旧字符串，返回旧字符串，value不是字符串-\u0026gt;err,key不存在-\u0026gt;nil getset key value mget # # 返回所有key的value,不存在对应value-\u0026gt;nil mget key1 key2 mset # # 设置多个key-value对,会覆盖旧值 mset key1 value1 key2 value2 psetex # # 设置key的生存时间，毫秒 psetex key time value setrange # # 从offset开始覆盖字符串 setrange key 6 value Hash # key ckey1 cvalue ckey2 cvalue2\nhget # # 返回value,不存再-\u0026gt;nil hget key value hset # # 设置key-values hset key ckey1 cvalue1 hmget # # 返回多个key-value,不存在-\u0026gt;nil hmget key1 key2 hmset # # 设置多个key-value hmset key ckey1 cvalue1 ckey2 cvalue2 hdel # # 返回删除的数量 hdel key value hexists # # 查看value是否存在，存在-\u0026gt;1,不存在返回-\u0026gt;0 hexists key value hgetall # # 返回所有value hgetall key hincrby # # value 加增量，非数字-\u0026gt;err hincrby key value 增量 hincrbyfloat # # value 加浮点增量，不能解析为浮点-\u0026gt;err hincrbyfloat key value 浮点增量 hkeys # # 返回所有的key hkeys key hlen # # 返回key的数量 hlen key hsetnx # # key-value,当key不存在时 hsetnx key ckey1 cvalue1 hvals # # 返回key所有的values hvals key List (key-value1,value2) # lpop # # 移除并返回该元素 lpop key lpush # # key中插入多个value lpush key value1 value2 value3 llen # # 返回key的values长度，不存在-\u0026gt;0;非列表-\u0026gt;err llen key lindex # # 返回下标为index的元素 lindex key index linsert # # 将value插入到某个value之前/之后 linsert key before|after pivot value lpushx # # 当key存在且列表，才插入 lpushx key value1 value2 value3 lrange # # 返回包含区间的元素 lrange key start stop lrem # # 移除一定数据的value lrem key count value lset # # 将key的下标值设置为value lset key index value Set (key1,key2) # sadd # # 将一个多个元素加入key，存在即忽略,返回添加数量 sadd key value sismember # # 判断value是否在集合key内，1：是，0非 sismember key value smembers # # 返回key的所有元素 smembers key srem # # 移除一个、多个元素 spop key value1 value2 value3 scard # # 返回key的所有value数量 scard key sdiff # # 返回第一个集合再第二个集合不存在的值 sdiff key1 key2 sdiffstore # # 和sdiff一样，但保存在一个集合内 sdiffstore newKey key1 key2 sinter # # 返回两个集合的交集 sinter key1 key2 sinterstore # # 和sinter一样，但保存在一个集合内 sinterstore newKey key1 key2 smove # # 将key的value移动到key2内 smove key key2 value spop # # 随机移除一个元素 spop key SortedSet(有序集合) # zadd # # 将一个多个元素加入到有序key中,根据设置的score zadd key score value1 score value2 score value3 zcard # # 返回key的value数量 zcard key zcount # # 返回区间的数量 zcount key min max zincrby # # 为key的value的score增量 zincrby key 增量 value zrange # # 返回区间的元素 zrange key min max # 显示区间元素 zrange key min max # 显示score和对应元素 zrank # # 返回value的排名,从大到小 zrank key value zrem # # 移除一个、多个元素 zrem key value1 value2 value3 zscore # # 返回value的score值 zscore key value 持久化 # RDB # 某段时间间隔数据快照存储在磁盘，父进程判断需要save时，fork（父进程阻塞）一个子进程生成rdb文件，写入。 AOF # 以追加的方式记录每一次redis写的操作，当服务器重启时,重新执行这些命令恢复原始数据 redis和mysql 数据一致 # 先更新mysql 再删除redis # 更新时，先更新mysql,然后删除redis,查询时 no cache,查询mysql,然后更新redis 1.问题：c1更新mysql,删除 redis,c3查询redis,查询mysql,c2更新mysql,删除redis,c3同步redis结论是redis保存的是c1的数据， 不是最新的 2.在删除redis时，其他请求拿到的还是旧数据 延时双删 # 为了保证最终redis是最新的数据 更新时，先删除redis,再更新mysql,延迟删除redis 缓存击穿（某热点数据失效） # # 原因 高并发流量，访问的这个数据是热点数据，请求的数据在 DB 中存在，但是 Redis 存的那一份已经过期，后端需要从 DB 从加载数据并写到 Redis。 # 解决 过期时间 + 随机值（过期时间=baes 时间+随机时间）：让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力 预先把热门数据提前存入 Redis 中，并设热门数据的过期时间超大值。 使用锁，先获取分布式锁，获取锁成功才执行数据库查询和写数据到缓存的操作，获取锁失败，则说明当前有线程在执行数据库查询操作，当前线程睡眠一段时间在重试。 这样只让一个请求去数据库读取数据。 缓存穿透(redis mysql都不存在数据) # # 原因 意味着有特殊请求在查询一个不存在的数据，即数据不存在 Redis 也不存在于数据库。 # 解决 缓存空值：当请求的数据不存在 Redis 也不存在数据库的时候，设置一个缺省值（比如：None）。当后续再次进行查询则直接返回空值或者缺省值。 布隆过滤器：在数据写入数据库的同时将这个 ID 同步到到布隆过滤器中，当请求的 id 不存在布隆过滤器中则说明该请求查询的数据一定没有在数据库中保存，就不要去数据库查询了。 (布隆过滤器：分配一块内存空间做 bit 数组，数组的 bit 位初始值全部设为 0。添加：key经过多个hash组函数计算，的k位置置为1.判断key是否存在时，计算多个hansh组函数的k位置是否都是1) 缓存雪崩（大量数据同时失效） # # 原因 缓存雪崩指的是大量的请求无法在 Redis 缓存系统中处理，请求全部打到数据库，导致数据库压力激增，甚至宕机。 大量数据同时过期，导致大量请求需要查询数据库并写到缓存； Redis 故障宕机，缓存系统异常。 # 解决 过期时间添加随机值，要避免给大量的数据设置一样的过期时间，过期时间 = baes 时间+ 随机时间（较小的随机数，比如随机增加 1~5 分钟） 接口限流 一个 Redis 实例能支撑 10 万的 QPS，而一个数据库实例只有 1000 QPS。 服务熔断就是当从缓存获取数据发现异常，则直接返回错误数据给前端，防止所有流量打到数据库导致宕机。 redis分布式锁问题 # redis是单线程执行，当处理请求A时，请求B也可以得到响应，并被添加到队列中等待执行。 加锁问题：A加锁+过期时间，但是A在内部执行过长或者其他问题，在redis过期时间后，还没执行完毕，redis会自动删除key。这时B发现没锁，申请加锁+过期时间，B在内部执行中，A完毕后要释放锁会释放B的锁的问题：这时就需要使用luaScript脚本，找到key和对应的A设置的value,匹配才能删除。否则不能删除。 A在加锁后的过期时间后，redis删除了key,此时A还在处理共享资源，B加锁也进入共享资源内，资源不能互斥问题：此时需要第一个加锁的在过期时间要删除时，用守护进程自动延期时间。相关代码github.com/jefferyjob/go-redislock "},{"id":27,"href":"/docs/docs/18_kafka/","title":"kafka","section":"Docs","content":" kafka基本组件 # ZooKeeper # 负责保存 broker 集群元数据，并对控制器进行选举等操作 Broker # 消息中间件节点（服务器），一个节点就是一个broker，一个kafka集群由一个或多个broker组成 Producer (生产者) # 负责发布消息到 Kafka broker Consumer（消费者） # 消息消费者，向 Kafka broker 读取消息的客户端 Topic（主题） # 每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处） Partition # partition 默认一个topic有一个分区（partition） Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition，数据存储最小单元， 分区分散存储在服务器不同节点上解决了一个海量数据如何存储的问题 Offset # offset 是消息在分区中的唯一标识，Kafka 通过它来保证消息在分区内的顺序性，不过 offset 并不跨越分区，也就是说，Kafka保证的是分区有序性而不是主题有序性。 Replication(副本) # Kafka 同一 Partition 的数据可以在多 Broker 上存在多个副本，通常只有主副本对外提供读写服务，当主副本所在 broker 崩溃或发生网络异常，Kafka 会在 Controller 的管理下会重新选择新的 Leader 副本对外提供读写服务。 kafka零拷贝机制保证读取数据高性能 # 消费者读取数据流程：消费者-》发送请求到kafka服务-》kafka服务去os cache 缓存读取数据（缓存灭有就从磁盘读取数据）-》os cache 复制数据到kafka应用中-》kafka将数据（复制）发送到socket cache中-》socket cache 通过网卡传输给消费者 //零拷贝 kafka linux sendfile技术 ：消费者-》发送请求到kafka服务-》kafka服务去os cache 缓存读取数据（缓存灭有就从磁盘读取数据） 从os cache直接将数据发送给网卡传输给消费者 //如何做到高性能？基于以下相当于 Kafka 完全基于内存提供数据的写和读 1.页缓存技术：数据写入 os cache 2.磁盘顺序写： 追加文件末尾按照顺序的方式来写数据，内存相当 3.os cache直接发送数据 kafka 和 zookeeper 之间的关系 # kafka 使用 zookeeper 来保存集群的元数据信息和消费者信息(偏移量) 每个 Broker 服务器在启动时，都会到 Zookeeper 上进行注册，即创建 /brokers/ids/[0-N] 的节点，然后写入 IP，端口等信息，Broker 创建的是临时节点，所以一旦 Broker 上线或者下线，对应 Broker 节点也就被删除了，因此可以通过 zookeeper 上 Broker 节点的变化来动态表征 Broker 服务器的可用性。 如何根据 offset 找到对应的 Message？ # 第一步，根据 Offset 找到所属的 Segment 文件 第二步，从 Segment 中获取对应 Offset 的消息数据 说一下什么是副本？ # kafka 为了保证数据不丢失，从 0.8.0 版本开始引入了分区副本机制。在创建 topic 的时候指定 replication-factor,默认副本为 3 。 副本是相对 partition 而言的，一个分区中包含一个或多个副本，其中一个为leader 副本，其余为follower 副本，各个副本位于不同的 broker 节点中。 所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上复制数据。当 Leader 挂掉之后，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。 说一下 kafka 的 ISR 机制？ # ISR 是指与 leader 副本保持同步状态的副本集合.当 ISR 中的 follower 完成数据同步之后， leader 就会给 follower 发送 ack ,如果其中一个 follower 长时间未向 leader 同步数据，该 follower 将会被踢出 ISR 集合，该时间阈值由 replica.log.time.max.ms 参数设定。当 leader 发生故障后，就会从 ISR 集合中重新选举出新的 leader。 kafka 如何保证对应topic的消息被写到相同的分区？ # 消息在不同的 Partition 是不能保证有序的，只有一个 Partition 内的消息是有序的 通过 消息键 和 分区器 来实现，分区器为键生成一个 offset，然后使用 offset 对主题分区进行取模，为消息选取分区，这样就可以保证包含同一个键的消息会被写到同一个分区上。 kafka 消费支持几种消费模式？ # at most once 模式 最多一次。保证每一条消息 commit 成功之后，再进行消费处理。消息可能会丢失，但不会重复。 at least once 模式 至少一次。保证每一条消息处理成功之后，再进行commit。消息不会丢失，但可能会重复。 exactly once 模式 精确传递一次。将 offset 作为唯一 id 与消息同时处理，并且保证处理的原子性。消息只会处理一次，不丢失也不会重复。但这种方式很难做到。 kafka 如何保证数据的不重复和不丢失？ # exactly once 模式 精确传递一次。将 offset 作为唯一 id 与消息同时处理，并且保证处理的原子性。消息只会处理一次，不丢失也不会重复。但这种方式很难做到。 kafka 默认的模式是 at least once ，但这种模式可能会产生重复消费的问题，所以在业务逻辑必须做幂等设计。 使用 exactly Once + 幂等操作，可以保证数据不重复，不丢失。 kafka 是如何清理过期数据的？ # //删除 log.cleanup.policy=delete 启用删除策略 log.retention.hours=16 #清理超过指定时间清理： log.retention.bytes=1073741824 #超过指定大小后，删除旧的消息： //压缩 log.cleanup.policy=compact 启用压缩策略。 将数据压缩，只保留每个 key 最后一个版本的数据 首先在 broker 的配置中设置 log.cleaner.enable=true 启用 cleaner，这个默认是关闭的。 kafka发送数据流程 # 生产者-》生产数据到kafka服务-》kafka服务写入os cache-》每隔一段时间fsync到磁盘 kafka二分查找定位数据 # 一条消息就有两个位置：offset：相对偏移量（相对位置） position：磁盘物理位置 稀疏索引 Kafka中采用了稀疏索引的方式读取索引，kafka每当写入了4k大小的日志（.log），就往index里写入一个记录索引。其中会采用二分查找. 第三方包 # github.com/Shopify/sarama 客户端示例 # package boot import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/Shopify/sarama\u0026#34; \u0026#34;github.com/xiaohubai/alpha/config\u0026#34; ) var ( client sarama.SyncProducer ) func Kafka() { cfg := sarama.NewConfig() //sarama.WaitForAll //数据推出的成功与否都与我无关了 //sarama.WaitForLocal //当local(leader)确认接收成功后，就可以返回了 //sarama.WaitForAll //当所有的leader和follower都接收成功时，才会返回 cfg.Producer.RequiredAcks = sarama.WaitForAll // 发送完数据需要leader和follow都确认 cfg.Producer.Partitioner = sarama.NewRandomPartitioner // 新选出一个partition cfg.Producer.Return.Successes = true // 成功交付的消息将在success channel返回 cfg.Producer.Retry.Max = 3 // 重试三次 cfg.Producer.Retry.Backoff = 100 * time.Millisecond // 连接kafka err := errors.New(\u0026#34;\u0026#34;) client, err = sarama.NewSyncProducer(config.CONFIG.Kafka.Address, cfg) if err != nil { panic(err) } } func SendToKafka(topic, data string) { // 构造一个消息 msg := \u0026amp;sarama.ProducerMessage{} msg.Topic = topic msg.Value = sarama.StringEncoder(data) // 发送消息 pid, offset, err := client.SendMessage(msg) if err != nil { return } fmt.Printf(\u0026#34;pid:%v offset:%v\\n\u0026#34;, pid, offset) } 服务端示例 # func KafkaConsumer() { cfg := sarama.NewConfig() c, err := sarama.NewConsumer(config.CONFIG.Kafka.Address, cfg) if err != nil { panic(err) } consumer = c go ReadFromKafka(\u0026#34;my_log\u0026#34;) } func ReadFromKafka(topic string) { partitionList, err := consumer.Partitions(topic) if err != nil { panic(err) } for partition := range partitionList { pc, err := consumer.ConsumePartition(topic, int32(partition), sarama.OffsetNewest) if err != nil { panic(err) } defer pc.AsyncClose() for msg := range pc.Messages() { fmt.Println(string(msg.Value)) } } } "},{"id":28,"href":"/docs/docs/20_swaggo/","title":"swaggo","section":"Docs","content":" main总入口 # // @title Swagger Example API // @version 0.0.1 // @description 总入口 // @in header // @BasePath / Get不带参数跟Header请求 # // @Summary 接口描述 // @Tags 分类名称 // @Accept application/json // @Success 200 object result.Response 返回值 // @Router /api/get [get] Get带Query参数请求 # // @Summary 接口描述 // @Tags 分类名称 // @Accept application/json // @Param userId path integer true \u0026#34;用户ID\u0026#34; // @Success 200 object result.Response 返回值 // @Router /api/get/{userId} [get] Get带token请求 # // @Summary 接口描述 // @Tags 分类名称 // @Security ApiKeyAuth // @Accept application/json // @Param token header string true \u0026#34;登录信息\u0026#34; // @Success 200 object result.Response 返回值 // @Router /api/get [get] Post带toekn+body # // @Summary 接口描述 // @Tags 分类名称 // @Accept application/json // @Param token header string true \u0026#34;登录信息\u0026#34; // @Param data body service.ReqData true \u0026#34;请求参数结构体\u0026#34; // @Success 200 object result.Response 返回值 // @Router /api/post [post] Post带token+文件 # // @Summary 接口描述 // @Tags 分类名称 // @Accept application/json // @Param token header string true \u0026#34;登录信息\u0026#34; // @Param data formData file true \u0026#34;文件\u0026#34; // @Success 200 object result.Response 返回值 // @Router /api/file [post] "},{"id":29,"href":"/docs/docs/21_validator/","title":"validator","section":"Docs","content":" go-playground/validator # https://github.com/go-playground/validator tag参数 # required //必填； len=11 //长度=11； min=3 //如果是数字，验证的是数据大小范围，最小值为3，如果是文本，验证的是最小长度为3， max=6 //如果是数字，验证的是数字最大值为6，如果是文本，验证的是最大长度为6 mail //验证邮箱 gt=3 //对于文本就是长度\u0026gt;=3 lt=6 //对于文本就是长度\u0026lt;=6 示例 # type Register struct { UserName string `form:\u0026#34;user_name\u0026#34; json:\u0026#34;user_name\u0026#34; binding:\u0026#34;required,min=1\u0026#34;` Pass string `form:\u0026#34;pass\u0026#34; json:\u0026#34;pass\u0026#34; binding:\u0026#34;required,min=6,max=20\u0026#34;` Captcha string `form:\u0026#34;captcha\u0026#34; json:\u0026#34;captcha\u0026#34; binding:\u0026#34;required,len=4\u0026#34;` Age float64 `form:\u0026#34;age\u0026#34; json:\u0026#34;age\u0026#34; binding:\u0026#34;required,min=1,max=200\u0026#34;` // 注意： 如果你的表单参数含有0值是允许提交的，必须用指针类型（*float64），而 float64 类型则认为 0 值不合格 Status *float64 `form:\u0026#34;status\u0026#34; json:\u0026#34;status\u0026#34; binding:\u0026#34;required,min=0,max=1\u0026#34;` } var r Register if err := c.ShouldBindJSON(\u0026amp;r); err != nil { //返回参数错误 } proto-gen-validate # https://github.com/envoyproxy/protoc-gen-validate 数字类型 # // 参数必须大于 0 int64 id = 1 [(validate.rules).int64 = {gt: 0}]; // 参数必须在 0 到 120 之间 int32 age = 2 [(validate.rules).int64 = {gt:0, lte: 120}]; // 参数是 1 或 2 或 3 uint32 code = 3 [(validate.rules).uint32 = {in: [1,2,3]}]; // 参数不能是 0 或 99.99 float score = 1 [(validate.rules).float = {not_in: [0, 99.99]}]; 布尔类型 # // 参数必须为 true bool state = 5 [(validate.rules).bool.const = true]; // 参数必须为 false bool state = 5 [(validate.rules).bool.const = false]; 文本类型 # // 参数必须为 /hello string path = 6 [(validate.rules).string.const = \u0026#34;/hello\u0026#34;]; // 参数文本长度必须为 11 string phone = 7 [(validate.rules).string.len = 11]; // 参数文本长度不能小于 10 个字符 string explain = 8 [(validate.rules).string.min_len = 10]; // 参数文本长度不能小于 1 个字符并且不能大于 10 个字符 string name = 9 [(validate.rules).string = {min_len: 1, max_len: 10}]; // 参数文本使用正则匹配,匹配必须是非空的不区分大小写的十六进制字符串 string card = 10 [(validate.rules).string.pattern = \u0026#34;(?i)^[0-9a-f]+$\u0026#34;]; // 参数文本必须是 email 格式 string email = 11 [(validate.rules).string.email = true]; "},{"id":30,"href":"/docs/docs/22_vue/","title":"vue","section":"Docs","content":" 新建项目 # pnpm init vite@latest pnpm create vite 生命周期函数 # data(): 定义该页面使用的变量 created(): 页面加载完成之前 执行 父组件-- \u0026gt; 子组件 mounted(): 页面加载完成之后，执行 子组件-- \u0026gt; 父组件 watch(): 监听一个值的变化，然后执行相应的函数 destroyed(): 离开页面后，会调用 销毁一些监听事件及定时函数 methods() ：事件方法执行 //vue3 onMounted() :用来在组件完成初始渲染并创建 DOM 节点后运行代码 ref reactive # ref和reactive是实现响应式数据的方法,即界面和数据同步，能实现实时更新 reactive:参数必须是一个对象，包括json数据和数组都可以，否则不具有响应式 ref:ref本质也是reactive,是reactive+简单值: ref(\u0026#34;\u0026#34;) 在vue中使用ref的值，不用通过.value获取 在js中使用ref的值，必须通过.value获取 "},{"id":31,"href":"/docs/docs/23_js/","title":"js","section":"Docs","content":" 字符串 # length 获取长度 indexOf() 字符串中指定文本首次出现的索引 lastIndexOf()字符串中指定文本最后一次出现的索引 search() 方法搜索特定值的字符串，并返回匹配的位置 slice() 提取字符串的某个部分并返回被提取的部分 substring()类似于 slice(),不同之处在于 substring() 无法接受负的索引 substr() 方法 类似于 slice(),不同之处在于第二个参数规定被提取部分的长度。 replace() 另一个值替换在字符串中指定的值,只替换首个匹配： /xxxx/ :正则写法不带引号 /xxxx/i :不区分大小写 /xxxx/g :替换所有匹配的 toUpperCase() 字符串大写 toLowerCase() 字符拆小写 concat() 连接两个或多个字符串 trim() 方法删除字符串两端的空白符： charAt() 方法返回字符串中指定下标（位置）的字符串 charCodeAt() 方法返回字符串中指定索引的字符 unicode 编码 split() 将字符串转换为数组：括号内是根据什么切割 数字 # toString() 将数值转化为字符串 toFixed() 返回字符串值，它包含了指定长度的数字： toPrecision() 返回字符串值，它包含了指定长度的数字： Number() 其它类型 转换为数字 parseInt(str) 去除小数点 取整 数组 # join() 返回一个新字符串，将所有数组元素结合为一个字符串 pop() 方法从数组中删除最后一个元素： push() 方法（在数组结尾处）向数组添加一个新的元素，返回新数组的长度 shift() 方法会删除首个数组元素，回被“位移出”的字符串： unshift() 方法（在开头）向数组添加新元素，返回新数组的长度。 sort() 方法以字母顺序对数组进行排序 (“25”大于100：str1.sort(function(a,b){return a-b}); ） （随机顺序排序数组：sort(function(a, b){return 0.5 - Math.random()}); ） (对对象排序：cars.sort(function(a, b){return a.year - b.year});) reverse() 方法反转数组中的元素 Math.max.apply(null,arr); 查找数组中的最高值 Math.min.apply(null,arr); 查找数组中的最低值 日期 # var d= new Date(); d.toUTCString() 方法将日期转换为 UTC 字符串 d.toDateString() 方法将日期转换为更易读的格式 d.getDate()\t以数值返回天（1-31） d.getDay()\t以数值获取周名（0-6） d.getFullYear()\t获取四位的年（yyyy） d.getHours()\t获取小时（0-23） d.getMilliseconds()\t获取毫秒（0-999） d.getMinutes()\t获取分（0-59） d.getMonth()\t获取月（0-11） d.getSeconds()\t获取秒（0-59） d.getTime()\t获取时间（从 1970 年 1 月 1 日至今） parseInt() Math # Math.round(x) 的返回值是 x 四舍五入为最接近的整数： Math.pow(x, y) 的返回值是 x 的 y 次幂： Math.sqrt(x) 返回 x 的平方根： Math.abs(x) 返回 x 的绝对（正）值： Math.ceil(x) 的返回值是 x 上舍入最接近的整数： Math.floor(x) 的返回值是 x 下舍入最接近的整数： Math.sin(x) 返回角 x（以弧度计）的正弦（介于 -1 与 1 之间的值） Math.cos(x) 返回角 x（以弧度计）的余弦（介于 -1 与 1 之间的值）。 Math.min() 和 Math.max() 可用于查找参数列表中的最低或最高值： Math.random() 返回介于 0（包括） 与 1（不包括） 之间的随机数： (Math.floor(Math.random() * 10);\t// 返回 0 至 9 之间的数) (Math.floor(Math.random() * 100) + 1;\t// 返回 1 至 100 之间的数） (Math.floor(Math.random() * (max - min) ) + min; //返回介于 min（包括）和 max（不包括）之间的随机数） 循环 # for - 多次遍历代码块 for/in - 遍历对象属性 while - 当指定条件为 true 时循环一段代码块 do/while - 当指定条件为 true 时循环一段代码块 类型转换 # Number() 转换数值 String() 转换字符串 Boolean() 转换布尔值。 let const # 使用 let 关键词声明拥有块作用域的变量。在块 {} 内声明的变量无法从块外访问： const 与let类似 但是不能重新赋值 debugger 关键词 Json # JSON.parse() 来把这个字符串转换为 JavaScript 对象 时间处理 # // 时间戳 let timestamp = timestamp() function timestamp(){ return Math.round(new Date()/1000) } //格式化时间 let res = formatDate(timestamp, \u0026#39;YYYY-MM-DD hh:mm:ss\u0026#39;) function formatDate(timestamp, type = \u0026#39;YYYY-MM-DD hh:mm:ss\u0026#39;) { timestamp = (\u0026#39;\u0026#39; + timestamp).length \u0026lt; 13 ? parseInt(timestamp) * 1000 : parseInt(timestamp) let time = new Date(timestamp) let YYYY = time.getFullYear() let MM = (time.getMonth() + 1 \u0026lt; 10 ? \u0026#39;0\u0026#39; + (time.getMonth() + 1) : time.getMonth() + 1) let DD = (time.getDate() \u0026lt; 10 ? \u0026#39;0\u0026#39; + (time.getDate()) : time.getDate()) let hh = (time.getHours() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getHours() : time.getHours()) let mm = (time.getMinutes() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getMinutes() : time.getMinutes()) let ss = (time.getSeconds() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getSeconds() : time.getSeconds()) let type_0 = \u0026#39;\u0026#39; + YYYY + \u0026#39;-\u0026#39; + MM + \u0026#39;-\u0026#39; + DD + \u0026#39; \u0026#39; + hh + \u0026#39;:\u0026#39; + mm + \u0026#39;:\u0026#39; + ss let type_1= \u0026#39;\u0026#39; + YYYY + \u0026#39;\u0026#39; + MM + \u0026#39;\u0026#39; + DD + \u0026#39;\u0026#39; + hh + \u0026#39;\u0026#39; + mm + \u0026#39;\u0026#39; + ss return type === \u0026#39;YYYY-MM-DD hh:mm:ss\u0026#39; ? type_0 : type_1 } function formatCurrentDate(type = \u0026#39;YYYY-MM-DD hh:mm:ss\u0026#39;) { let time = new Date() let YYYY = time.getFullYear() let MM = (time.getMonth() + 1 \u0026lt; 10 ? \u0026#39;0\u0026#39; + (time.getMonth() + 1) : time.getMonth() + 1) let DD = (time.getDate() \u0026lt; 10 ? \u0026#39;0\u0026#39; + (time.getDate()) : time.getDate()) let hh = (time.getHours() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getHours() : time.getHours()) let mm = (time.getMinutes() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getMinutes() : time.getMinutes()) let ss = (time.getSeconds() \u0026lt; 10 ? \u0026#39;0\u0026#39; + time.getSeconds() : time.getSeconds()) let type_0 = \u0026#39;\u0026#39; + YYYY + \u0026#39;-\u0026#39; + MM + \u0026#39;-\u0026#39; + DD + \u0026#39; \u0026#39; + hh + \u0026#39;:\u0026#39; + mm + \u0026#39;:\u0026#39; + ss //YYYY-MM-DD hh:mm:ss let type_1= \u0026#39;\u0026#39; + YYYY + \u0026#39;\u0026#39; + MM + \u0026#39;\u0026#39; + DD + \u0026#39;\u0026#39; + hh + \u0026#39;\u0026#39; + mm + \u0026#39;\u0026#39; + ss //YYYYMMDDhhmmss return type === \u0026#39;YYYY-MM-DD hh:mm:ss\u0026#39; ? type_0 : type_1 } 递归遍历格式化数据 # const menuListFormat = (tableData, list) =\u0026gt; { list.forEach(item =\u0026gt; { const tData = { path: item.path, name: item.name, component: item.component, ID: item.meta.ID, parentID: item.meta.parentID, title: item.meta.title, icon: item.meta.icon, hidden: item.meta.hidden, keepAlive: item.meta.keepAlive, sort: item.meta.sort, roleIDs: item.meta.roleIDs, redirect: item.redirect, children: [] } tableData.push(tData) if (!item.children) { return } menuListFormat(tData.children, item.children) }) } "},{"id":32,"href":"/docs/docs/24_scss/","title":"scss","section":"Docs","content":" 优先级 # !important 行间样式 id选择器 class选择器 标签选择器 通配符选择器: * 继承 # //不可继承,但子元素使用inherit继承父元素的值. width border margin height padding. background float clear position top right bottom left overflow z-index content outline size vertical-align：垂直文本对齐 text-decoration：规定添加到文本的装饰 text-shadow：文本阴影效果 white-space：空白符的处理 unicode-bidi：设置文本的方向 //可继承: font：组合字体 font-family：规定元素的字体系列 font-weight：设置字体的粗细 font-size：设置字体的尺寸 font-style：定义字体的风格 text-indent：文本缩进 text-align：文本水平对齐 line-height：行高 color：文本颜色 visibility cursor page page-break-inside windows orphans text-indent、text-align 块级可继承 vh vw px # 1vh 等于视口高度的1% 1vw 等于视口宽度的1% px 像素 display(块,行内元素) :只是决定了宽高居中方式 # display: none //隐藏元素,将元素所占的位置一并隐藏 display: block //将元素强制转换为块级元素 display: inline //将元素强制转换为行内元素 display: inline-block //将元素强制转换为行内块元素,可设置宽高 display: flex/inline-flex //弹性,通过参数决定块或行 flex-direction: row（默认值）：主轴为水平方向，起点在左端。 row-reverse：主轴为水平方向，起点在右端。 column：主轴为垂直方向，起点在上沿。 column-reverse：主轴为垂直方向，起点在下沿。 flex-wrap: nowrap（默认）：不换行。 wrap：换行，第一行在上方。 wrap-reverse：换行，第一行在下方。 justify-content: flex-start（默认值）：左对齐 flex-end：右对齐 center： 居中 space-between：两端对齐，项目之间的间隔都相等。 space-around：每个项目两侧的间隔相等。所以，项目之间的间隔比项目与边框的间隔大一倍。 align-items: flex-start：交叉轴的起点对齐。 flex-end：交叉轴的终点对齐。 center：交叉轴的中点对齐。 baseline: 项目的第一行文字的基线对齐。 stretch（默认值）：如果项目未设置高度或设为auto，将占满整个容器的高度。 flex:拉伸 收缩 元素大小；默认值为0 1 auto(不拉伸,收缩/0不收缩,自动) position(定位) # //配合top，right，bottom left决定了该元素的最终位置。 position: relative; //相对定位,相对于上 左,父元素定位 position: absolute; //绝对定位,整个尺寸做基点定位 position: sticky; //吸附定位,元素到达某位置将其固定 position: fixed; //固定定位,不管滚动条怎么动，都在一个固定的位置 overflow # overflow:visible /内容不会被修剪，会呈现在元素框之外。 overflow:hidden //内容会被修剪，并且其余内容是不可见的。 overflow:scroll//内容会被修剪，但是浏览器会显示滚动条以便查看其余的内容。 overflow:auto //如果内容被修剪，则浏览器会显示滚动条以便查看其余的内容。 overflow:inherit //规定应该从父元素继承 overflow 属性的值。 text(文字对齐方式) # //文字对齐都使用line-height的值等于行高,然后text-align居中就OK text-align:left; //把文本排列到左边。默认值：由浏览器决定。 text-align:right; //把文本排列到右边。 text-align:center; //把文本排列到中间。 text-align:justify; //实现两端对齐文本效果。 font-weight: 600; //字体粗细 font-style: italic; //斜体 line-height: 0%; //单行文本所在的高度,当 line-height=height(文本高度等于容器高度,单行文本水平垂直居中) font-size: 12px; //字体大小 font-family: \u0026#39;Courier New\u0026#39;, Courier, monospace; //字体包样式 min-height: 60px; //最小高度 border(块加边框样式) # border-width: 11; //,粗细 border-style: solid; //实线 虚线 border-color: #333; //颜色 border-radius: 10px; //定义圆角 color(颜色) # color: #fff //前景色 background #333 //背景色 background-size cursor(鼠标样式) # cursor: pointer; //鼠标变成小手 margin(块外间距) # margin: auto; //块外间距 margin-top: auto; margin-left: auto; margin-right: 8px; margin-bottom: 12px; //下外边距 padding(块内间距) # padding: 3px; //四边同值 padding: 3px 3px; //上下 左右 padding: 3px 3px 3px 3px; //上 右 下 左 (正时针) padding-bottom padding-left padding-right padding-top calc(动态计算值) # width: calc(100% - 10px) 行内元素 # span 块元素 # p "},{"id":33,"href":"/posts/errgroup%E4%BD%BF%E7%94%A8/","title":"errgroup使用","section":"Posts","content":"在go中errgroup包的使用，和源码分析\n包 # golang.org/x/sync/errgroup errgroup.WithContext() # //创建一个errGroup对象和ctx对象 g, ctx := errgroup.WithContext(context.TODO()) //Group结构体 type Group struct { cancel func() // 这个存的是context的cancel方法 wg sync.WaitGroup // 封装sync.WaitGroup errOnce sync.Once // 保证获取的是第一次出错的信息，避免被后面的goroutine的错误覆盖 err error // 保存第一个返回的错误 } func WithContext(ctx context.Context) (*Group, context.Context) { ctx, cancel := context.WithCancel(ctx) return \u0026amp;Group{cancel: cancel}, ctx } Go() # //创建并发线程 g.Go(func() error { return nil }) //内部实现 func (g *Group) Go(f func() error) { g.wg.Add(1) go func() { defer g.wg.Done() if err := f(); err != nil { g.errOnce.Do(func() { g.err = err //记录子协程中的错误 if g.cancel != nil { g.cancel() } }) } }() } wait() # func (g *Group) Wait() error { g.wg.Wait() if g.cancel != nil { g.cancel() } return g.err } 实现 # g, ctx := errgroup.WithContext(context.TODO()) dataMapping := []string{ { \u0026#34;test\u0026#34;, \u0026#34;sadhkl\u0026#34;, \u0026#34;ejbfrkw\u0026#34;, }, } doces := make(chan interface{}, len(dataMapping)) g.Go(func() error { defer close(doces) for _, val := range dataMapping { select { case doces \u0026lt;- val: case \u0026lt;-ctx.Done(): //若可以读取，则其他进程已经发起了取消。 return ctx.Err() } } return nil }) if err := g.Wait(); err != nil { fmt.Println(err) } fmt.Println(doces) "},{"id":34,"href":"/posts/gin%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/","title":"gin参数校验","section":"Posts","content":" 第三方包 # github.com/go-playground/validator/v10 常用参数 # required //必填； len=11 //长度=11； min=3 //如果是数字，验证的是数据大小范围，最小值为3，如果是文本，验证的是最小长度为3， max=6 //如果是数字，验证的是数字最大值为6，如果是文本，验证的是最大长度为6 mail //验证邮箱 gt=3 //对于文本就是长度\u0026gt;=3 lt=6 //对于文本就是长度\u0026lt;=6 翻译中间件 # package middleware import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/gin-gonic/gin/binding\u0026#34; \u0026#34;github.com/go-playground/locales/en\u0026#34; \u0026#34;github.com/go-playground/locales/zh\u0026#34; ut \u0026#34;github.com/go-playground/universal-translator\u0026#34; validator \u0026#34;github.com/go-playground/validator/v10\u0026#34; enTranslations \u0026#34;github.com/go-playground/validator/v10/translations/en\u0026#34; zhTranslations \u0026#34;github.com/go-playground/validator/v10/translations/zh\u0026#34; ) func Translations() gin.HandlerFunc { return func(c *gin.Context) { //locale := .GetHeader(\u0026#34;Acept-Language\u0026#34;) locale := \u0026#34;zh\u0026#34; uni := ut.New(en.New(), zh.New()) trans, _ := uni.GetTranslator(locale) v, ok := binding.Validator.Engine().(*validator.Validate) if ok { switch locale { case \u0026#34;zh\u0026#34;: _ = zhTranslations.RegisterDefaultTranslations(v, trans) case \u0026#34;en\u0026#34;: _ = enTranslations.RegisterDefaultTranslations(v, trans) default: _ = zhTranslations.RegisterDefaultTranslations(v, trans) } c.Set(\u0026#34;trans\u0026#34;, trans) } c.Next() } } 参数校验及错误翻译 # /* type Register struct { UserName string `form:\u0026#34;user_name\u0026#34; json:\u0026#34;user_name\u0026#34; binding:\u0026#34;required,min=1\u0026#34;` Pass string `form:\u0026#34;pass\u0026#34; json:\u0026#34;pass\u0026#34; binding:\u0026#34;required,min=6,max=20\u0026#34;` Captcha string `form:\u0026#34;captcha\u0026#34; json:\u0026#34;captcha\u0026#34; binding:\u0026#34;required,len=4\u0026#34;` Age float64 `form:\u0026#34;age\u0026#34; json:\u0026#34;age\u0026#34; binding:\u0026#34;required,min=1,max=200\u0026#34;` // 注意： 如果你的表单参数含有0值是允许提交的，必须用指针类型（*float64），而 float64 类型则认为 0 值不合格 Status *float64 `form:\u0026#34;status\u0026#34; json:\u0026#34;status\u0026#34; binding:\u0026#34;required,min=0,max=1\u0026#34;` } */ package utils import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ut \u0026#34;github.com/go-playground/universal-translator\u0026#34; \u0026#34;github.com/go-playground/validator/v10\u0026#34; ) // ShouldBindJSON 封装绑定及错误翻译 func ShouldBindJSON(c *gin.Context, obj interface{}) error { if err := c.ShouldBindJSON(obj); err != nil { //翻译错误信息 if errs, ok := err.(validator.ValidationErrors); ok { v := c.Value(\u0026#34;trans\u0026#34;) trans, _ := v.(ut.Translator) var e string for _, v := range errs.Translate(trans) { e += fmt.Sprintf(\u0026#34;%s \u0026#34;, v) } return fmt.Errorf(e) } } return nil } "},{"id":35,"href":"/posts/gin%E8%AE%B0%E5%BD%95%E8%AF%B7%E6%B1%82%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C%E6%97%A5%E5%BF%97/","title":"gin记录请求,返回结果日志","section":"Posts","content":"gin的使用如何收集请求返回结果到日志\n记录请求、应答中间件 # 请求体只能读取一次，解决为重写进请求体 reqBody, _ := c.GetRawData() c.Request.Body = ioutil.NopCloser(bytes.NewBuffer(reqBody)) 写入关键词 key(req、resp、func)便于排查分类 package middleware import ( \u0026#34;bytes\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; uuid \u0026#34;github.com/satori/go.uuid\u0026#34; \u0026#34;github.com/xiaohubai/alpha/config\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; ) func Logger() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() path := c.Request.RequestURI ip := c.ClientIP() mothod := c.Request.Method uid := uuid.NewV4().String() c.Set(\u0026#34;trace_id\u0026#34;, uid) reqBody, _ := c.GetRawData() c.Request.Body = ioutil.NopCloser(bytes.NewBuffer(reqBody)) config.LOG.Info(uid, zap.Any(\u0026#34;key\u0026#34;, \u0026#34;req\u0026#34;), zap.Any(\u0026#34;ip\u0026#34;, ip), zap.Any(\u0026#34;path\u0026#34;, path), zap.String(\u0026#34;method\u0026#34;, mothod), zap.Any(\u0026#34;query\u0026#34;, string(reqBody)), ) c.Next() respBpdy, _ := c.Get(\u0026#34;respBody\u0026#34;) elapse := time.Since(start) config.LOG.Info(uid, zap.Any(\u0026#34;key\u0026#34;, \u0026#34;resp\u0026#34;), zap.String(\u0026#34;ip\u0026#34;, ip), zap.String(\u0026#34;path\u0026#34;, path), zap.String(\u0026#34;method\u0026#34;, mothod), zap.Duration(\u0026#34;elapse\u0026#34;, elapse), zap.Any(\u0026#34;body\u0026#34;, respBpdy), ) } } "},{"id":36,"href":"/posts/gorm%E8%87%AA%E5%88%B6%E6%97%A5%E5%BF%97/","title":"gorm自制日志","section":"Posts","content":" 第三方包 # gorm.io/gorm/logger 示例 # package boot import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/xiaohubai/alpha/config\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;gorm.io/gorm/logger\u0026#34; \u0026#34;gorm.io/gorm/utils\u0026#34; ) type cfg struct { SlowThreshold time.Duration Colorful bool LogLevel logger.LogLevel } type traceRecorder struct { logger.Interface BeginAt time.Time SQL string RowsAffected int64 Err error } func (t traceRecorder) New() *traceRecorder { return \u0026amp;traceRecorder{Interface: t.Interface, BeginAt: time.Now()} } func (t *traceRecorder) Trace(ctx context.Context, begin time.Time, fc func() (string, int64), err error) { t.BeginAt = begin t.SQL, t.RowsAffected = fc() t.Err = err } var ( Discard = New(log.New(ioutil.Discard, \u0026#34;\u0026#34;, log.LstdFlags), cfg{}) Default = New(log.New(os.Stdout, \u0026#34;\\r\\n\u0026#34;, log.LstdFlags), cfg{ SlowThreshold: 200 * time.Millisecond, LogLevel: logger.Warn, Colorful: true, }) Recorder = traceRecorder{Interface: Default, BeginAt: time.Now()} ) type _logger struct { cfg logger.Writer infoStr, warnStr, errStr string traceStr, traceErrStr, traceWarnStr string } func New(writer logger.Writer, c cfg) logger.Interface { var ( infoStr = \u0026#34;%s\\n[info] \u0026#34; warnStr = \u0026#34;%s\\n[warn] \u0026#34; errStr = \u0026#34;%s\\n[error] \u0026#34; traceStr = \u0026#34;%s\\n[%.3fms] [rows:%v] %s\\n\u0026#34; traceWarnStr = \u0026#34;%s %s\\n[%.3fms] [rows:%v] %s\\n\u0026#34; traceErrStr = \u0026#34;%s %s\\n[%.3fms] [rows:%v] %s\\n\u0026#34; ) if c.Colorful { infoStr = logger.Green + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.Green + \u0026#34;[info] \u0026#34; + logger.Reset warnStr = logger.BlueBold + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.Magenta + \u0026#34;[warn] \u0026#34; + logger.Reset errStr = logger.Magenta + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.Red + \u0026#34;[error] \u0026#34; + logger.Reset traceStr = logger.Green + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.Yellow + \u0026#34;[%.3fms] \u0026#34; + logger.BlueBold + \u0026#34;[rows:%v]\u0026#34; + logger.Reset + \u0026#34; %s\\n\u0026#34; traceWarnStr = logger.Green + \u0026#34;%s \u0026#34; + logger.Yellow + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.RedBold + \u0026#34;[%.3fms] \u0026#34; + logger.Yellow + \u0026#34;[rows:%v]\u0026#34; + logger.Magenta + \u0026#34; %s\\n\u0026#34; + logger.Reset traceErrStr = logger.RedBold + \u0026#34;%s \u0026#34; + logger.MagentaBold + \u0026#34;%s\\n\u0026#34; + logger.Reset + logger.Yellow + \u0026#34;[%.3fms] \u0026#34; + logger.BlueBold + \u0026#34;[rows:%v]\u0026#34; + logger.Reset + \u0026#34; %s\\n\u0026#34; } return \u0026amp;_logger{ Writer: writer, cfg: c, infoStr: infoStr, warnStr: warnStr, errStr: errStr, traceStr: traceStr, traceWarnStr: traceWarnStr, traceErrStr: traceErrStr, } } // LogMode log mode func (c *_logger) LogMode(level logger.LogLevel) logger.Interface { newLogger := *c newLogger.LogLevel = level return \u0026amp;newLogger } // Info print info func (c *_logger) Info(ctx context.Context, message string, data ...interface{}) { if c.LogLevel \u0026gt;= logger.Info { c.Printf(ctx, \u0026#34;info\u0026#34;, append([]interface{}{utils.FileWithLineNum()}, data...)...) } } // Warn print warn messages func (c *_logger) Warn(ctx context.Context, message string, data ...interface{}) { if c.LogLevel \u0026gt;= logger.Warn { c.Printf(ctx, \u0026#34;warn\u0026#34;, append([]interface{}{utils.FileWithLineNum()}, data...)...) } } // Error print error messages func (c *_logger) Error(ctx context.Context, message string, data ...interface{}) { if c.LogLevel \u0026gt;= logger.Error { c.Printf(ctx, \u0026#34;error\u0026#34;, append([]interface{}{utils.FileWithLineNum()}, data...)...) } } // Trace print sql message func (c *_logger) Trace(ctx context.Context, begin time.Time, fc func() (string, int64), err error) { if c.LogLevel \u0026gt; 0 { elapsed := time.Since(begin) switch { case err != nil \u0026amp;\u0026amp; c.LogLevel \u0026gt;= logger.Error: sql, rows := fc() c.Printf(ctx, \u0026#34;error\u0026#34;, fmt.Sprintf(\u0026#34;fileLine:%s,elapsed:%f,row:%d,sql:%s,err:%s\u0026#34;, utils.FileWithLineNum(), float64(elapsed.Nanoseconds())/1e6, rows, sql, err)) case elapsed \u0026gt; c.SlowThreshold \u0026amp;\u0026amp; c.SlowThreshold != 0 \u0026amp;\u0026amp; c.LogLevel \u0026gt;= logger.Warn: sql, rows := fc() slowLog := fmt.Sprintf(\u0026#34;SLOW SQL \u0026gt;= %v\u0026#34;, c.SlowThreshold) c.Printf(ctx, \u0026#34;warn\u0026#34;, fmt.Sprintf(\u0026#34;fileLine:%s,elapsed:%f,row:%d,sql:%s,slowLog:%s\u0026#34;, utils.FileWithLineNum(), float64(elapsed.Nanoseconds())/1e6, rows, sql, slowLog)) case c.LogLevel \u0026gt;= logger.Info: sql, rows := fc() c.Printf(ctx, \u0026#34;info\u0026#34;, fmt.Sprintf(\u0026#34;fileLine:%s,elapsed:%f,row:%d,sql:%s\u0026#34;, utils.FileWithLineNum(), float64(elapsed.Nanoseconds())/1e6, rows, sql)) } } } func (c *_logger) Printf(ctx context.Context, key string, data ...interface{}) { traceId := ctx.Value(\u0026#34;trace_id\u0026#34;) if traceId == nil { return } if config.CONFIG.Mysql.LogZap { switch { case key == \u0026#34;error\u0026#34;: config.LOG.Error(traceId.(string), zap.Any(\u0026#34;key\u0026#34;, \u0026#34;sql\u0026#34;), zap.Any(\u0026#34;msg\u0026#34;, fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, \u0026#34;sql\u0026#34;, data))) default: config.LOG.Info(traceId.(string), zap.Any(\u0026#34;key\u0026#34;, \u0026#34;sql\u0026#34;), zap.Any(\u0026#34;msg\u0026#34;, fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, \u0026#34;sql\u0026#34;, data))) } } else { c.Writer.Printf(traceId.(string), data...) } } 问题 # 怎么在gorm 传递trace_id //使用WithContext(c.Request.Context())传递 config.DB.WithContext(c.Request.Context()).Where(\u0026#34;username = ? AND password = ?\u0026#34;, u.Username, u.Password).First(\u0026amp;user) 在gin中*gin.Context中怎么写入context.Context //中间件写入trace_id c.Request = c.Request.WithContext(context.WithValue(c.Request.Context(), \u0026#34;trace_id\u0026#34;, uid)) "},{"id":37,"href":"/posts/go%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/","title":"go垃圾回收机制","section":"Posts","content":"go的垃圾回收机制三色标记法和写屏障技术\n标记清除算法 # 最常见的垃圾回收算法有标记清除(Mark-Sweep) 和引用计数(Reference Count)，Go 语言采用的是标记清除算法。并在此基础上使用了三色标记法和写屏障技术，提高了效率。 标记清除收集器是跟踪式垃圾收集器，其执行过程可以分成标记（Mark）和清除（Sweep）两个阶段： 标记阶段 — 从根对象出发查找并标记堆中所有存活的对象； 清除阶段 — 遍历堆中的全部对象，回收未被标记的垃圾对象并将回收的内存加入空闲链表。 标记清除算法的一大问题是在标记期间，需要暂停程序（Stop the world，STW），标记结束之后，用户程序才可以继续执行。为了能够异步执行，减少 STW 的时间，Go 语言采用了三色标记法。 三色标记法 # 三色标记算法将程序中的对象分成白色、黑色和灰色三类。 标记开始时，所有对象加入白色集合（这一步需 STW ）。 首先将根对象标记为灰色，加入灰色集合，垃圾搜集器取出一个灰色对象，将其标记为黑色，并将其指向的对象标记为灰色，加入灰色集合。重复这个过程，直到灰色集合为空为止，标记阶段结束。那么白色对象即可需要清理的对象，而黑色对象均为根可达的对象，不能被清理。 三色标记法因为多了一个白色的状态来存放不确定对象，所以后续的标记阶段可以并发地执行。当然并发执行的代价是可能会造成一些遗漏，因为那些早先被标记为黑色的对象可能目前已经是不可达的了。所以三色标记法是一个 false negative（假阴性）的算法。 三色标记法并发执行仍存在一个问题，即在 GC 过程中，对象指针发生了改变。比如下面的例子： 1 A (黑) -\u0026gt; B (灰) -\u0026gt; C (白) -\u0026gt; D (白) 正常情况下，D 对象最终会被标记为黑色，不应被回收。但在标记和用户程序并发执行过程中，用户程序删除了 C 对 D 的引用，而 A 获得了 D 的引用。标记继续进行，D 就没有机会被标记为黑色了（A 已经处理过，这一轮不会再被处理）。 1 2 3 A (黑) -\u0026gt; B (灰) -\u0026gt; C (白) ↓ D (白) 为了解决这个问题，Go 使用了内存屏障技术，它是在用户程序读取对象、创建新对象以及更新对象指针时执行的一段代码，类似于一个钩子。垃圾收集器使用了写屏障（Write Barrier）技术，当对象新增或更新时，会将其着色为灰色。这样即使与用户程序并发执行，对象的引用发生改变时，垃圾收集器也能正确处理了。 一次完整的 GC 分为四个阶段： 1）标记准备(Mark Setup，需 STW)，打开写屏障(Write Barrier) 2）使用三色标记法标记（Marking, 并发） 3）标记结束(Mark Termination，需 STW)，关闭写屏障。 4）清理(Sweeping, 并发) "},{"id":38,"href":"/posts/go%E6%8E%A5%E5%8F%A3%E6%96%B9%E6%B3%95%E9%87%8D%E8%BD%BD/","title":"go接口方法重载","section":"Posts","content":" 源码 # package main import \u0026#34;fmt\u0026#34; type Log interface { Error() } type LocalLog struct { Level string `json:\u0026#34;level\u0026#34;` Key string `json:\u0026#34;key\u0026#34;` Msg string `json:\u0026#34;msg\u0026#34;` TraceId string `json:\u0026#34;trace_id\u0026#34;` } type Atta struct { Level string `json:\u0026#34;level\u0026#34;` Key string `json:\u0026#34;key\u0026#34;` Msg string `json:\u0026#34;msg\u0026#34;` TraceId string `json:\u0026#34;trace_id\u0026#34;` AttaId string `json:\u0026#34;atta_id\u0026#34;` } func CommError(obj Log) { obj.Error() } func (l *LocalLog) Error() { fmt.Println(l.Level, l.TraceId) } func (a *Atta) Error() { fmt.Println(a.Level, a.TraceId, a.AttaId) } func main() { l := LocalLog{ Level: \u0026#34;error\u0026#34;, Key: \u0026#34;local\u0026#34;, Msg: \u0026#34;本地日志消息\u0026#34;, TraceId: \u0026#34;76482-32r872t87d-21e787e8\u0026#34;, } l.Error() a := Atta{ Level: \u0026#34;error\u0026#34;, Key: \u0026#34;atta\u0026#34;, Msg: \u0026#34;atta日志消息\u0026#34;, TraceId: \u0026#34;67235r4-3hh232j0-32-384hrbj\u0026#34;, AttaId: \u0026#34;72364028640326402\u0026#34;, } a.Error() CommError(\u0026amp;l) CommError(\u0026amp;a) } "},{"id":39,"href":"/posts/go%E9%A1%B9%E7%9B%AE%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1/","title":"go项目分层设计","section":"Posts","content":" 单系统 Project Layout 介绍 # ├── api (API) │ └── v1 (版本) ├── service (业务层) ├── model (结构体层) ├── dao (数据库操作) ├── middleware (中间件) ├── config (全局配置) ├── boot (初始组件) ├── router (路由控制) ├── plugin (组件具体实现) ├── utils (工具) ├── script (脚本) ├── docs (文档) ├── public (静态文件) ├── log (日志) ├── main.go (入口) ├── go.mod (项目依赖包版本) ├── go.sum (已下载的所有依赖版本) ├── Dockerfile (docker部署文件) └── README.md (项目说明) 大仓微服务 project layout # ├── api //所有proto文件定义，外部引用 │ └── user │ ├── user.pb.go │ ├── user.pb.validate.go │ ├── user.proto │ ├── user_error.pb.go │ ├── user_error.proto │ ├── user_error_errors.pb.go │ ├── user_grpc.pb.go │ └── user_http.pb.go ├── app //大仓多微服务 │ └── user //单服务 │ ├── Makefile //make指令 │ ├── README.md //项目说明 │ ├── cmd //初始配置、启动 │ ├── configs //配置文件 │ ├── scripts //脚本 │ └── internal //内部业务逻辑 │ ├── biz //模型定义、关联接口定义 │ ├── conf //配置实例 │ ├── data //数据库相关连接、生成实例 │ ├── server //http、grpc配置、服务注册 │ └── service //服务实现 ├── third_party //第三方依赖包 │ ├── errors │ ├── google │ └── validate ├── deploy //部署 │ ├── README.md //部署说明 │ ├── build //构建dockerfile │ │ └── Dockerfile //dockerfile文件 │ ├── docker-compose //docker-compose部署 │ │ └── docker-compose.yaml //docker-compose文件 │ └── kubernetes //k8s部署 ├── pkg //公共包 │ └── utlis //工具 ├── Makefile //Makefile指令 ├── go.mod //项目依赖 ├── go.sum └── README.md //大仓说明 "},{"id":40,"href":"/posts/redis%E5%86%85%E5%AD%98%E6%BB%A1%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/","title":"redis内存满了怎么办？","section":"Posts","content":" maxmemory # redis 设置最大使用内存 maxmemory，默认资源超过maxmemory后，不允许新key加入 7种淘汰策略 # volatile-lru：淘汰那些设置了过期时间且最近最少被访问的数据 volatile-random：随机淘汰，腾出坑位给新人； volatile-ttl：淘汰设置了过期时间key，谁最接近时间就先淘汰谁 allkeys-lru：淘汰最近最少上一线干活的职员； allkeys-lfu：淘汰最少上一线干活的公务员； allkeys-random：随机淘汰职员，为新兵腾出空位。 淘汰执行过程 # 客户端发送新命令到服务端； 服务端收到客户端命令，Redis 检查内存使用情况，如果大于 maxmemory 限制，则根据策略驱逐数据。执行新命令，否则执行新命令 使用场景 # allkeys-lru 使用场景有明显的冷热数据区分，充分利用 LRU 算法把最近最常访问的数据保留，有限的内存提高访问性能 allkeys-random 使用场景数据没有明显的冷热分别，所有的数据分布查询比较均衡，让其随机选择淘汰数据 volatile-lru 业务场景有一些数据不能删除，比如置顶新闻、视频，这时候我们为这些数据不设置过期时间，这样的话数据就不会被删除，该策略就会去根据 LRU 算法去淘汰那些设置了过期时间且最近最少被访问的数据 "},{"id":41,"href":"/posts/sql%E8%AF%AD%E5%8F%A5%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","title":"sql语句性能优化","section":"Posts","content":"1、对查询进行优化，应尽量避免全表扫描，首先应考虑在 WHERE 及 ORDER BY 涉及的列上建立索引。 2、应尽量避免在 WHERE 子句中对字段进行 NULL 值判断，创建表时 NULL 是默认值，但大多数时候应该使用 NOT NULL，或者使用一个特殊的值，如 0，-1 作为默认值。 3、应尽量避免在 WHERE 子句中使用 != 或 \u0026lt;\u0026gt; 操作符。MySQL 只有对以下操作符才使用索引：\u0026lt;，\u0026lt;=，=，\u0026gt;，\u0026gt;=，BETWEEN，IN，以及某些时候的 LIKE。 4、应尽量避免在 WHERE 子句中使用 OR 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，可以使用 UNION 合并查询：select id from t where num=10 union all select id from t where num=20。 5、IN 和 NOT IN 也要慎用，否则会导致全表扫描。对于连续的数值，能用 BETWEEN 就不要用 IN：select id from t where num between 1 and 3。、 6、下面的查询也将导致全表扫描：select id from t where name like‘%abc%’ 或者select id from t where name like‘%abc’若要提高效率，可以考虑全文检索。而select id from t where name like‘abc%’才用到索引。 7、如果在 WHERE 子句中使用参数，也会导致全表扫描。 8、应尽量避免在 WHERE 子句中对字段进行表达式操作，应尽量避免在 WHERE 子句中对字段进行函数操作。 9、很多时候用 EXISTS 代替 IN 是一个好的选择：select num from a where num in(select num from b)。用下面的语句替换：select num from a where exists(select 1 from b where num=a.num)。 10、索引固然可以提高相应的 SELECT 的效率，但同时也降低了 INSERT 及 UPDATE 的效率。因为 INSERT 或 UPDATE 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过 6 个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 11、应尽可能的避免更新 clustered（聚合） 索引数据列， 因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 12、尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。 13、尽可能的使用 varchar, nvarchar 代替 char, nchar。因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 14、最好不要使用返回所有：select from t ，用具体的字段列表代替 “*”，不要返回用不到的任何字段。 15、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 16、使用表的别名（Alias）：当在 SQL 语句中连接多个表时，请使用表的别名并把别名前缀于每个 Column 上。这样一来，就可以减少解析的时间并减少那些由 Column 歧义引起的语法错误。 17、使用“临时表”暂存中间结果 ： 18、一些 SQL 查询语句应加上 nolock，读、写是会相互阻塞的，为了提高并发性能。对于一些查询，可以加上 nolock，这样读的时候可以允许写，但缺点是可能读到未提交的脏数据。 19、不要有超过 5 个以上的表连接（JOIN），考虑使用临时表或表变量存放中间结果。少用子查询，视图嵌套不要过深，一般视图嵌套不要超过 2 个为宜。 20、将需要查询的结果预先计算好放在表中，查询的时候再Select。这在SQL7.0以前是最重要的手段，例如医院的住院费计算。 21、用 OR 的字句可以分解成多个查询，并且通过 UNION 连接多个查询。他们的速度只同是否使用索引有关，如果查询需要用到联合索引，用 UNION all 执行的效率更高。多个 OR 的字句没有用到索引，改写成 UNION 的形式再试图与索引匹配。一个关键的问题是否用到索引 22、在IN后面值的列表中，将出现最频繁的值放在最前面，出现得最少的放在最后面，减少判断的次数。 23、尽量将数据的处理工作放在服务器上，减少网络的开销，如使用存储过程。 24、当服务器的内存够多时，配制线程数量 = 最大连接数+5，这样能发挥最大的效率；否则使用配制线程数量\u0026lt; 最大连接数，启用 SQL SERVER 的线程池来解决，如果还是数量 = 最大连接数+5，严重的损害服务器的性能。 25、尽量使用 “\u0026gt;=”，不要使用 “\u0026gt;”。 26、索引的使用规范：\n索引的创建要与应用结合考虑，建议大的 OLTP 表不要超过 6 个索引； 尽可能的使用索引字段作为查询条件，尤其是聚簇索引，必要时可以通过 index index_name 来强制指定索引； 避免对大表查询时进行 table scan，必要时考虑新建索引； 在使用索引字段作为条件时，如果该索引是联合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用； 要注意索引的维护，周期性重建索引，重新编译存储过程。 27、慢查询实例\n//慢查询 SELECT * FROM record WHERE substrINg(card_no,1,4)=’5378’ (13秒) SELECT * FROM record WHERE amount/30\u0026lt; 1000 （11秒） SELECT * FROM record WHERE convert(char(10),date,112)=’19991201’ （10秒） //优化 SELECT * FROM record WHERE card_no like ‘5378%’ （\u0026lt; 1秒） SELECT * FROM record WHERE amount\u0026lt; 1000*30 （\u0026lt; 1秒） SELECT * FROM record WHERE date= ‘1999/12/01’ （\u0026lt; 1秒） 28、当有一批处理的插入或更新时，用批量插入或批量更新，绝不会一条条记录的去更新。\n29、提高 GROUP BY 语句的效率，可以通过将不需要的记录在 GROUP BY 之前过滤掉。下面两个查询返回相同结果，但第二个明显就快了许多。\n//低效 SELECT JOB,AVG(SAL) FROM EMP GROUP BY JOB HAVING JOB =\u0026#34;PRESIDENT\u0026#34; OR JOB =\u0026#34;MANAGER\u0026#34; //高效 SELECT JOB,AVG(SAL) FROM EMP WHERE JOB =\u0026#34;PRESIDENT\u0026#34; OR JOB =\u0026#34;MANAGER\u0026#34; GROUP BY JOB 30、SQL 语句用大写，因为 Oracle 总是先解析 SQL 语句，把小写的字母转换成大写的再执行。\n31、别名的使用，别名是大型数据库的应用技巧，就是表名、列名在查询中以一个字母为别名，查询速度要比建连接表快 1.5 倍。\n32、索引创建规则\n表的主键、外键必须有索引； 数据量超过 300 的表应该有索引； 经常与其他表进行连接的表，在连接字段上应该建立索引； 经常出现在 WHERE 子句中的字段，特别是大表的字段，应该建立索引； 索引应该建在选择性高的字段上； 索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引； 复合索引的建立需要进行仔细分析，尽量考虑用单字段索引代替； 正确选择复合索引中的主列字段，一般是选择性较好的字段； 复合索引的几个字段是否经常同时以 AND 方式出现在 WHERE 子句中？单字段查询是否极少甚至没有？如果是，则可以建立复合索引；否则考虑单字段索引； 如果复合索引中包含的字段经常单独出现在 WHERE 子句中，则分解为多个单字段索引； 如果复合索引所包含的字段超过 3 个，那么仔细考虑其必要性，考虑减少复合的字段； 如果既有单字段索引，又有这几个字段上的复合索引，一般可以删除复合索引； 频繁进行数据操作的表，不要建立太多的索引； 删除无用的索引，避免对执行计划造成负面影响； 表上建立的每个索引都会增加存储开销，索引对于插入、删除、更新操作也会增加处理上的开销。另外，过多的复合索引，在有单字段索引的情况下，一般都是没有存在价值的；相反，还会降低数据增加删除时的性能，特别是对频繁更新的表来说，负面影响更大。 尽量不要对数据库中某个含有大量重复的值的字段建立索引。 33、查询缓冲并不自动处理空格，因此，在写 SQL 语句时，应尽量减少空格的使用，尤其是在 SQL 首和尾的空格（因为查询缓冲并不自动截取首尾空格）。\n34、我们应该为数据库里的每张表都设置一个 ID 做为其主键，而且最好的是一个 INT 型的（推荐使用 UNSIGNED），并设置上自动增加的 AUTO_INCREMENT 标志。\n35、EXPLAIN SELECT 查询用来跟踪查看效果\n36、当只要一行数据时使用 LIMIT 1\n37、选择表合适存储引擎：\nmyisam：应用时以读和插入操作为主，只有少量的更新和删除，并且对事务的完整性，并发性要求不是很高的。 InnoDB：事务处理，以及并发条件下要求数据的一致性。除了插入和查询外，包括很多的更新和删除。（InnoDB 有效地降低删除和更新导致的锁定）。 对于支持事务的 InnoDB类 型的表来说，影响速度的主要原因是 AUTOCOMMIT 默认设置是打开的，而且程序没有显式调用 BEGIN 开始事务，导致每插入一条都自动提交，严重影响了速度。可以在执行 SQL 前调用 begin，多条 SQL 形成一个事物（即使 autocommit 打开也可以），将大大提高性能。 38、优化表的数据类型，选择合适的数据类型：\n更小通常更好，简单就好，所有字段都得有默认值，尽量避免 NULL。 39、任何对列的操作都将导致表扫描，它包括数据库函数、计算表达式等等，查询时要尽可能将操作移至等号右边。\n40、避免使用 select *\n41、用 union all 代替 union，除非是有些特殊的场景，比如 union all 之后，结果集中出现了重复数据，而业务场景中是不允许产生重复数据的，这时可以使用 union。\n42、小表驱动大表\nin 适用于左边大表，右边小表。 exists 适用于左边小表，右边大表。 "},{"id":42,"href":"/posts/wsl2%E5%9B%BA%E5%AE%9Aip/","title":"wsl2固定ip","section":"Posts","content":" 解决wsl2重启 内网ip发生变化 # @echo off setlocal enabledelayedexpansion wsl --shutdown ::重新拉起来，并且用root的身份，启动ssh服务和docker服务 wsl -u root service docker start | findstr \u0026#34;Starting Docker\u0026#34; \u0026gt; nul if !errorlevel! equ 0 ( echo docker start success :: 看看我要的IP在不在 wsl -u root ip addr | findstr \u0026#34;172.21.0.2\u0026#34; \u0026gt; nul if !errorlevel! equ 0 ( echo wsl ip has set ) else ( wsl -u root ip addr add 172.21.0.2/24 broadcast 172.21.0.0 dev eth0 label eth0:1 echo set wsl ip success: 172.21.0.2 ) ::windows作为wsl的宿主，在wsl的固定IP的同一网段也给安排另外一个IP ipconfig | findstr \u0026#34;172.21.0.1\u0026#34; \u0026gt; nul if !errorlevel! equ 0 ( echo windows ip has set ) else ( netsh interface ip add address \u0026#34;vEthernet (WSL)\u0026#34; 172.21.0.1 255.255.255.0 echo set windows ip success: 172.21.0.1 ) ) pause "},{"id":43,"href":"/posts/wsl2%E8%AE%BE%E7%BD%AEroot%E5%AF%86%E7%A0%81%E5%92%8C%E7%99%BB%E5%BD%95/","title":"wsl2设置root密码和登录","section":"Posts","content":" 设置root密码 # sudo passwd root root登录 # su "},{"id":44,"href":"/posts/zap/","title":"zap使用","section":"Posts","content":" 第三方包 # go.uber.org/zap zap记录日志 # uber 开源的高性能日志库，面向高性能 package boot import ( \u0026#34;fmt\u0026#34; \u0026#34;path\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/xiaohubai/alpha/config\u0026#34; zaprotatelogs \u0026#34;github.com/lestrrat-go/file-rotatelogs\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; ) var level zapcore.Level // Zap 日志组件 func Zap() (logger *zap.Logger) { level = zap.InfoLevel logger = zap.New(getEncoderCore()) logger.WithOptions(zap.AddCaller()) return logger } func getEncoderCore() (core zapcore.Core) { writer, err := GetWriteSyncer() // 使用file-rotatelogs进行日志分割 if err != nil { panic(fmt.Errorf(\u0026#34;Get Write Syncer Failed err:%v\u0026#34;, err.Error())) } return zapcore.NewCore(zapcore.NewJSONEncoder(getEncoderConfig()), writer, level) } func getEncoderConfig() (cfg zapcore.EncoderConfig) { cfg = zapcore.EncoderConfig{ MessageKey: \u0026#34;trace_id\u0026#34;, LevelKey: \u0026#34;level\u0026#34;, TimeKey: \u0026#34;time\u0026#34;, NameKey: \u0026#34;logger\u0026#34;, CallerKey: \u0026#34;caller\u0026#34;, StacktraceKey: config.CONFIG.Zap.StacktraceKey, LineEnding: zapcore.DefaultLineEnding, //默认换行 EncodeLevel: zapcore.LowercaseLevelEncoder, //小写 EncodeTime: CustomTimeEncoder, //输出时间 EncodeDuration: zapcore.SecondsDurationEncoder, EncodeCaller: zapcore.FullCallerEncoder, //记录调用路径 } return cfg } // 自定义日志输出时间格式 func CustomTimeEncoder(t time.Time, enc zapcore.PrimitiveArrayEncoder) { enc.AppendString(t.Format(\u0026#34;2006-01-02 15:04:05.000\u0026#34;)) } // GetWriteSyncer 日志分割 func GetWriteSyncer() (zapcore.WriteSyncer, error) { fileWriter, err := zaprotatelogs.New( path.Join(config.CONFIG.Zap.Director, \u0026#34;%Y-%m-%d.log\u0026#34;), zaprotatelogs.WithLinkName(config.CONFIG.Zap.LinkName), zaprotatelogs.WithMaxAge(30*24*time.Hour), //日志清除时间 zaprotatelogs.WithRotationTime(24*time.Hour), //日志文件创建时间 ) return zapcore.AddSync(fileWriter), err } "},{"id":45,"href":"/posts/%E6%8E%A7%E5%88%B6%E5%8D%8F%E7%A8%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4/","title":"控制协程运行时间","section":"Posts","content":"go控制协程运行时间，使用context.WithTimeout(newCtx, timeout)，用recover捕获异常情况\nfunc Go(ctx context.Context, timeout time.Duration, handler func(context.Context)) error { oldMsg := codec.Message(ctx) newCtx, newMsg := codec.WithNewMessage(detach(ctx)) codec.CopyMsg(newMsg, oldMsg) newCtx, cancel := context.WithTimeout(newCtx, timeout) go func() { defer func() { if e := recover(); e != nil { buf := make([]byte, PanicBufLen) buf = buf[:runtime.Stack(buf, false)] log.Errorf(\u0026#34;[PANIC]%v\\n%s\\n\u0026#34;, e, buf) report.PanicNum.Incr() } cancel() }() handler(newCtx) }() return nil } "},{"id":46,"href":"/posts/%E9%AB%98%E5%B9%B6%E5%8F%91%E6%97%B6%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","title":"高并发时，数据一致性问题","section":"Posts","content":"对于共享资源，在多用户访问、程序内、数据库中，怎么保证共享资源独占，数据一致性。\n分布式锁 # 分布式锁可以保证多用户独占资源。 读写锁 # 读写锁可以在程序内保证共享资源的临界 事务 # 事务可以解决mysql数据库的数据一致性 唯一索引 # 将一个字段加入唯一索引，可以避免重复数据 为什么唯一索引可以保证数据唯一？ # mysql作为数据库内部主要为 server层（连接管理-\u0026gt;分析器-\u0026gt;优化器-\u0026gt;执行器）和存储引擎层（myISAM、InnoDB、MEMORY）, 数据库读写不从磁盘直接读取，速度太慢，磁盘前面加一层内存（buffer pool），主要是使用双向链表，存放一个个数据页，每个默认16kb, 数据页存放的是磁盘的数据。一般插入、更新 放入change buffer后直接返回，异步更新数据页，再写回磁盘。 但是唯一索引要保证数据唯一，先看数据有无重复，无重复再插入或更新，性能上输了一大截。唯一索引会绕过change buffer , 确保把磁盘数据读取到内存后再判断数据是否存在，不存在才会插入数据，否则报错，以此保证数据唯一。 串行隔离级别 # 事务们都是依次执行的,保证数据唯一，但是性能较差， // 修改当前会话为串行化 SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; // 查看当前会话的事务隔离级别 select @@tx_isolation; // 输出 SERIALIZABLE // 查看全局事务隔离级别 select @@global.tx_isolation; // 输出 REPEATABLE-READ 总结 # 加唯一索引可以保证数据并发写入时数据唯一，而且最省事省心。 数据库通过引入一层buffer pool内存来提升读写速度，普通索引可以利用change buffer提高数据插入的性能。 唯一索引会绕过change buffer，确保把磁盘数据读到内存后再判断数据是否存在，不存在才能插入数据，否则报错，以此来保证数据是唯一的。 更改隔离级别为串行化，也能实现并发写入时数据唯一。 "}]